{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ee5f81f7-c915-4754-9a11-e4042245f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements \n",
    "import json\n",
    "import obonet\n",
    "from itertools import combinations \n",
    "from Bio import Medline\n",
    "import networkx as nx\n",
    "import string\n",
    "from textblob import TextBlob  \n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import tree\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "63974b6d-8293-4445-ab3a-439a2609ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment configurations\n",
    "# MAX_NUMBER_BIGRAMS = 30\n",
    "MAX_NUMBER_ARTICLE = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8ce8bc1a-8191-4147-8c82-f02f98486f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing a medline file \n",
    "def parse_medline_rmap(medline_file):    \n",
    "    map_abstracts = {}    \n",
    "    pmid = ''\n",
    "    abstract = ''  \n",
    "    with open(medline_file) as medline_handle:\n",
    "        records = Medline.parse(medline_handle)\n",
    "        for record in records:         \n",
    "            keys = record.keys()            \n",
    "            if 'PMID' in keys and 'AB' in keys: \n",
    "\n",
    "                pmid = record['PMID']\n",
    "                abstract = record['AB']\n",
    "                \n",
    "                map_abstracts[pmid] = abstract.lower()\n",
    "    return map_abstracts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "85f0e020-a6bb-4b94-b88d-b7500d99de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_gpt_api_data(json_file):\n",
    "\n",
    "    json_records_map = {}\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Now json_data is a list of dictionaries, each representing an item in the array\n",
    "    for item in json_data:\n",
    "        gpt_id = item['GPT-ID']\n",
    "        title = item['Title']\n",
    "        abstract = item['Abstract']\n",
    "        # json_records_map[gpt_id]=(title + \" \" + abstract)\n",
    "        json_records_map[gpt_id]=(title + \" \" + abstract)        \n",
    "    return json_records_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "13080a90-1415-4f9e-ae19-a183512273b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "      \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e603b3a6-895e-4887-9d4f-611ce9a333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_abstracts = parse_medline_rmap('../dataset/pubmed-cancerandc-set-2015-2019.txt')\n",
    "cgpt_abstracts = parse_json_gpt_api_data('../dataset/cancer-gpt-apis.txt')\n",
    "\n",
    "# cleaning PubMed articles from special characters\n",
    "clean_pubmed_articles = []\n",
    "for abst in list(pubmed_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_pubmed_articles.append(cleaned)\n",
    "    \n",
    "# cleaning chatGPT articles from special characters\n",
    "clean_chatGPT_articles = []\n",
    "for abst in list(cgpt_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_chatGPT_articles.append(cleaned)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b71e1951-6355-4495-96f3-f11c6de17793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "although rare bronchopleural fistula bpf following anatomic lung resection is a serious complication associated with high rates of mortality risk factors for bpf include surgical approach neoadjuvant therapy diabetes mellitus and chronic obstructive pulmonary disease as neoadjuvant treatment is increasingly being administered to patients with locally advanced lung cancer and as more patients are being diagnosed with lung cancer at an older ageelderly patients present with a higher index of multiple comorbiditiesthe incidence of bpf among patients undergoing anatomic resection for lung cancer is expected to increase in this manuscript we detail risk factors and considerations for bpf and describe a stepwise approach to treat bpf following lobectomy for lung cancer\n"
     ]
    }
   ],
   "source": [
    "print(clean_pubmed_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7cd597f6-5b64-465a-940a-22512d16f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "special_list = ['abstract']\n",
    "\n",
    "def stopwords_rem_pubmed(clean_pubmed_training):\n",
    "    stopped_pubmed_training = []\n",
    "    for abst in clean_pubmed_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if token not in stop_words:\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_pubmed_training.append(valid_rec)\n",
    "    return stopped_pubmed_training\n",
    "    \n",
    "    \n",
    "def stopwords_rem_chatGPT_dataset(clean_chatGPT):    \n",
    "    stopped_chatGPT_training = []\n",
    "    for abst in clean_chatGPT_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if (token not in stop_words) and (token not in special_list):\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_chatGPT_training.append(valid_rec)   \n",
    "    return stopped_chatGPT_training\n",
    "\n",
    "\n",
    "def stopwords_rem_chatGPT_article(clean_chatGPT_article):    \n",
    "    stopped_chatGPT_training = []\n",
    "    valid_l = []\n",
    "    valid_rec = []\n",
    "    blob_object = TextBlob(clean_chatGPT_article)\n",
    "    list_tokens = blob_object.words\n",
    "\n",
    "    for token in list_tokens:        \n",
    "        if (token not in stop_words) and (token not in special_list):\n",
    "            valid_l.append(token)            \n",
    "    valid_rec = ' '.join(valid_l)\n",
    "    # stopped_chatGPT_training.append(valid_rec)   \n",
    "    return str(valid_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "756e0025-68d5-4820-afbb-3c220042d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202\n"
     ]
    }
   ],
   "source": [
    "pubmed_articles_ready = stopwords_rem_pubmed(clean_pubmed_articles)\n",
    "\n",
    "# print(len(stopped_pubmed_training))  \n",
    "gpt_articles_ready = []\n",
    "for article in clean_chatGPT_articles:\n",
    "    gpt_articles_ready.append(stopwords_rem_chatGPT_article(article))\n",
    "print(len(gpt_articles_ready))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8345cb7a-ac87-4af6-aafe-5fb4d2babe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pubmed_articles_ready[0])\n",
    "# print('-----')\n",
    "# print(gpt_articles_ready[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d5ffe92e-af59-4d04-b6c3-9c318836e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting PubMed bigrams\n",
    "def compute_bigrams(training_articles):\n",
    "    list_bigrams = []\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range =(2, 2))\n",
    "    X1 = vectorizer.fit_transform(training_articles)\n",
    "    features = (vectorizer.get_feature_names_out())\n",
    "    # print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "    # Applying TFIDF\n",
    "    # You can still get n-grams here\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "    X2 = vectorizer.fit_transform(training_articles)\n",
    "    scores = (X2.toarray())\n",
    "    # print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "    # Getting top ranking features\n",
    "    sums = X2.sum(axis = 0)\n",
    "    data1 = []\n",
    "    for col, term in enumerate(features):\n",
    "        data1.append( (term, sums[0, col] ))\n",
    "    ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "    words = (ranking.sort_values('rank', ascending = False))\n",
    "\n",
    "    bigram_ranks = {}\n",
    "    for index, row in words.iterrows():\n",
    "        # print(row['term'],'\\t\\t\\t',  row['rank'])\n",
    "\n",
    "        splits = row['term'].split()\n",
    "        bigram_ranks[row['rank']] = (splits[0], splits[1])\n",
    "\n",
    "    count = 0    \n",
    "    for k, v in bigram_ranks.items():\n",
    "        # if count < MAX_NUMBER_BIGRAMS:\n",
    "        #     # print(k,'\\t',  v)\n",
    "        #     count += 1\n",
    "        list_bigrams.append(v)\n",
    "    return bigram_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7ff019e0-a5ba-4c93-b9ba-62f7b9065699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_model(training_articles):\n",
    "    bigrams_map_training = compute_bigrams(training_articles)\n",
    "    gpt_training_bigrams = bigrams_map_training.values()\n",
    "    \n",
    "    graph_training_model = nx.Graph()\n",
    "    graph_training_model.add_edges_from(list(gpt_training_bigrams))\n",
    "    \n",
    "    return graph_training_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ded8c1c2-3712-48c4-803c-7f828e9b62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT Training Model --------\n",
      "Original node count:  559\n",
      "Original edge count:  1050\n",
      " -------- PubMed Training Model --------\n",
      "Original node count:  828\n",
      "Original edge count:  977\n"
     ]
    }
   ],
   "source": [
    "# construct a network training model from both datasets (gpt and pubmed)\n",
    "\n",
    "gpt_training_model = construct_training_model(gpt_articles_ready[:100])\n",
    "pubmed_training_model = construct_training_model(pubmed_articles_ready[:100])\n",
    "\n",
    "# ----------   Verifying GPT Training  Model ----------# \n",
    "print(' -------- GPT Training Model --------')\n",
    "node_count = len(gpt_training_model.nodes())\n",
    "edge_count = len(gpt_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)\n",
    "\n",
    "# ----------   Verifying PubMed Training  Model ----------# \n",
    "print(' -------- PubMed Training Model --------')\n",
    "node_count = len(pubmed_training_model.nodes())\n",
    "edge_count = len(pubmed_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5ffffcb0-d548-4376-96d7-9354472b35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giant_lcc(graph_training_model):\n",
    "    gcc = sorted(nx.connected_components(graph_training_model), key=len, reverse=True)\n",
    "    giant_cc = graph_training_model.subgraph(gcc[0])\n",
    "    return giant_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b7da8e22-e4a9-4973-922d-bd411fa4ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT GIANT LCC Graph --------\n",
      "Graph with 489 nodes and 1008 edges\n",
      " -------- PUBMED GIANT LCC Graph --------\n",
      "Graph with 588 nodes and 842 edges\n"
     ]
    }
   ],
   "source": [
    "print(' -------- GPT GIANT LCC Graph --------')\n",
    "gpt_lcc = get_giant_lcc(gpt_training_model)\n",
    "print(gpt_lcc)\n",
    "\n",
    "print(' -------- PUBMED GIANT LCC Graph --------')\n",
    "pubmed_lcc = get_giant_lcc(pubmed_training_model)\n",
    "print(pubmed_lcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0950208d-636b-4784-b0a0-c1b8f4561eae",
   "metadata": {},
   "outputs": [],
   "source": [
    " # STEP2: -- compute individual articles bigrams -------\n",
    "def calibrate_model(ds_label, begin_index, end_index, training_graph, calibrate_set):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy() \n",
    "\n",
    "    ratios_added_per_fold = []\n",
    "    for abst in calibrate_set[begin_index:end_index]:\n",
    "        \n",
    "        tokens = nltk.word_tokenize(abst)\n",
    "\n",
    "        # compute the bigrams\n",
    "        bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "        # -------  check if the giant has the bigram components, add new edge \n",
    "        # -------          otherwise, don't add new edges\n",
    "        # -------  count how many nodes            \n",
    "        count = 0\n",
    "        added_edges = []\n",
    "        for bigram in bigrams:\n",
    "\n",
    "            if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "                if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                    training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                    count += 1\n",
    "                    added_edges.append((bigram[0], bigram[1]))\n",
    "        ratio_ = count / len(tokens)        \n",
    "        \n",
    "        ratios_added_per_fold.append(ratio_) \n",
    "        \n",
    "        training_graph_copy.remove_edges_from(added_edges)      \n",
    "    return ratios_added_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bd014f6c-9a92-4e85-a279-78a8258b941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(tst_set_list):\n",
    "    average = sum(tst_set_list) / len(tst_set_list)        \n",
    "    formatted_avg = float(\"{:.5f}\".format(average))        \n",
    "    return formatted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "df86f04a-4d5e-4535-b810-1cadc3bb0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of the list is: 0.27947\n",
      "The average of the list is: 0.29009\n",
      "The average of the list is: 0.26738\n",
      "The average of the list is: 0.25622\n",
      "The average of the list is: 0.25135\n",
      "The average of the list is: 0.28374\n",
      "The average of the list is: 0.27115\n",
      "The average of the list is: 0.26867\n",
      "The average of the list is: 0.25155\n",
      "The average of the list is: 0.25541\n",
      "The average of the list is: 0.24857\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "gpt_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, gpt_lcc, gpt_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_g = calc_mean(calb_ratios_list) \n",
    "        print(\"The average of the list is:\", tst_mean_g)\n",
    "        gpt_means.append(tst_mean_g)\n",
    "        \n",
    "gpt_min_value = min(gpt_means)\n",
    "gpt_max_value = max(gpt_means)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d67982e6-5346-48f0-8317-96fb6c705464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15838\n",
      "0.15191\n",
      "0.16585\n",
      "0.15232\n",
      "0.16439\n",
      "0.16477\n",
      "0.15776\n",
      "0.15788\n",
      "0.15757\n",
      "0.15096\n",
      "0.15829\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pubmed_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, pubmed_lcc, pubmed_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_p = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_p)\n",
    "        pubmed_means.append(tst_mean_p)\n",
    "        \n",
    "pubmed_min_value = min(pubmed_means)\n",
    "pubmed_max_value = max(pubmed_means) \n",
    "# print(gpt_means)\n",
    "for ratio in pubmed_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ed318152-06ef-4e6e-8a70-aba811af55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_an_article(article_text, training_graph):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy()\n",
    "    \n",
    "    # chat_no_added_edges = []\n",
    "    # for abst in stopped_pubmed_training[begin_index:end_index]:\n",
    "\n",
    "    tokens = nltk.word_tokenize(article_text)\n",
    "\n",
    "    # compute the bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # -------  check if the giant has the bigram components, add new edge \n",
    "    # -------          otherwise, don't add new edges\n",
    "    # -------  count how many nodes    \n",
    "\n",
    "    count = 0\n",
    "    added_edges = []\n",
    "    for bigram in bigrams:\n",
    "\n",
    "        if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "            if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                count += 1\n",
    "                added_edges.append((bigram[0], bigram[1]))\n",
    "    ratio_ = count / len(tokens)        \n",
    "    training_graph_copy.remove_edges_from(added_edges)\n",
    "        \n",
    "    return ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f5000d5a-e4bf-4940-a3ec-7a68f557d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.0\n",
      "CORRECT CLASSIFIED:  1.25\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The average of the list is: 0.15191\n",
    "# The average of the list is: 0.16585\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in gpt_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= pubmed_max_value and ratio_val <= pubmed_max_value :       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/800)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/800)   \n",
    "print('-------------------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cc6b890e-7be7-4214-ab38-809eae6018b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.03\n",
      "CORRECT CLASSIFIED:  9.97\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in pubmed_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= gpt_min_value and ratio_val <= gpt_max_value:       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/100)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/100)   \n",
    "print('-------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a8dbdf11-489f-4143-9c67-65320e99d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_range(point, range_start, range_end):\n",
    "    # Calculate the distance to the nearest endpoint of the range\n",
    "    distance = min(abs(point - range_start), abs(point - range_end))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "30ed6e22-56d9-48bd-a0d8-31adf88a5073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance to range 1:  0.013519241706161145\n",
      "distance to range 2:  0.12060791469194315\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13744075829383887 , evidence: PUBMED: background l\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0014790243902438882\n",
      "distance to range 2:  0.16930170731707317\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1524390243902439 , evidence: PUBMED: background p\n",
      " -------------------------------- \n",
      "distance to range 1:  0.11199896103896105\n",
      "distance to range 2:  0.22259597402597403\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.03896103896103896 , evidence: PUBMED: comorbid pai\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0012139130434782597\n",
      "distance to range 2:  0.18878739130434785\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.15217391304347827 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.052122790697674434\n",
      "distance to range 2:  0.1904304651162791\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09883720930232558 , evidence: PUBMED: background t\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07934230769230768\n",
      "distance to range 2:  0.18607\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.24519230769230768 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03944801324503311\n",
      "distance to range 2:  0.20883490066225166\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.2052980132450331 , evidence: PUBMED: objective an\n",
      " -------------------------------- \n",
      "distance to range 1:  0.004618536585365873\n",
      "distance to range 2:  0.1814968292682927\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.14634146341463414 , evidence: PUBMED: purpose stud\n",
      " -------------------------------- \n",
      "distance to range 1:  0.09364367088607595\n",
      "distance to range 2:  0.13464594936708862\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.25949367088607594 , evidence: PUBMED: purpose pati\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06965918699186993\n",
      "distance to range 2:  0.1510090243902439\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08130081300813008 , evidence: PUBMED: past decade \n",
      " -------------------------------- \n",
      "distance to range 1:  0.007179534883720917\n",
      "distance to range 2:  0.19740720930232558\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.15813953488372093 , evidence: PUBMED: objective ai\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0016626903553299544\n",
      "distance to range 2:  0.20288472081218276\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.16751269035532995 , evidence: PUBMED: background e\n",
      " -------------------------------- \n",
      "distance to range 1:  0.001788729281767959\n",
      "distance to range 2:  0.19884624309392268\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.14917127071823205 , evidence: PUBMED: objective ut\n",
      " -------------------------------- \n",
      "distance to range 1:  0.01762666666666668\n",
      "distance to range 2:  0.17449592592592594\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13333333333333333 , evidence: PUBMED: purpose firs\n",
      " -------------------------------- \n",
      "distance to range 1:  0.01645707602339183\n",
      "distance to range 2:  0.13161093567251464\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13450292397660818 , evidence: PUBMED: objectives h\n",
      " -------------------------------- \n",
      "distance to range 1:  0.040853910614525146\n",
      "distance to range 2:  0.13125156424581008\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.20670391061452514 , evidence: PUBMED: background p\n",
      " -------------------------------- \n",
      "distance to range 1:  0.017823469387755114\n",
      "distance to range 2:  0.18054278911564625\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1836734693877551 , evidence: PUBMED: purpose fast\n",
      " -------------------------------- \n",
      "distance to range 1:  0.10546782945736433\n",
      "distance to range 2:  0.15554674418604653\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.2713178294573643 , evidence: PUBMED: purpose inve\n",
      " -------------------------------- \n",
      "distance to range 1:  0.10498333333333332\n",
      "distance to range 2:  0.14440333333333333\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.2708333333333333 , evidence: PUBMED: background s\n",
      " -------------------------------- \n",
      "distance to range 1:  0.049939473684210534\n",
      "distance to range 2:  0.20646473684210528\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.21578947368421053 , evidence: PUBMED: objective ob\n",
      " -------------------------------- \n",
      "distance to range 1:  0.011927777777777787\n",
      "distance to range 2:  0.14486629629629633\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.17777777777777778 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.08644387096774195\n",
      "distance to range 2:  0.1759893548387097\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06451612903225806 , evidence: PUBMED: obesity asso\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06403505747126437\n",
      "distance to range 2:  0.07615620689655173\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.22988505747126436 , evidence: PUBMED: background c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.10684235294117647\n",
      "distance to range 2:  0.1897464705882353\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.04411764705882353 , evidence: PUBMED: background n\n",
      " -------------------------------- \n",
      "distance to range 1:  0.006759090909090909\n",
      "distance to range 2:  0.15197909090909092\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1590909090909091 , evidence: PUBMED: background n\n",
      " -------------------------------- \n",
      "distance to range 1:  0.005356172839506179\n",
      "distance to range 2:  0.17449592592592594\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.16049382716049382 , evidence: PUBMED: aim gonadotr\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04499337349397592\n",
      "distance to range 2:  0.17025674698795182\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.21084337349397592 , evidence: PUBMED: purpose dete\n",
      " -------------------------------- \n",
      "distance to range 1:  0.004024273858921151\n",
      "distance to range 2:  0.1448355601659751\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.16182572614107885 , evidence: PUBMED: objectives c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.12103524590163933\n",
      "distance to range 2:  0.18299622950819672\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.28688524590163933 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02851102040816328\n",
      "distance to range 2:  0.12612102040816328\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12244897959183673 , evidence: PUBMED: background k\n",
      " -------------------------------- \n",
      "distance to range 1:  0.059123265306122455\n",
      "distance to range 2:  0.12612102040816328\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09183673469387756 , evidence: PUBMED: il antagonis\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03191238095238097\n",
      "distance to range 2:  0.18904619047619048\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11904761904761904 , evidence: PUBMED: background m\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07256059602649007\n",
      "distance to range 2:  0.06976205298013247\n",
      "GPT PREDICTED INCORRECTLY =>  ratio: 0.17880794701986755 , evidence: PUBMED: managing eld\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04648238805970151\n",
      "distance to range 2:  0.08439089552238807\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1044776119402985 , evidence: PUBMED: despite pers\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02666621468926554\n",
      "distance to range 2:  0.1807733898305085\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12429378531073447 , evidence: PUBMED: background c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03294757763975156\n",
      "distance to range 2:  0.1988805590062112\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11801242236024845 , evidence: PUBMED: purpose anal\n",
      " -------------------------------- \n",
      "distance to range 1:  0.01033500000000001\n",
      "distance to range 2:  0.15091375\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.140625 , evidence: PUBMED: background s\n",
      " -------------------------------- \n",
      "distance to range 1:  0.09529649681528662\n",
      "distance to range 2:  0.19124515923566882\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.2611464968152866 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.023300425531914915\n",
      "distance to range 2:  0.14927921985815604\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1276595744680851 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.042783093525179866\n",
      "distance to range 2:  0.1262678417266187\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.20863309352517986 , evidence: PUBMED: purpose earl\n",
      " -------------------------------- \n",
      "distance to range 1:  0.12303888888888886\n",
      "distance to range 2:  0.08190333333333336\n",
      "GPT PREDICTED INCORRECTLY =>  ratio: 0.16666666666666666 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.004618536585365873\n",
      "distance to range 2:  0.1662529268292683\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.14634146341463414 , evidence: PUBMED: background s\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07264674698795182\n",
      "distance to range 2:  0.17628084337349398\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0783132530120482 , evidence: PUBMED: aim evaluate\n",
      " -------------------------------- \n",
      "distance to range 1:  0.01842987951807229\n",
      "distance to range 2:  0.17628084337349398\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13253012048192772 , evidence: PUBMED: background u\n",
      " -------------------------------- \n",
      "distance to range 1:  0.00329531914893616\n",
      "distance to range 2:  0.051761489361702134\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.15425531914893617 , evidence: PUBMED: purpose adve\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0015642857142857125\n",
      "distance to range 2:  0.18428428571428573\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.16428571428571428 , evidence: PUBMED: although ele\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04507764705882354\n",
      "distance to range 2:  0.06033470588235296\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10588235294117647 , evidence: PUBMED: availability\n",
      " -------------------------------- \n",
      "distance to range 1:  0.026696583850931682\n",
      "distance to range 2:  0.1926693788819876\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.19254658385093168 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.011295195530726276\n",
      "distance to range 2:  0.16477111731843577\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13966480446927373 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.09399797468354432\n",
      "distance to range 2:  0.12198772151898735\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.056962025316455694 , evidence: PUBMED: classificati\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2905982905982906 evidence GPT: cardiovascular \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3157894736842105 evidence GPT: association ova\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07953142857142859\n",
      "distance to range 2:  0.03428428571428574\n",
      "GPT CLASS PREDICTED =>  ratio: 0.21428571428571427 , evidence: GPT: complex interac\n",
      " -------------------------------- \n",
      "distance to range 1:  0.01274861788617887\n",
      "distance to range 2:  0.012797642276422772\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.13821138211382114 , evidence: GPT: thyroid cancer \n",
      " -------------------------------- \n",
      "distance to range 1:  0.08746793650793652\n",
      "distance to range 2:  0.08983984126984129\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.06349206349206349 , evidence: GPT: association mel\n",
      " -------------------------------- \n",
      "distance to range 1:  0.021649655172413795\n",
      "distance to range 2:  0.05891482758620692\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.12931034482758622 , evidence: GPT: association bla\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.27049180327868855 evidence GPT: association pro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2727272727272727 evidence GPT: association bre\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06225032258064517\n",
      "distance to range 2:  0.006634516129032264\n",
      "GPT CLASS PREDICTED =>  ratio: 0.24193548387096775 , evidence: GPT: impact obesity \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2727272727272727 evidence GPT: association non\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32432432432432434 evidence GPT: bidirectional r\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2636363636363636 evidence GPT: association lun\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3508771929824561 evidence GPT: association hep\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.288135593220339 evidence GPT: association gas\n",
      " -------------------------------- \n",
      "distance to range 1:  0.019381052631578966\n",
      "distance to range 2:  0.029271754385964938\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.13157894736842105 , evidence: GPT: relationship pr\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2909090909090909 evidence GPT: association bre\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.275 evidence GPT: association pan\n",
      " -------------------------------- \n",
      "distance to range 1:  0.005202631578947375\n",
      "distance to range 2:  0.03804368421052634\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.17105263157894737 , evidence: GPT: association lun\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32051282051282054 evidence GPT: role inflammati\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34065934065934067 evidence GPT: genetic suscept\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.42696629213483145 evidence GPT: impact cancer t\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3076923076923077 evidence GPT: metabolic alter\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.26373626373626374 evidence GPT: impact cancer c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3617021276595745 evidence GPT: psychosocial fa\n",
      " -------------------------------- \n",
      "distance to range 1:  0.009655652173913065\n",
      "distance to range 2:  0.09639608695652174\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.14130434782608695 , evidence: GPT: impact cancer c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.08685743589743591\n",
      "distance to range 2:  0.06908282051282053\n",
      "GPT CLASS PREDICTED =>  ratio: 0.1794871794871795 , evidence: GPT: relationship in\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06222017543859648\n",
      "distance to range 2:  0.05558754385964915\n",
      "GPT CLASS PREDICTED =>  ratio: 0.19298245614035087 , evidence: GPT: association chr\n",
      " -------------------------------- \n",
      "distance to range 1:  0.049265084745762716\n",
      "distance to range 2:  0.045180169491525424\n",
      "GPT CLASS PREDICTED =>  ratio: 0.2033898305084746 , evidence: GPT: impact chronic \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29069767441860467 evidence GPT: impact cancer c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3026315789473684 evidence GPT: hepatic comorbi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3037974683544304 evidence GPT: link obesity ca\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.26582278481012656 evidence GPT: impact socioeco\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29577464788732394 evidence GPT: immunotherapy c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2631578947368421 evidence GPT: role chronic in\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2604166666666667 evidence GPT: impact cancer s\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.43157894736842106 evidence GPT: targeted therap\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07141454545454547\n",
      "distance to range 2:  0.0326609090909091\n",
      "GPT CLASS PREDICTED =>  ratio: 0.2159090909090909 , evidence: GPT: gut microbiota \n",
      " -------------------------------- \n",
      "distance to range 1:  0.004618536585365873\n",
      "distance to range 2:  0.041252926829268316\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.14634146341463414 , evidence: GPT: impact cancer c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.05418580645161292\n",
      "distance to range 2:  0.033516236559139795\n",
      "GPT CLASS PREDICTED =>  ratio: 0.21505376344086022 , evidence: GPT: impact cancer c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.36893203883495146 evidence GPT: exploring relat\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30392156862745096 evidence GPT: association can\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3584905660377358 evidence GPT: comorbidities l\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3142857142857143 evidence GPT: understanding b\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2857142857142857 evidence GPT: impact cancer r\n",
      " -------------------------------- \n",
      "distance to range 1:  0.024644210526315796\n",
      "distance to range 2:  0.006464736842105273\n",
      "GPT CLASS PREDICTED =>  ratio: 0.24210526315789474 , evidence: GPT: association can\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35454545454545455 evidence GPT: impact cancer n\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3333333333333333 evidence GPT: relationship ca\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32 evidence GPT: comorbidities t\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2621359223300971 evidence GPT: interplay cance\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.37373737373737376 evidence GPT: gastrointestina\n",
      " -------------------------------- \n",
      "---------------COUNTS---------------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  48\n",
      "CHATGPT CLASSIFIED:  43\n",
      "FAILED_TO_CLASSIFY:  0\n",
      "GPT MISCLASSIFIED AS PUBMED:  7\n",
      "PUBMED MISCLASSIFIED AS GPT:  2\n",
      "-------------------------------------------------\n",
      "------------- %PERCENTAGE% -----------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  0.96\n",
      "CHATGPT CLASSIFIED:  0.86\n",
      "FAILED_TO_CLASSIFY:  0.0\n",
      "GPT MISCLASSIFIED AS PUBMED:  0.14\n",
      "PUBMED MISCLASSIFIED AS GPT:  0.04\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# two classes classification\n",
    "\n",
    "two_articles_dataset = []\n",
    "\n",
    "for pubmed_article in pubmed_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('PUBMED: ' + pubmed_article)\n",
    "\n",
    "for gpt_article in gpt_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('GPT: ' + gpt_article)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "chatgpt_class = 0\n",
    "pubmed_class = 0\n",
    "\n",
    "failed_to_classify = 0\n",
    "misclassified_as_gpt = 0\n",
    "misclassified_as_pubmed = 0\n",
    "\n",
    "\n",
    "# RANGE 1: PUBMED\n",
    "range1_start = pubmed_min_value\n",
    "range1_end = pubmed_max_value\n",
    "\n",
    "# RANGE 2: GPT\n",
    "range2_start = gpt_min_value\n",
    "range2_end = gpt_max_value\n",
    "\n",
    "for article in two_articles_dataset:\n",
    "    \n",
    "    gpt_ratio_val    = fit_an_article(article, gpt_lcc)\n",
    "    pubmed_ratio_val = fit_an_article(article, pubmed_lcc)\n",
    "    \n",
    "    # Classifying GPT\n",
    "    if gpt_ratio_val >= range2_start and ratio_val <= range2_end :       \n",
    "        if article[:20].startswith('GPT'):\n",
    "            chatgpt_class+=1\n",
    "            print('ChatGPT : Fit ratio for individual articles: ', gpt_ratio_val, 'evidence', article[:20])\n",
    "        else:\n",
    "            misclassified_as_pubmed+=1\n",
    "            \n",
    "    # Classifying PUBMED\n",
    "    elif pubmed_ratio_val >= range1_start and ratio_val <= range1_end:\n",
    "        if article[:20].startswith('PUBMED'):\n",
    "            pubmed_class += 1\n",
    "            print('PUBMED : Fit ratio for individual articles: ', pubmed_ratio_val, 'evidence', article[:20])\n",
    "        else: \n",
    "            misclassified_as_gpt+=1\n",
    "        \n",
    "    else:\n",
    "        # Calculate distances\n",
    "        distance_to_range1 = distance_to_range(pubmed_ratio_val, range1_start, range1_end)\n",
    "        distance_to_range2 = distance_to_range(gpt_ratio_val, range2_start, range2_end) \n",
    "        \n",
    "        print('distance to range 1: ', distance_to_range1)\n",
    "        print('distance to range 2: ', distance_to_range2)        \n",
    "        \n",
    "        # RANGE 1: PUBMED SHOULD WIN\n",
    "        if distance_to_range1 < distance_to_range2:\n",
    "            if article[:20].startswith('GPT'):\n",
    "                misclassified_as_gpt+=1\n",
    "                print('PUBMED PREDICTED INCORRECTLY => ', 'ratio:', pubmed_ratio_val ,', evidence:', article[:20])                \n",
    "            else:   \n",
    "                # count+=1\n",
    "                pubmed_class += 1\n",
    "                print('PUBMED CLASS PREDICTED => ', 'ratio:', pubmed_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "        # RANGE 2: GPT SHOULD WIN\n",
    "        elif distance_to_range2 < distance_to_range1:\n",
    "            if article[:20].startswith('PUBMED'):                \n",
    "                misclassified_as_pubmed+=1\n",
    "                print('GPT PREDICTED INCORRECTLY => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])                     \n",
    "            else:\n",
    "                chatgpt_class += 1\n",
    "                print('GPT CLASS PREDICTED => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "    print(' -------------------------------- ')\n",
    "    \n",
    "    \n",
    "print('---------------COUNTS---------------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed) \n",
    "print('-------------------------------------------------') \n",
    "    \n",
    "    \n",
    "print('------------- %PERCENTAGE% -----------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class/50)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class/50)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify/50)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt/50)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed/50) \n",
    "print('-------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698f44f-4493-4cd1-a627-e7096b3cd336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67681f37-ab2d-4490-ade6-bad39c1ea9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e4d62-59ee-4ede-9fcd-3631b465e9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351fff5-0186-43a8-acb2-e4fd65adfed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d5d63-88a4-4bdb-b58c-a1191c25c0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42211b7d-0c98-4c89-81ae-d1ec25637e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e3bb0-e92f-4efa-884a-2adaf0957bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5e5dc-4150-4959-ac17-9efe826b462e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
