{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ee5f81f7-c915-4754-9a11-e4042245f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements \n",
    "import json\n",
    "import obonet\n",
    "from itertools import combinations \n",
    "from Bio import Medline\n",
    "import networkx as nx\n",
    "import string\n",
    "from textblob import TextBlob  \n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import tree\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "63974b6d-8293-4445-ab3a-439a2609ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment configurations\n",
    "# MAX_NUMBER_BIGRAMS = 30\n",
    "MAX_NUMBER_ARTICLE = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8ce8bc1a-8191-4147-8c82-f02f98486f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing a medline file \n",
    "def parse_medline_rmap(medline_file):    \n",
    "    map_abstracts = {}    \n",
    "    pmid = ''\n",
    "    abstract = ''  \n",
    "    with open(medline_file) as medline_handle:\n",
    "        records = Medline.parse(medline_handle)\n",
    "        for record in records:         \n",
    "            keys = record.keys()            \n",
    "            if 'PMID' in keys and 'AB' in keys: \n",
    "\n",
    "                pmid = record['PMID']\n",
    "                abstract = record['AB']\n",
    "                \n",
    "                map_abstracts[pmid] = abstract.lower()\n",
    "    return map_abstracts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "85f0e020-a6bb-4b94-b88d-b7500d99de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_gpt_api_data(json_file):\n",
    "\n",
    "    json_records_map = {}\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Now json_data is a list of dictionaries, each representing an item in the array\n",
    "    for item in json_data:\n",
    "        gpt_id = item['GPT-ID']\n",
    "        title = item['Title']\n",
    "        abstract = item['Abstract']\n",
    "        # json_records_map[gpt_id]=(title + \" \" + abstract)\n",
    "        json_records_map[gpt_id]=(title + \" \" + abstract)        \n",
    "    return json_records_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "13080a90-1415-4f9e-ae19-a183512273b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "      \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e603b3a6-895e-4887-9d4f-611ce9a333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_abstracts = parse_medline_rmap('../dataset/pubmed-depression-set-2010-2014.txt')\n",
    "cgpt_abstracts = parse_json_gpt_api_data('../dataset/depr-gpt-apis.txt')\n",
    "\n",
    "# cleaning PubMed articles from special characters\n",
    "clean_pubmed_articles = []\n",
    "for abst in list(pubmed_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_pubmed_articles.append(cleaned)\n",
    "    \n",
    "# cleaning chatGPT articles from special characters\n",
    "clean_chatGPT_articles = []\n",
    "for abst in list(cgpt_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_chatGPT_articles.append(cleaned)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b71e1951-6355-4495-96f3-f11c6de17793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_pubmed_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7cd597f6-5b64-465a-940a-22512d16f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "special_list = ['abstract']\n",
    "\n",
    "def stopwords_rem_pubmed(clean_pubmed_training):\n",
    "    stopped_pubmed_training = []\n",
    "    for abst in clean_pubmed_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if token not in stop_words:\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_pubmed_training.append(valid_rec)\n",
    "    return stopped_pubmed_training\n",
    "    \n",
    "    \n",
    "def stopwords_rem_chatGPT_dataset(clean_chatGPT):    \n",
    "    stopped_chatGPT_training = []\n",
    "    for abst in clean_chatGPT_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if (token not in stop_words) and (token not in special_list):\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_chatGPT_training.append(valid_rec)   \n",
    "    return stopped_chatGPT_training\n",
    "\n",
    "\n",
    "def stopwords_rem_chatGPT_article(clean_chatGPT_article):    \n",
    "    stopped_chatGPT_training = []\n",
    "    valid_l = []\n",
    "    valid_rec = []\n",
    "    blob_object = TextBlob(clean_chatGPT_article)\n",
    "    list_tokens = blob_object.words\n",
    "\n",
    "    for token in list_tokens:        \n",
    "        if (token not in stop_words) and (token not in special_list):\n",
    "            valid_l.append(token)            \n",
    "    valid_rec = ' '.join(valid_l)\n",
    "    # stopped_chatGPT_training.append(valid_rec)   \n",
    "    return str(valid_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "756e0025-68d5-4820-afbb-3c220042d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506\n"
     ]
    }
   ],
   "source": [
    "pubmed_articles_ready = stopwords_rem_pubmed(clean_pubmed_articles)\n",
    "\n",
    "# print(len(stopped_pubmed_training))  \n",
    "gpt_articles_ready = []\n",
    "for article in clean_chatGPT_articles:\n",
    "    gpt_articles_ready.append(stopwords_rem_chatGPT_article(article))\n",
    "print(len(gpt_articles_ready))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8345cb7a-ac87-4af6-aafe-5fb4d2babe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pubmed_articles_ready[0])\n",
    "# print('-----')\n",
    "# print(gpt_articles_ready[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d5ffe92e-af59-4d04-b6c3-9c318836e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting PubMed bigrams\n",
    "def compute_bigrams(training_articles):\n",
    "    list_bigrams = []\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range =(2, 2))\n",
    "    X1 = vectorizer.fit_transform(training_articles)\n",
    "    features = (vectorizer.get_feature_names_out())\n",
    "    # print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "    # Applying TFIDF\n",
    "    # You can still get n-grams here\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "    X2 = vectorizer.fit_transform(training_articles)\n",
    "    scores = (X2.toarray())\n",
    "    # print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "    # Getting top ranking features\n",
    "    sums = X2.sum(axis = 0)\n",
    "    data1 = []\n",
    "    for col, term in enumerate(features):\n",
    "        data1.append( (term, sums[0, col] ))\n",
    "    ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "    words = (ranking.sort_values('rank', ascending = False))\n",
    "\n",
    "    bigram_ranks = {}\n",
    "    for index, row in words.iterrows():\n",
    "        # print(row['term'],'\\t\\t\\t',  row['rank'])\n",
    "\n",
    "        splits = row['term'].split()\n",
    "        bigram_ranks[row['rank']] = (splits[0], splits[1])\n",
    "\n",
    "    count = 0    \n",
    "    for k, v in bigram_ranks.items():\n",
    "        # if count < MAX_NUMBER_BIGRAMS:\n",
    "        #     # print(k,'\\t',  v)\n",
    "        #     count += 1\n",
    "        list_bigrams.append(v)\n",
    "    return bigram_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7ff019e0-a5ba-4c93-b9ba-62f7b9065699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_model(training_articles):\n",
    "    bigrams_map_training = compute_bigrams(training_articles)\n",
    "    gpt_training_bigrams = bigrams_map_training.values()\n",
    "    \n",
    "    graph_training_model = nx.Graph()\n",
    "    graph_training_model.add_edges_from(list(gpt_training_bigrams))\n",
    "    \n",
    "    return graph_training_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ded8c1c2-3712-48c4-803c-7f828e9b62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT Training Model --------\n",
      "Original node count:  577\n",
      "Original edge count:  1108\n",
      " -------- PubMed Training Model --------\n",
      "Original node count:  802\n",
      "Original edge count:  878\n"
     ]
    }
   ],
   "source": [
    "# construct a network training model from both datasets (gpt and pubmed)\n",
    "\n",
    "gpt_training_model = construct_training_model(gpt_articles_ready[:100])\n",
    "pubmed_training_model = construct_training_model(pubmed_articles_ready[:100])\n",
    "\n",
    "# ----------   Verifying GPT Training  Model ----------# \n",
    "print(' -------- GPT Training Model --------')\n",
    "node_count = len(gpt_training_model.nodes())\n",
    "edge_count = len(gpt_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)\n",
    "\n",
    "# ----------   Verifying PubMed Training  Model ----------# \n",
    "print(' -------- PubMed Training Model --------')\n",
    "node_count = len(pubmed_training_model.nodes())\n",
    "edge_count = len(pubmed_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5ffffcb0-d548-4376-96d7-9354472b35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giant_lcc(graph_training_model):\n",
    "    gcc = sorted(nx.connected_components(graph_training_model), key=len, reverse=True)\n",
    "    giant_cc = graph_training_model.subgraph(gcc[0])\n",
    "    return giant_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b7da8e22-e4a9-4973-922d-bd411fa4ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT GIANT LCC Graph --------\n",
      "Graph with 494 nodes and 1061 edges\n",
      " -------- PUBMED GIANT LCC Graph --------\n",
      "Graph with 558 nodes and 743 edges\n"
     ]
    }
   ],
   "source": [
    "print(' -------- GPT GIANT LCC Graph --------')\n",
    "gpt_lcc = get_giant_lcc(gpt_training_model)\n",
    "print(gpt_lcc)\n",
    "\n",
    "print(' -------- PUBMED GIANT LCC Graph --------')\n",
    "pubmed_lcc = get_giant_lcc(pubmed_training_model)\n",
    "print(pubmed_lcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0950208d-636b-4784-b0a0-c1b8f4561eae",
   "metadata": {},
   "outputs": [],
   "source": [
    " # STEP2: -- compute individual articles bigrams -------\n",
    "def calibrate_model(ds_label, begin_index, end_index, training_graph, calibrate_set):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy() \n",
    "\n",
    "    ratios_added_per_fold = []\n",
    "    for abst in calibrate_set[begin_index:end_index]:\n",
    "        \n",
    "        tokens = nltk.word_tokenize(abst)\n",
    "\n",
    "        # compute the bigrams\n",
    "        bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "        # -------  check if the giant has the bigram components, add new edge \n",
    "        # -------          otherwise, don't add new edges\n",
    "        # -------  count how many nodes            \n",
    "        count = 0\n",
    "        added_edges = []\n",
    "        for bigram in bigrams:\n",
    "\n",
    "            if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "                if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                    training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                    count += 1\n",
    "                    added_edges.append((bigram[0], bigram[1]))\n",
    "        ratio_ = count / len(tokens)        \n",
    "        \n",
    "        ratios_added_per_fold.append(ratio_) \n",
    "        \n",
    "        training_graph_copy.remove_edges_from(added_edges)      \n",
    "    return ratios_added_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bd014f6c-9a92-4e85-a279-78a8258b941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(tst_set_list):\n",
    "    average = sum(tst_set_list) / len(tst_set_list)        \n",
    "    formatted_avg = float(\"{:.5f}\".format(average))        \n",
    "    return formatted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "df86f04a-4d5e-4535-b810-1cadc3bb0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28825\n",
      "0.30136\n",
      "0.3247\n",
      "0.27978\n",
      "0.30504\n",
      "0.30281\n",
      "0.29469\n",
      "0.28088\n",
      "0.27455\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "gpt_means = []\n",
    "for index in range(100,1000):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, gpt_lcc, gpt_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_g = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_g)\n",
    "        gpt_means.append(tst_mean_g)\n",
    "        \n",
    "gpt_min_value = min(gpt_means)\n",
    "gpt_max_value = max(gpt_means) \n",
    "# print(gpt_means)\n",
    "for ratio in gpt_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d67982e6-5346-48f0-8317-96fb6c705464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1032\n",
      "0.11699\n",
      "0.11136\n",
      "0.11471\n",
      "0.11841\n",
      "0.11809\n",
      "0.1067\n",
      "0.11824\n",
      "0.10409\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pubmed_means = []\n",
    "for index in range(100,1000):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, pubmed_lcc, pubmed_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_p = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_p)\n",
    "        pubmed_means.append(tst_mean_p)\n",
    "        \n",
    "pubmed_min_value = min(pubmed_means)\n",
    "pubmed_max_value = max(pubmed_means) \n",
    "# print(gpt_means)\n",
    "for ratio in pubmed_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ed318152-06ef-4e6e-8a70-aba811af55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_an_article(article_text, training_graph):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy()\n",
    "    \n",
    "    # chat_no_added_edges = []\n",
    "    # for abst in stopped_pubmed_training[begin_index:end_index]:\n",
    "\n",
    "    tokens = nltk.word_tokenize(article_text)\n",
    "\n",
    "    # compute the bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # -------  check if the giant has the bigram components, add new edge \n",
    "    # -------          otherwise, don't add new edges\n",
    "    # -------  count how many nodes    \n",
    "\n",
    "    count = 0\n",
    "    added_edges = []\n",
    "    for bigram in bigrams:\n",
    "\n",
    "        if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "            if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                count += 1\n",
    "                added_edges.append((bigram[0], bigram[1]))\n",
    "    ratio_ = count / len(tokens)        \n",
    "    training_graph_copy.remove_edges_from(added_edges)\n",
    "        \n",
    "    return ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f5000d5a-e4bf-4940-a3ec-7a68f557d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.0\n",
      "CORRECT CLASSIFIED:  1.0\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "\n",
    "\n",
    "for article in gpt_articles_ready[200:1000]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= pubmed_min_value and ratio_val <= pubmed_max_value:\n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED CLASS: Fit ratio for individual articles: ', ratio_val, 'evidence', article[:20])                \n",
    "        \n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val,'evidence', article[:20])\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/800)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/800)   \n",
    "print('-------------------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "cc6b890e-7be7-4214-ab38-809eae6018b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.06125\n",
      "CORRECT CLASSIFIED:  0.93875\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pubmed_min_value = min(pubmed_means)\n",
    "# pubmed_max_value = max(pubmed_means)\n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in pubmed_articles_ready[200:1000]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= pubmed_min_value and ratio_val <= pubmed_max_value:       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/800)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/800)   \n",
    "print('-------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "71150a2b-3b27-4372-9bde-d8543229eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_range(point, range_start, range_end):\n",
    "    # Calculate the distance to the nearest endpoint of the range\n",
    "    distance = min(abs(point - range_start), abs(point - range_end))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9efd5d03-90a9-47ae-a4d9-806c9cb44e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance to range 1:  0.007455319148936171\n",
      "distance to range 2:  0.22667765957446812\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09574468085106383 , evidence: PUBMED: background w\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1509433962264151 evidence PUBMED: objectives a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0313437125748503\n",
      "distance to range 2:  0.22065778443113773\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0718562874251497 , evidence: PUBMED: context prev\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.26 evidence PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.037779439252336455\n",
      "distance to range 2:  0.1997836448598131\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06542056074766354 , evidence: PUBMED: aim study as\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.12878787878787878 evidence PUBMED: psychologica\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1076923076923077 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.09367619047619047\n",
      "distance to range 2:  0.26502619047619047\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.009523809523809525 , evidence: PUBMED: spontaneous \n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.20967741935483872 evidence PUBMED: objectives c\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13333333333333333 evidence PUBMED: background s\n",
      " -------------------------------- \n",
      "distance to range 1:  0.023729801324503308\n",
      "distance to range 2:  0.23481490066225166\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07947019867549669 , evidence: PUBMED: aims many st\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.176056338028169 evidence PUBMED: objective ex\n",
      " -------------------------------- \n",
      "distance to range 1:  0.006609090909090912\n",
      "distance to range 2:  0.22341363636363637\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09659090909090909 , evidence: PUBMED: background e\n",
      " -------------------------------- \n",
      "distance to range 1:  0.001848648648648643\n",
      "distance to range 2:  0.19346891891891893\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10135135135135136 , evidence: PUBMED: background c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03482393162393162\n",
      "distance to range 2:  0.22326794871794875\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06837606837606838 , evidence: PUBMED: purpose purp\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.18333333333333332 evidence PUBMED: background m\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1414141414141414 evidence PUBMED: recent findi\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.11458333333333333 evidence PUBMED: background c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.08911549295774648\n",
      "distance to range 2:  0.2393387323943662\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.014084507042253521 , evidence: PUBMED: glutamatergi\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02035976331360946\n",
      "distance to range 2:  0.23312988165680476\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08284023668639054 , evidence: PUBMED: depression i\n",
      " -------------------------------- \n",
      "distance to range 1:  0.047255944055944056\n",
      "distance to range 2:  0.253570979020979\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.055944055944055944 , evidence: PUBMED: kounis syndr\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15384615384615385 evidence PUBMED: background c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02912592592592593\n",
      "distance to range 2:  0.1634388888888889\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07407407407407407 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0011591836734693856\n",
      "distance to range 2:  0.19291734693877555\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10204081632653061 , evidence: PUBMED: dysphagia di\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02744242424242424\n",
      "distance to range 2:  0.22909545454545455\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07575757575757576 , evidence: PUBMED: background m\n",
      " -------------------------------- \n",
      "distance to range 1:  0.08174077253218884\n",
      "distance to range 2:  0.25309077253218887\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.02145922746781116 , evidence: PUBMED: endotoxin to\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07041311475409837\n",
      "distance to range 2:  0.2499598360655738\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.03278688524590164 , evidence: PUBMED: two experime\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1388888888888889 evidence PUBMED: background l\n",
      " -------------------------------- \n",
      "distance to range 1:  0.061533333333333336\n",
      "distance to range 2:  0.22097857142857144\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.041666666666666664 , evidence: PUBMED: objective co\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24528301886792453 evidence PUBMED: objective re\n",
      " -------------------------------- \n",
      "distance to range 1:  0.067055421686747\n",
      "distance to range 2:  0.2504536144578313\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.03614457831325301 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13432835820895522 evidence PUBMED: objective de\n",
      " -------------------------------- \n",
      "distance to range 1:  0.015768306010928962\n",
      "distance to range 2:  0.20897622950819672\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08743169398907104 , evidence: PUBMED: comorbidity \n",
      " -------------------------------- \n",
      "distance to range 1:  0.005261855670103094\n",
      "distance to range 2:  0.19723041237113403\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0979381443298969 , evidence: PUBMED: article aims\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13978494623655913 evidence PUBMED: current term\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16279069767441862 evidence PUBMED: sought recon\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1487603305785124 evidence PUBMED: background s\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13761467889908258 evidence PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06449032258064516\n",
      "distance to range 2:  0.22938870967741937\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.03870967741935484 , evidence: PUBMED: spasticity o\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04764444444444445\n",
      "distance to range 2:  0.14955000000000002\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.05555555555555555 , evidence: PUBMED: study undert\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1111111111111111 evidence PUBMED: aim autonomi\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04587515923566879\n",
      "distance to range 2:  0.23633343949044588\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.05732484076433121 , evidence: PUBMED: prior invest\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.10967741935483871 evidence PUBMED: objective ai\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07489811320754716\n",
      "distance to range 2:  0.2462481132075472\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.02830188679245283 , evidence: PUBMED: perinatal is\n",
      " -------------------------------- \n",
      "distance to range 1:  0.029336363636363633\n",
      "distance to range 2:  0.17227727272727272\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07386363636363637 , evidence: PUBMED: depressive s\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02887567567567567\n",
      "distance to range 2:  0.22049594594594596\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07432432432432433 , evidence: PUBMED: background n\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1565217391304348 evidence PUBMED: aims compari\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17647058823529413 evidence PUBMED: aims influen\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13333333333333333 evidence PUBMED: background l\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13333333333333333 evidence PUBMED: background c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2987012987012987 evidence GPT: depression asso\n",
      " -------------------------------- \n",
      "distance to range 1:  0.014964705882352936\n",
      "distance to range 2:  0.039255882352941196\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.08823529411764706 , evidence: GPT: depression rela\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2876712328767123 evidence GPT: depression como\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32051282051282054 evidence GPT: depression asso\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.325 evidence GPT: association dep\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.336734693877551 evidence GPT: depression diab\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.27472527472527475 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      "distance to range 1:  0.027931182795698922\n",
      "distance to range 2:  0.005732795698924764\n",
      "GPT CLASS PREDICTED =>  ratio: 0.26881720430107525 , evidence: GPT: impact depressi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2765957446808511 evidence GPT: depression anxi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29411764705882354 evidence GPT: depression obes\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2916666666666667 evidence GPT: depression subs\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32608695652173914 evidence GPT: depression slee\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3673469387755102 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      "distance to range 1:  0.018093617021276598\n",
      "distance to range 2:  0.10433723404255321\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.0851063829787234 , evidence: GPT: gutbrain axis e\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2777777777777778 evidence GPT: depression chil\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.33980582524271846 evidence GPT: depression post\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3829787234042553 evidence GPT: depression schi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.37777777777777777 evidence GPT: depression coro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29545454545454547 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35 evidence GPT: depression elde\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32673267326732675 evidence GPT: depression rheu\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3103448275862069 evidence GPT: depression canc\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3956043956043956 evidence GPT: depression alzh\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.38095238095238093 evidence GPT: depression atte\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.36046511627906974 evidence GPT: depression epil\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3157894736842105 evidence GPT: depression asso\n",
      " -------------------------------- \n",
      "distance to range 1:  0.022391919191919185\n",
      "distance to range 2:  0.032125757575757585\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.08080808080808081 , evidence: GPT: relationship de\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3125 evidence GPT: depression diab\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3191489361702128 evidence GPT: depression obes\n",
      " -------------------------------- \n",
      "distance to range 1:  0.036533333333333334\n",
      "distance to range 2:  0.052327777777777806\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.06666666666666667 , evidence: GPT: depression subs\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.31521739130434784 evidence GPT: depression slee\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.42528735632183906 evidence GPT: depression card\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3409090909090909 evidence GPT: depression alzh\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3058823529411765 evidence GPT: depression post\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.275 evidence GPT: depression rheu\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2911392405063291 evidence GPT: depression schi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3375 evidence GPT: depression atte\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2875 evidence GPT: depression migr\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29411764705882354 evidence GPT: depression eati\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2826086956521739 evidence GPT: depression post\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30666666666666664 evidence GPT: depression subs\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2804878048780488 evidence GPT: depression anxi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2891566265060241 evidence GPT: depression subs\n",
      " -------------------------------- \n",
      "---------------COUNTS---------------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  50\n",
      "CHATGPT CLASSIFIED:  40\n",
      "FAILED_TO_CLASSIFY:  0\n",
      "GPT MISCLASSIFIED AS PUBMED:  10\n",
      "PUBMED MISCLASSIFIED AS GPT:  0\n",
      "-------------------------------------------------\n",
      "------------- %PERCENTAGE% -----------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  1.0\n",
      "CHATGPT CLASSIFIED:  0.8\n",
      "FAILED_TO_CLASSIFY:  0.0\n",
      "GPT MISCLASSIFIED AS PUBMED:  0.2\n",
      "PUBMED MISCLASSIFIED AS GPT:  0.0\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# two classes classification\n",
    "\n",
    "two_articles_dataset = []\n",
    "\n",
    "for pubmed_article in pubmed_articles_ready[1200:1250]:\n",
    "    two_articles_dataset.append('PUBMED: ' + pubmed_article)\n",
    "\n",
    "for gpt_article in gpt_articles_ready[1200:1250]:\n",
    "    two_articles_dataset.append('GPT: ' + gpt_article)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "chatgpt_class = 0\n",
    "pubmed_class = 0\n",
    "\n",
    "failed_to_classify = 0\n",
    "misclassified_as_gpt = 0\n",
    "misclassified_as_pubmed = 0\n",
    "\n",
    "\n",
    "# RANGE 1: PUBMED\n",
    "range1_start = pubmed_min_value\n",
    "range1_end = pubmed_max_value\n",
    "\n",
    "# RANGE 2: GPT\n",
    "range2_start = gpt_min_value\n",
    "range2_end = gpt_max_value\n",
    "\n",
    "for article in two_articles_dataset:\n",
    "    \n",
    "    gpt_ratio_val    = fit_an_article(article, gpt_lcc)\n",
    "    pubmed_ratio_val = fit_an_article(article, pubmed_lcc)\n",
    "    \n",
    "    # Classifying GPT\n",
    "    if gpt_ratio_val >= range2_start and ratio_val <= range2_end :       \n",
    "        if article[:20].startswith('GPT'):\n",
    "            chatgpt_class+=1\n",
    "            print('ChatGPT : Fit ratio for individual articles: ', gpt_ratio_val, 'evidence', article[:20])\n",
    "        else:\n",
    "            misclassified_as_pubmed+=1\n",
    "            \n",
    "    # Classifying PUBMED\n",
    "    elif pubmed_ratio_val >= range1_start and ratio_val <= range1_end:\n",
    "        if article[:20].startswith('PUBMED'):\n",
    "            pubmed_class += 1\n",
    "            print('PUBMED : Fit ratio for individual articles: ', pubmed_ratio_val, 'evidence', article[:20])\n",
    "        else: \n",
    "            misclassified_as_gpt+=1\n",
    "        \n",
    "    else:\n",
    "        # Calculate distances\n",
    "        distance_to_range1 = distance_to_range(pubmed_ratio_val, range1_start, range1_end)\n",
    "        distance_to_range2 = distance_to_range(gpt_ratio_val, range2_start, range2_end) \n",
    "        \n",
    "        print('distance to range 1: ', distance_to_range1)\n",
    "        print('distance to range 2: ', distance_to_range2)        \n",
    "        \n",
    "        # RANGE 1: PUBMED SHOULD WIN\n",
    "        if distance_to_range1 < distance_to_range2:\n",
    "            if article[:20].startswith('GPT'):\n",
    "                misclassified_as_gpt+=1\n",
    "                print('PUBMED PREDICTED INCORRECTLY => ', 'ratio:', pubmed_ratio_val ,', evidence:', article[:20])                \n",
    "            else:   \n",
    "                # count+=1\n",
    "                pubmed_class += 1\n",
    "                print('PUBMED CLASS PREDICTED => ', 'ratio:', pubmed_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "        # RANGE 2: GPT SHOULD WIN\n",
    "        elif distance_to_range2 < distance_to_range1:\n",
    "            if article[:20].startswith('PUBMED'):                \n",
    "                misclassified_as_pubmed+=1\n",
    "                print('GPT PREDICTED INCORRECTLY => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])                     \n",
    "            else:\n",
    "                chatgpt_class += 1\n",
    "                print('GPT CLASS PREDICTED => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "    print(' -------------------------------- ')\n",
    "    \n",
    "    \n",
    "print('---------------COUNTS---------------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed) \n",
    "print('-------------------------------------------------') \n",
    "    \n",
    "    \n",
    "print('------------- %PERCENTAGE% -----------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class/50)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class/50)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify/50)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt/50)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed/50) \n",
    "print('-------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea69e1-5e4a-49fa-b829-61be33611c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbdfd9-0b72-4fa6-9198-fdee33c889a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f3743-923e-43f5-8fdd-c5ec59faf21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40bbb20-dae6-4e1c-90a3-d9c7979d3b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e03ac8-ed29-4423-95b9-c8a1ad9d2953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
