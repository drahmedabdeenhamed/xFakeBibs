{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ee5f81f7-c915-4754-9a11-e4042245f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements \n",
    "import json\n",
    "import obonet\n",
    "from itertools import combinations \n",
    "from Bio import Medline\n",
    "import networkx as nx\n",
    "import string\n",
    "from textblob import TextBlob  \n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import tree\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "63974b6d-8293-4445-ab3a-439a2609ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment configurations\n",
    "# MAX_NUMBER_BIGRAMS = 30\n",
    "MAX_NUMBER_ARTICLE = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8ce8bc1a-8191-4147-8c82-f02f98486f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing a medline file \n",
    "def parse_medline_rmap(medline_file):    \n",
    "    map_abstracts = {}    \n",
    "    pmid = ''\n",
    "    abstract = ''  \n",
    "    with open(medline_file) as medline_handle:\n",
    "        records = Medline.parse(medline_handle)\n",
    "        for record in records:         \n",
    "            keys = record.keys()            \n",
    "            if 'PMID' in keys and 'AB' in keys: \n",
    "\n",
    "                pmid = record['PMID']\n",
    "                abstract = record['AB']\n",
    "                \n",
    "                map_abstracts[pmid] = abstract.lower()\n",
    "    return map_abstracts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "85f0e020-a6bb-4b94-b88d-b7500d99de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_gpt_api_data(json_file):\n",
    "\n",
    "    json_records_map = {}\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Now json_data is a list of dictionaries, each representing an item in the array\n",
    "    for item in json_data:\n",
    "        gpt_id = item['GPT-ID']\n",
    "        title = item['Title']\n",
    "        abstract = item['Abstract']\n",
    "        # json_records_map[gpt_id]=(title + \" \" + abstract)\n",
    "        json_records_map[gpt_id]=(title + \" \" + abstract)        \n",
    "    return json_records_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "13080a90-1415-4f9e-ae19-a183512273b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "      \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e603b3a6-895e-4887-9d4f-611ce9a333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_abstracts = parse_medline_rmap('../dataset/pubmed-depression-set-2020-2024-present.txt')\n",
    "cgpt_abstracts = parse_json_gpt_api_data('../dataset/depr-gpt-apis.txt')\n",
    "\n",
    "# cleaning PubMed articles from special characters\n",
    "clean_pubmed_articles = []\n",
    "for abst in list(pubmed_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_pubmed_articles.append(cleaned)\n",
    "    \n",
    "# cleaning chatGPT articles from special characters\n",
    "clean_chatGPT_articles = []\n",
    "for abst in list(cgpt_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_chatGPT_articles.append(cleaned)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b71e1951-6355-4495-96f3-f11c6de17793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_pubmed_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7cd597f6-5b64-465a-940a-22512d16f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "special_list = ['abstract']\n",
    "\n",
    "def stopwords_rem_pubmed(clean_pubmed_training):\n",
    "    stopped_pubmed_training = []\n",
    "    for abst in clean_pubmed_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if token not in stop_words:\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_pubmed_training.append(valid_rec)\n",
    "    return stopped_pubmed_training\n",
    "    \n",
    "    \n",
    "def stopwords_rem_chatGPT_dataset(clean_chatGPT):    \n",
    "    stopped_chatGPT_training = []\n",
    "    for abst in clean_chatGPT_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if (token not in stop_words) and (token not in special_list):\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_chatGPT_training.append(valid_rec)   \n",
    "    return stopped_chatGPT_training\n",
    "\n",
    "\n",
    "def stopwords_rem_chatGPT_article(clean_chatGPT_article):    \n",
    "    stopped_chatGPT_training = []\n",
    "    valid_l = []\n",
    "    valid_rec = []\n",
    "    blob_object = TextBlob(clean_chatGPT_article)\n",
    "    list_tokens = blob_object.words\n",
    "\n",
    "    for token in list_tokens:        \n",
    "        if (token not in stop_words) and (token not in special_list):\n",
    "            valid_l.append(token)            \n",
    "    valid_rec = ' '.join(valid_l)\n",
    "    # stopped_chatGPT_training.append(valid_rec)   \n",
    "    return str(valid_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "756e0025-68d5-4820-afbb-3c220042d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506\n"
     ]
    }
   ],
   "source": [
    "pubmed_articles_ready = stopwords_rem_pubmed(clean_pubmed_articles)\n",
    "\n",
    "# print(len(stopped_pubmed_training))  \n",
    "gpt_articles_ready = []\n",
    "for article in clean_chatGPT_articles:\n",
    "    gpt_articles_ready.append(stopwords_rem_chatGPT_article(article))\n",
    "print(len(gpt_articles_ready))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8345cb7a-ac87-4af6-aafe-5fb4d2babe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pubmed_articles_ready[0])\n",
    "# print('-----')\n",
    "# print(gpt_articles_ready[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d5ffe92e-af59-4d04-b6c3-9c318836e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting PubMed bigrams\n",
    "def compute_bigrams(training_articles):\n",
    "    list_bigrams = []\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range =(2, 2))\n",
    "    X1 = vectorizer.fit_transform(training_articles)\n",
    "    features = (vectorizer.get_feature_names_out())\n",
    "    # print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "    # Applying TFIDF\n",
    "    # You can still get n-grams here\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "    X2 = vectorizer.fit_transform(training_articles)\n",
    "    scores = (X2.toarray())\n",
    "    # print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "    # Getting top ranking features\n",
    "    sums = X2.sum(axis = 0)\n",
    "    data1 = []\n",
    "    for col, term in enumerate(features):\n",
    "        data1.append( (term, sums[0, col] ))\n",
    "    ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "    words = (ranking.sort_values('rank', ascending = False))\n",
    "\n",
    "    bigram_ranks = {}\n",
    "    for index, row in words.iterrows():\n",
    "        # print(row['term'],'\\t\\t\\t',  row['rank'])\n",
    "\n",
    "        splits = row['term'].split()\n",
    "        bigram_ranks[row['rank']] = (splits[0], splits[1])\n",
    "\n",
    "    count = 0    \n",
    "    for k, v in bigram_ranks.items():\n",
    "        # if count < MAX_NUMBER_BIGRAMS:\n",
    "        #     # print(k,'\\t',  v)\n",
    "        #     count += 1\n",
    "        list_bigrams.append(v)\n",
    "    return bigram_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7ff019e0-a5ba-4c93-b9ba-62f7b9065699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_model(training_articles):\n",
    "    bigrams_map_training = compute_bigrams(training_articles)\n",
    "    gpt_training_bigrams = bigrams_map_training.values()\n",
    "    \n",
    "    graph_training_model = nx.Graph()\n",
    "    graph_training_model.add_edges_from(list(gpt_training_bigrams))\n",
    "    \n",
    "    return graph_training_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ded8c1c2-3712-48c4-803c-7f828e9b62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT Training Model --------\n",
      "Original node count:  577\n",
      "Original edge count:  1108\n",
      " -------- PubMed Training Model --------\n",
      "Original node count:  790\n",
      "Original edge count:  809\n"
     ]
    }
   ],
   "source": [
    "# construct a network training model from both datasets (gpt and pubmed)\n",
    "\n",
    "gpt_training_model = construct_training_model(gpt_articles_ready[:100])\n",
    "pubmed_training_model = construct_training_model(pubmed_articles_ready[:100])\n",
    "\n",
    "# ----------   Verifying GPT Training  Model ----------# \n",
    "print(' -------- GPT Training Model --------')\n",
    "node_count = len(gpt_training_model.nodes())\n",
    "edge_count = len(gpt_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)\n",
    "\n",
    "# ----------   Verifying PubMed Training  Model ----------# \n",
    "print(' -------- PubMed Training Model --------')\n",
    "node_count = len(pubmed_training_model.nodes())\n",
    "edge_count = len(pubmed_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5ffffcb0-d548-4376-96d7-9354472b35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giant_lcc(graph_training_model):\n",
    "    gcc = sorted(nx.connected_components(graph_training_model), key=len, reverse=True)\n",
    "    giant_cc = graph_training_model.subgraph(gcc[0])\n",
    "    return giant_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b7da8e22-e4a9-4973-922d-bd411fa4ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT GIANT LCC Graph --------\n",
      "Graph with 494 nodes and 1061 edges\n",
      " -------- PUBMED GIANT LCC Graph --------\n",
      "Graph with 496 nodes and 643 edges\n"
     ]
    }
   ],
   "source": [
    "print(' -------- GPT GIANT LCC Graph --------')\n",
    "gpt_lcc = get_giant_lcc(gpt_training_model)\n",
    "print(gpt_lcc)\n",
    "\n",
    "print(' -------- PUBMED GIANT LCC Graph --------')\n",
    "pubmed_lcc = get_giant_lcc(pubmed_training_model)\n",
    "print(pubmed_lcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0950208d-636b-4784-b0a0-c1b8f4561eae",
   "metadata": {},
   "outputs": [],
   "source": [
    " # STEP2: -- compute individual articles bigrams -------\n",
    "def calibrate_model(ds_label, begin_index, end_index, training_graph, calibrate_set):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy() \n",
    "\n",
    "    ratios_added_per_fold = []\n",
    "    for abst in calibrate_set[begin_index:end_index]:\n",
    "        \n",
    "        tokens = nltk.word_tokenize(abst)\n",
    "\n",
    "        # compute the bigrams\n",
    "        bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "        # -------  check if the giant has the bigram components, add new edge \n",
    "        # -------          otherwise, don't add new edges\n",
    "        # -------  count how many nodes            \n",
    "        count = 0\n",
    "        added_edges = []\n",
    "        for bigram in bigrams:\n",
    "\n",
    "            if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "                if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                    training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                    count += 1\n",
    "                    added_edges.append((bigram[0], bigram[1]))\n",
    "        ratio_ = count / len(tokens)        \n",
    "        \n",
    "        ratios_added_per_fold.append(ratio_) \n",
    "        \n",
    "        training_graph_copy.remove_edges_from(added_edges)      \n",
    "    return ratios_added_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bd014f6c-9a92-4e85-a279-78a8258b941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(tst_set_list):\n",
    "    average = sum(tst_set_list) / len(tst_set_list)        \n",
    "    formatted_avg = float(\"{:.5f}\".format(average))        \n",
    "    return formatted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "df86f04a-4d5e-4535-b810-1cadc3bb0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of the list is: 0.28825\n",
      "The average of the list is: 0.30136\n",
      "The average of the list is: 0.3247\n",
      "The average of the list is: 0.27978\n",
      "The average of the list is: 0.30504\n",
      "The average of the list is: 0.30281\n",
      "The average of the list is: 0.29469\n",
      "The average of the list is: 0.28088\n",
      "The average of the list is: 0.27455\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "gpt_means = []\n",
    "for index in range(100,1000):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, gpt_lcc, gpt_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_g = calc_mean(calb_ratios_list) \n",
    "        print(\"The average of the list is:\", tst_mean_g)\n",
    "        gpt_means.append(tst_mean_g)\n",
    "        \n",
    "gpt_min_value = min(gpt_means)\n",
    "gpt_max_value = max(gpt_means) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d67982e6-5346-48f0-8317-96fb6c705464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11835\n",
      "0.10951\n",
      "0.11831\n",
      "0.11937\n",
      "0.11695\n",
      "0.11542\n",
      "0.11684\n",
      "0.10798\n",
      "0.11346\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pubmed_means = []\n",
    "for index in range(100,1000):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, pubmed_lcc, pubmed_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_p = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_p)\n",
    "        pubmed_means.append(tst_mean_p)\n",
    "        \n",
    "pubmed_min_value = min(pubmed_means)\n",
    "pubmed_max_value = max(pubmed_means) \n",
    "for ratio in pubmed_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ed318152-06ef-4e6e-8a70-aba811af55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_an_article(article_text, training_graph):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy()\n",
    "    \n",
    "    # chat_no_added_edges = []\n",
    "    # for abst in stopped_pubmed_training[begin_index:end_index]:\n",
    "\n",
    "    tokens = nltk.word_tokenize(article_text)\n",
    "\n",
    "    # compute the bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # -------  check if the giant has the bigram components, add new edge \n",
    "    # -------          otherwise, don't add new edges\n",
    "    # -------  count how many nodes    \n",
    "\n",
    "    count = 0\n",
    "    added_edges = []\n",
    "    for bigram in bigrams:\n",
    "\n",
    "        if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "            if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                count += 1\n",
    "                added_edges.append((bigram[0], bigram[1]))\n",
    "    ratio_ = count / len(tokens)        \n",
    "    training_graph_copy.remove_edges_from(added_edges)\n",
    "        \n",
    "    return ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f5000d5a-e4bf-4940-a3ec-7a68f557d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.004\n",
      "CORRECT CLASSIFIED:  0.996\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pubmed_min_value = min(pubmed_means)\n",
    "# pubmed_max_value = max(pubmed_means) \n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in gpt_articles_ready[1000:MAX_NUMBER_ARTICLE]:\n",
    "    \n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= pubmed_min_value and ratio_val <= pubmed_max_value:       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/500)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/500)   \n",
    "print('-------------------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "cc6b890e-7be7-4214-ab38-809eae6018b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.002\n",
      "CORRECT CLASSIFIED:  0.998\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# gpt_min_value = min(gpt_means)\n",
    "# gpt_max_value = max(gpt_means)\n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in pubmed_articles_ready[1000:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= gpt_min_value and ratio_val <= gpt_max_value:       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/500)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/500)   \n",
    "print('-------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "251c85d6-87cc-40ad-8143-0c02f6040c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_range(point, range_start, range_end):\n",
    "    # Calculate the distance to the nearest endpoint of the range\n",
    "    distance = min(abs(point - range_start), abs(point - range_end))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "fe3d9bfc-c343-4a14-aa97-25c33c9e2181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUBMED : Fit ratio for individual articles:  0.19072164948453607 evidence PUBMED: background m\n",
      " -------------------------------- \n",
      "distance to range 1:  0.05000898550724638\n",
      "distance to range 2:  0.20691714975845413\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.057971014492753624 , evidence: PUBMED: biofeedback \n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16226415094339622 evidence PUBMED: adolescent b\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13333333333333333 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17105263157894737 evidence PUBMED: scarcity res\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15789473684210525 evidence PUBMED: current stud\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2459016393442623 evidence PUBMED: perspective \n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16826923076923078 evidence PUBMED: background o\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04548000000000001\n",
      "distance to range 2:  0.24330000000000002\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0625 , evidence: PUBMED: mitochondria\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2023121387283237 evidence PUBMED: objectives b\n",
      " -------------------------------- \n",
      "distance to range 1:  0.014522056074766365\n",
      "distance to range 2:  0.20912943925233646\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09345794392523364 , evidence: PUBMED: study aimed \n",
      " -------------------------------- \n",
      "distance to range 1:  0.0295486274509804\n",
      "distance to range 2:  0.20102058823529412\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0784313725490196 , evidence: PUBMED: objective in\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15934065934065933 evidence PUBMED: objective ai\n",
      " -------------------------------- \n",
      "distance to range 1:  0.014230000000000007\n",
      "distance to range 2:  0.22455000000000003\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09375 , evidence: PUBMED: substance us\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.11538461538461539 evidence PUBMED: inflammatory\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.11818181818181818 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03995278911564626\n",
      "distance to range 2:  0.2269309523809524\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06802721088435375 , evidence: PUBMED: objectives c\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1724137931034483 evidence PUBMED: background s\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16990291262135923 evidence PUBMED: background b\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.12437810945273632 evidence PUBMED: background d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17880794701986755 evidence PUBMED: background h\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1357142857142857 evidence PUBMED: recent years\n",
      " -------------------------------- \n",
      "distance to range 1:  0.026347346938775523\n",
      "distance to range 2:  0.16230510204081633\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08163265306122448 , evidence: PUBMED: sars cov rai\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.19852941176470587 evidence PUBMED: objective be\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2138364779874214 evidence PUBMED: background e\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14285714285714285 evidence PUBMED: background s\n",
      " -------------------------------- \n",
      "distance to range 1:  0.012020404040404054\n",
      "distance to range 2:  0.24929747474747477\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09595959595959595 , evidence: PUBMED: background s\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17699115044247787 evidence PUBMED: conducted um\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06914504854368933\n",
      "distance to range 2:  0.15804514563106797\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.038834951456310676 , evidence: PUBMED: major depres\n",
      " -------------------------------- \n",
      "distance to range 1:  0.030599047619047623\n",
      "distance to range 2:  0.2269309523809524\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07738095238095238 , evidence: PUBMED: rationale ps\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13636363636363635 evidence PUBMED: alkaisey par\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16058394160583941 evidence PUBMED: study aimed \n",
      " -------------------------------- \n",
      "distance to range 1:  0.07373342465753426\n",
      "distance to range 2:  0.23345410958904111\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.03424657534246575 , evidence: PUBMED: serotonin ht\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15625 evidence PUBMED: background r\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1864406779661017 evidence PUBMED: objectives s\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16923076923076924 evidence PUBMED: objective in\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02903263157894738\n",
      "distance to range 2:  0.21665526315789474\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07894736842105263 , evidence: PUBMED: objective as\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0008371428571428696\n",
      "distance to range 2:  0.11026428571428573\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10714285714285714 , evidence: PUBMED: background m\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.11764705882352941 evidence PUBMED: background v\n",
      " -------------------------------- \n",
      "distance to range 1:  0.08042094488188978\n",
      "distance to range 2:  0.25486496062992126\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.027559055118110236 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.12048192771084337 evidence PUBMED: gammaaminobu\n",
      " -------------------------------- \n",
      "distance to range 1:  0.011010303030303037\n",
      "distance to range 2:  0.2018227272727273\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09696969696969697 , evidence: PUBMED: background e\n",
      " -------------------------------- \n",
      "distance to range 1:  0.00019556886227545256\n",
      "distance to range 2:  0.1907176646706587\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10778443113772455 , evidence: PUBMED: objective al\n",
      " -------------------------------- \n",
      "distance to range 1:  0.05148282485875707\n",
      "distance to range 2:  0.2576008474576271\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.05649717514124294 , evidence: PUBMED: purpose earl\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1111111111111111 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2331288343558282 evidence PUBMED: background m\n",
      " -------------------------------- \n",
      "distance to range 1:  0.1014440522875817\n",
      "distance to range 2:  0.2287983660130719\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.006535947712418301 , evidence: PUBMED: objectives e\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14367816091954022 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.008532486187845315\n",
      "distance to range 2:  0.21377651933701658\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09944751381215469 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.21296296296296297 evidence PUBMED: objective st\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2987012987012987 evidence GPT: depression asso\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2876712328767123 evidence GPT: depression como\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32051282051282054 evidence GPT: depression asso\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.325 evidence GPT: association dep\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.336734693877551 evidence GPT: depression diab\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.27472527472527475 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2765957446808511 evidence GPT: depression anxi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29411764705882354 evidence GPT: depression obes\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2916666666666667 evidence GPT: depression subs\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32608695652173914 evidence GPT: depression slee\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3673469387755102 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      "distance to range 1:  0.012235319148936177\n",
      "distance to range 2:  0.10433723404255321\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.09574468085106383 , evidence: GPT: gutbrain axis e\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2777777777777778 evidence GPT: depression chil\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.33980582524271846 evidence GPT: depression post\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3829787234042553 evidence GPT: depression schi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.37777777777777777 evidence GPT: depression coro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29545454545454547 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35 evidence GPT: depression elde\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32673267326732675 evidence GPT: depression rheu\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3103448275862069 evidence GPT: depression canc\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3956043956043956 evidence GPT: depression alzh\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.38095238095238093 evidence GPT: depression atte\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.36046511627906974 evidence GPT: depression epil\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3157894736842105 evidence GPT: depression asso\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3125 evidence GPT: depression diab\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3191489361702128 evidence GPT: depression obes\n",
      " -------------------------------- \n",
      "distance to range 1:  0.030202222222222228\n",
      "distance to range 2:  0.052327777777777806\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.07777777777777778 , evidence: GPT: depression subs\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.31521739130434784 evidence GPT: depression slee\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.42528735632183906 evidence GPT: depression card\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3409090909090909 evidence GPT: depression alzh\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3058823529411765 evidence GPT: depression post\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.275 evidence GPT: depression rheu\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2911392405063291 evidence GPT: depression schi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3375 evidence GPT: depression atte\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2875 evidence GPT: depression migr\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29411764705882354 evidence GPT: depression eati\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2826086956521739 evidence GPT: depression post\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30666666666666664 evidence GPT: depression subs\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2804878048780488 evidence GPT: depression anxi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2891566265060241 evidence GPT: depression subs\n",
      " -------------------------------- \n",
      "---------------COUNTS---------------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  50\n",
      "CHATGPT CLASSIFIED:  39\n",
      "FAILED_TO_CLASSIFY:  0\n",
      "GPT MISCLASSIFIED AS PUBMED:  11\n",
      "PUBMED MISCLASSIFIED AS GPT:  0\n",
      "-------------------------------------------------\n",
      "------------- %PERCENTAGE% -----------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  1.0\n",
      "CHATGPT CLASSIFIED:  0.78\n",
      "FAILED_TO_CLASSIFY:  0.0\n",
      "GPT MISCLASSIFIED AS PUBMED:  0.22\n",
      "PUBMED MISCLASSIFIED AS GPT:  0.0\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# two classes classification\n",
    "\n",
    "two_articles_dataset = []\n",
    "\n",
    "for pubmed_article in pubmed_articles_ready[1200:1250]:\n",
    "    two_articles_dataset.append('PUBMED: ' + pubmed_article)\n",
    "\n",
    "for gpt_article in gpt_articles_ready[1200:1250]:\n",
    "    two_articles_dataset.append('GPT: ' + gpt_article)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "chatgpt_class = 0\n",
    "pubmed_class = 0\n",
    "\n",
    "failed_to_classify = 0\n",
    "misclassified_as_gpt = 0\n",
    "misclassified_as_pubmed = 0\n",
    "\n",
    "\n",
    "# RANGE 1: PUBMED\n",
    "range1_start = pubmed_min_value\n",
    "range1_end = pubmed_max_value\n",
    "\n",
    "# RANGE 2: GPT\n",
    "range2_start = gpt_min_value\n",
    "range2_end = gpt_max_value\n",
    "\n",
    "for article in two_articles_dataset:\n",
    "    \n",
    "    gpt_ratio_val    = fit_an_article(article, gpt_lcc)\n",
    "    pubmed_ratio_val = fit_an_article(article, pubmed_lcc)\n",
    "    \n",
    "    # Classifying GPT\n",
    "    if gpt_ratio_val >= range2_start and ratio_val <= range2_end :       \n",
    "        if article[:20].startswith('GPT'):\n",
    "            chatgpt_class+=1\n",
    "            print('ChatGPT : Fit ratio for individual articles: ', gpt_ratio_val, 'evidence', article[:20])\n",
    "        else:\n",
    "            misclassified_as_pubmed+=1\n",
    "            \n",
    "    # Classifying PUBMED\n",
    "    elif pubmed_ratio_val >= range1_start and ratio_val <= range1_end:\n",
    "        if article[:20].startswith('PUBMED'):\n",
    "            pubmed_class += 1\n",
    "            print('PUBMED : Fit ratio for individual articles: ', pubmed_ratio_val, 'evidence', article[:20])\n",
    "        else: \n",
    "            misclassified_as_gpt+=1\n",
    "        \n",
    "    else:\n",
    "        # Calculate distances\n",
    "        distance_to_range1 = distance_to_range(pubmed_ratio_val, range1_start, range1_end)\n",
    "        distance_to_range2 = distance_to_range(gpt_ratio_val, range2_start, range2_end) \n",
    "        \n",
    "        print('distance to range 1: ', distance_to_range1)\n",
    "        print('distance to range 2: ', distance_to_range2)        \n",
    "        \n",
    "        # RANGE 1: PUBMED SHOULD WIN\n",
    "        if distance_to_range1 < distance_to_range2:\n",
    "            if article[:20].startswith('GPT'):\n",
    "                misclassified_as_gpt+=1\n",
    "                print('PUBMED PREDICTED INCORRECTLY => ', 'ratio:', pubmed_ratio_val ,', evidence:', article[:20])                \n",
    "            else:   \n",
    "                # count+=1\n",
    "                pubmed_class += 1\n",
    "                print('PUBMED CLASS PREDICTED => ', 'ratio:', pubmed_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "        # RANGE 2: GPT SHOULD WIN\n",
    "        elif distance_to_range2 < distance_to_range1:\n",
    "            if article[:20].startswith('PUBMED'):                \n",
    "                misclassified_as_pubmed+=1\n",
    "                print('GPT PREDICTED INCORRECTLY => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])                     \n",
    "            else:\n",
    "                chatgpt_class += 1\n",
    "                print('GPT CLASS PREDICTED => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "    print(' -------------------------------- ')\n",
    "    \n",
    "    \n",
    "print('---------------COUNTS---------------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed) \n",
    "print('-------------------------------------------------') \n",
    "    \n",
    "    \n",
    "print('------------- %PERCENTAGE% -----------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class/50)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class/50)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify/50)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt/50)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed/50) \n",
    "print('-------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c0999-346e-4d05-ac7b-012a5f3ec477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b835391-1e86-4f7d-8834-abfd1a6ea5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe33251-e5db-4105-98f5-dc73a4fa2809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7eab34-406e-403e-bafa-5140a1dab92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c24934-4bd8-47ee-8fd5-fe2cf1f24372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
