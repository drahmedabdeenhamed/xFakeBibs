{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee5f81f7-c915-4754-9a11-e4042245f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements \n",
    "import json\n",
    "import obonet\n",
    "from itertools import combinations \n",
    "from Bio import Medline\n",
    "import networkx as nx\n",
    "import string\n",
    "from textblob import TextBlob  \n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import tree\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "63974b6d-8293-4445-ab3a-439a2609ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment configurations\n",
    "# MAX_NUMBER_BIGRAMS = 30\n",
    "MAX_NUMBER_ARTICLE = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8ce8bc1a-8191-4147-8c82-f02f98486f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing a medline file \n",
    "def parse_medline_rmap(medline_file):    \n",
    "    map_abstracts = {}    \n",
    "    pmid = ''\n",
    "    abstract = ''  \n",
    "    with open(medline_file) as medline_handle:\n",
    "        records = Medline.parse(medline_handle)\n",
    "        for record in records:         \n",
    "            keys = record.keys()            \n",
    "            if 'PMID' in keys and 'AB' in keys: \n",
    "\n",
    "                pmid = record['PMID']\n",
    "                abstract = record['AB']\n",
    "                \n",
    "                map_abstracts[pmid] = abstract.lower()\n",
    "    return map_abstracts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85f0e020-a6bb-4b94-b88d-b7500d99de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_gpt_api_data(json_file):\n",
    "\n",
    "    json_records_map = {}\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Now json_data is a list of dictionaries, each representing an item in the array\n",
    "    for item in json_data:\n",
    "        gpt_id = item['GPT-ID']\n",
    "        title = item['Title']\n",
    "        abstract = item['Abstract']\n",
    "        # json_records_map[gpt_id]=(title + \" \" + abstract)\n",
    "        json_records_map[gpt_id]=(title + \" \" + abstract)        \n",
    "    return json_records_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "13080a90-1415-4f9e-ae19-a183512273b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "      \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e603b3a6-895e-4887-9d4f-611ce9a333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_abstracts = parse_medline_rmap('../dataset/pubmed-cancerandc-set-2020-2024-present.txt')\n",
    "cgpt_abstracts = parse_json_gpt_api_data('../dataset/cancer-gpt-apis.txt')\n",
    "\n",
    "# cleaning PubMed articles from special characters\n",
    "clean_pubmed_articles = []\n",
    "for abst in list(pubmed_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_pubmed_articles.append(cleaned)\n",
    "    \n",
    "# cleaning chatGPT articles from special characters\n",
    "clean_chatGPT_articles = []\n",
    "for abst in list(cgpt_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_chatGPT_articles.append(cleaned)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b71e1951-6355-4495-96f3-f11c6de17793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background we aimed to investigate mortality severity and risk of hospitalization in coronavirus disease covid patients with cancer methods data of all patients aged years from the korean disease control and prevention agencycovidnational health insurance service who were diagnosed with covid between january and march in korea were included after propensity score matching patients with cancer and patients without cancer were enrolled in the main analysis a cancer survivor was defined as a patient who had survived or more years since the diagnosis of cancer multiple logistic regression analysis was performed to compare the risk of covid according to the diagnosis of cancer and time since diagnosis results cancer old age male sex incomplete vaccination against covid lower economic status and a higher charlson comorbidity index were associated with an increased risk of hospitalization hospitalization with severe state and death compared to patients without cancer the adjusted odds ratios ors and confidence intervals cis for hospitalization hospitalization with severe state and death in patients with cancer were and respectively compared to patients without cancer the ors cis for hospitalization in cancer survivors patients with cancer diagnosed years years and year ago were and respectively the ors cis for hospitalization for severe disease among these patients were and respectively conclusion the risks of death severe state and hospitalization due to covid were higher in patients with cancer than in those without the more recent the diagnosis the higher the aforementioned risks cancer survivors had a lower risk of hospitalization and hospitalization with severe disease than those without cancer\n"
     ]
    }
   ],
   "source": [
    "print(clean_pubmed_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7cd597f6-5b64-465a-940a-22512d16f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "special_list = ['abstract']\n",
    "\n",
    "def stopwords_rem_pubmed(clean_pubmed_training):\n",
    "    stopped_pubmed_training = []\n",
    "    for abst in clean_pubmed_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if token not in stop_words:\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_pubmed_training.append(valid_rec)\n",
    "    return stopped_pubmed_training\n",
    "    \n",
    "    \n",
    "def stopwords_rem_chatGPT_dataset(clean_chatGPT):    \n",
    "    stopped_chatGPT_training = []\n",
    "    for abst in clean_chatGPT_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if (token not in stop_words) and (token not in special_list):\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_chatGPT_training.append(valid_rec)   \n",
    "    return stopped_chatGPT_training\n",
    "\n",
    "\n",
    "def stopwords_rem_chatGPT_article(clean_chatGPT_article):    \n",
    "    stopped_chatGPT_training = []\n",
    "    valid_l = []\n",
    "    valid_rec = []\n",
    "    blob_object = TextBlob(clean_chatGPT_article)\n",
    "    list_tokens = blob_object.words\n",
    "\n",
    "    for token in list_tokens:        \n",
    "        if (token not in stop_words) and (token not in special_list):\n",
    "            valid_l.append(token)            \n",
    "    valid_rec = ' '.join(valid_l)\n",
    "    # stopped_chatGPT_training.append(valid_rec)   \n",
    "    return str(valid_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "756e0025-68d5-4820-afbb-3c220042d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202\n"
     ]
    }
   ],
   "source": [
    "pubmed_articles_ready = stopwords_rem_pubmed(clean_pubmed_articles)\n",
    "\n",
    "# print(len(stopped_pubmed_training))  \n",
    "gpt_articles_ready = []\n",
    "for article in clean_chatGPT_articles:\n",
    "    gpt_articles_ready.append(stopwords_rem_chatGPT_article(article))\n",
    "print(len(gpt_articles_ready))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8345cb7a-ac87-4af6-aafe-5fb4d2babe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pubmed_articles_ready[0])\n",
    "# print('-----')\n",
    "# print(gpt_articles_ready[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5ffe92e-af59-4d04-b6c3-9c318836e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting PubMed bigrams\n",
    "def compute_bigrams(training_articles):\n",
    "    list_bigrams = []\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range =(2, 2))\n",
    "    X1 = vectorizer.fit_transform(training_articles)\n",
    "    features = (vectorizer.get_feature_names_out())\n",
    "    # print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "    # Applying TFIDF\n",
    "    # You can still get n-grams here\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "    X2 = vectorizer.fit_transform(training_articles)\n",
    "    scores = (X2.toarray())\n",
    "    # print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "    # Getting top ranking features\n",
    "    sums = X2.sum(axis = 0)\n",
    "    data1 = []\n",
    "    for col, term in enumerate(features):\n",
    "        data1.append( (term, sums[0, col] ))\n",
    "    ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "    words = (ranking.sort_values('rank', ascending = False))\n",
    "\n",
    "    bigram_ranks = {}\n",
    "    for index, row in words.iterrows():\n",
    "        # print(row['term'],'\\t\\t\\t',  row['rank'])\n",
    "\n",
    "        splits = row['term'].split()\n",
    "        bigram_ranks[row['rank']] = (splits[0], splits[1])\n",
    "\n",
    "    count = 0    \n",
    "    for k, v in bigram_ranks.items():\n",
    "        # if count < MAX_NUMBER_BIGRAMS:\n",
    "        #     # print(k,'\\t',  v)\n",
    "        #     count += 1\n",
    "        list_bigrams.append(v)\n",
    "    return bigram_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7ff019e0-a5ba-4c93-b9ba-62f7b9065699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_model(training_articles):\n",
    "    bigrams_map_training = compute_bigrams(training_articles)\n",
    "    gpt_training_bigrams = bigrams_map_training.values()\n",
    "    \n",
    "    graph_training_model = nx.Graph()\n",
    "    graph_training_model.add_edges_from(list(gpt_training_bigrams))\n",
    "    \n",
    "    return graph_training_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ded8c1c2-3712-48c4-803c-7f828e9b62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT Training Model --------\n",
      "Original node count:  559\n",
      "Original edge count:  1050\n",
      " -------- PubMed Training Model --------\n",
      "Original node count:  817\n",
      "Original edge count:  1030\n"
     ]
    }
   ],
   "source": [
    "# construct a network training model from both datasets (gpt and pubmed)\n",
    "\n",
    "gpt_training_model = construct_training_model(gpt_articles_ready[:100])\n",
    "pubmed_training_model = construct_training_model(pubmed_articles_ready[:100])\n",
    "\n",
    "# ----------   Verifying GPT Training  Model ----------# \n",
    "print(' -------- GPT Training Model --------')\n",
    "node_count = len(gpt_training_model.nodes())\n",
    "edge_count = len(gpt_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)\n",
    "\n",
    "# ----------   Verifying PubMed Training  Model ----------# \n",
    "print(' -------- PubMed Training Model --------')\n",
    "node_count = len(pubmed_training_model.nodes())\n",
    "edge_count = len(pubmed_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5ffffcb0-d548-4376-96d7-9354472b35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giant_lcc(graph_training_model):\n",
    "    gcc = sorted(nx.connected_components(graph_training_model), key=len, reverse=True)\n",
    "    giant_cc = graph_training_model.subgraph(gcc[0])\n",
    "    return giant_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b7da8e22-e4a9-4973-922d-bd411fa4ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT GIANT LCC Graph --------\n",
      "Graph with 489 nodes and 1008 edges\n",
      " -------- PUBMED GIANT LCC Graph --------\n",
      "Graph with 611 nodes and 918 edges\n"
     ]
    }
   ],
   "source": [
    "print(' -------- GPT GIANT LCC Graph --------')\n",
    "gpt_lcc = get_giant_lcc(gpt_training_model)\n",
    "print(gpt_lcc)\n",
    "\n",
    "print(' -------- PUBMED GIANT LCC Graph --------')\n",
    "pubmed_lcc = get_giant_lcc(pubmed_training_model)\n",
    "print(pubmed_lcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0950208d-636b-4784-b0a0-c1b8f4561eae",
   "metadata": {},
   "outputs": [],
   "source": [
    " # STEP2: -- compute individual articles bigrams -------\n",
    "def calibrate_model(ds_label, begin_index, end_index, training_graph, calibrate_set):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy() \n",
    "\n",
    "    ratios_added_per_fold = []\n",
    "    for abst in calibrate_set[begin_index:end_index]:\n",
    "        \n",
    "        tokens = nltk.word_tokenize(abst)\n",
    "\n",
    "        # compute the bigrams\n",
    "        bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "        # -------  check if the giant has the bigram components, add new edge \n",
    "        # -------          otherwise, don't add new edges\n",
    "        # -------  count how many nodes            \n",
    "        count = 0\n",
    "        added_edges = []\n",
    "        for bigram in bigrams:\n",
    "\n",
    "            if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "                if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                    training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                    count += 1\n",
    "                    added_edges.append((bigram[0], bigram[1]))\n",
    "        ratio_ = count / len(tokens)        \n",
    "        \n",
    "        ratios_added_per_fold.append(ratio_) \n",
    "        \n",
    "        training_graph_copy.remove_edges_from(added_edges)      \n",
    "    return ratios_added_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd014f6c-9a92-4e85-a279-78a8258b941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(tst_set_list):\n",
    "    average = sum(tst_set_list) / len(tst_set_list)        \n",
    "    formatted_avg = float(\"{:.5f}\".format(average))        \n",
    "    return formatted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "df86f04a-4d5e-4535-b810-1cadc3bb0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of the list is: 0.27947\n",
      "The average of the list is: 0.29009\n",
      "The average of the list is: 0.26738\n",
      "The average of the list is: 0.25622\n",
      "The average of the list is: 0.25135\n",
      "The average of the list is: 0.28374\n",
      "The average of the list is: 0.27115\n",
      "The average of the list is: 0.26867\n",
      "The average of the list is: 0.25155\n",
      "The average of the list is: 0.25541\n",
      "The average of the list is: 0.24857\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "gpt_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, gpt_lcc, gpt_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_g = calc_mean(calb_ratios_list) \n",
    "        print(\"The average of the list is:\", tst_mean_g)\n",
    "        gpt_means.append(tst_mean_g)\n",
    "        \n",
    "gpt_min_value = min(gpt_means)\n",
    "gpt_max_value = max(gpt_means) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d67982e6-5346-48f0-8317-96fb6c705464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1744\n",
      "0.17616\n",
      "0.15853\n",
      "0.15515\n",
      "0.15578\n",
      "0.17226\n",
      "0.17146\n",
      "0.15611\n",
      "0.17532\n",
      "0.1614\n",
      "0.17393\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pubmed_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, pubmed_lcc, pubmed_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_p = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_p)\n",
    "        pubmed_means.append(tst_mean_p)\n",
    "        \n",
    "pubmed_min_value = min(pubmed_means)\n",
    "pubmed_max_value = max(pubmed_means) \n",
    "# print(gpt_means)\n",
    "for ratio in pubmed_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed318152-06ef-4e6e-8a70-aba811af55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_an_article(article_text, training_graph):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy()\n",
    "    \n",
    "    # chat_no_added_edges = []\n",
    "    # for abst in stopped_pubmed_training[begin_index:end_index]:\n",
    "\n",
    "    tokens = nltk.word_tokenize(article_text)\n",
    "\n",
    "    # compute the bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # -------  check if the giant has the bigram components, add new edge \n",
    "    # -------          otherwise, don't add new edges\n",
    "    # -------  count how many nodes    \n",
    "\n",
    "    count = 0\n",
    "    added_edges = []\n",
    "    for bigram in bigrams:\n",
    "\n",
    "        if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "            if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                count += 1\n",
    "                added_edges.append((bigram[0], bigram[1]))\n",
    "    ratio_ = count / len(tokens)        \n",
    "    training_graph_copy.remove_edges_from(added_edges)\n",
    "        \n",
    "    return ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f5000d5a-e4bf-4940-a3ec-7a68f557d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.053\n",
      "CORRECT CLASSIFIED:  0.947\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The average of the list is: 0.15515\n",
    "# The average of the list is: 0.17616\n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in gpt_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= pubmed_min_value and ratio_val <= pubmed_max_value :       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/1000)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/1000)   \n",
    "print('-------------------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cc6b890e-7be7-4214-ab38-809eae6018b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.002\n",
      "CORRECT CLASSIFIED:  0.998\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The average of the list is: 0.25135\n",
    "# The average of the list is: 0.29009\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in pubmed_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= gpt_min_value and ratio_val <= gpt_max_value :       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/1000)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/1000)   \n",
    "print('-------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f9a12b10-d325-4f2c-8ac0-ca7531611514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_range(point, range_start, range_end):\n",
    "    # Calculate the distance to the nearest endpoint of the range\n",
    "    distance = min(abs(point - range_start), abs(point - range_end))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8f6c2ace-35b7-4f5e-9cd4-f6ffee8bad86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUBMED : Fit ratio for individual articles:  0.203125 evidence PUBMED: treatment ch\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16071428571428573 evidence PUBMED: radical cyst\n",
      " -------------------------------- \n",
      "distance to range 1:  0.007609016393442636\n",
      "distance to range 2:  0.19119295081967214\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.14754098360655737 , evidence: PUBMED: purpose stud\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03796250000000001\n",
      "distance to range 2:  0.2095075\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1171875 , evidence: PUBMED: present clea\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2903225806451613 evidence PUBMED: purpose test\n",
      " -------------------------------- \n",
      "distance to range 1:  0.019435714285714306\n",
      "distance to range 2:  0.13071285714285716\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1357142857142857 , evidence: PUBMED: background l\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17692307692307693 evidence PUBMED: background w\n",
      " -------------------------------- \n",
      "distance to range 1:  0.00431201117318436\n",
      "distance to range 2:  0.17035770949720672\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.15083798882681565 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0017409090909090985\n",
      "distance to range 2:  0.18038818181818184\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1534090909090909 , evidence: PUBMED: objectives o\n",
      " -------------------------------- \n",
      "distance to range 1:  0.05358750000000001\n",
      "distance to range 2:  0.139195\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1015625 , evidence: PUBMED: objectives u\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17261904761904762 evidence PUBMED: sjgrens synd\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.34108527131782945 evidence PUBMED: clinical dat\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0164638686131387\n",
      "distance to range 2:  0.1682780291970803\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1386861313868613 , evidence: PUBMED: background w\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0654639013452915\n",
      "distance to range 2:  0.1947583408071749\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08968609865470852 , evidence: PUBMED: purpose revi\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15757575757575756 evidence PUBMED: common varia\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24369747899159663 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06819347826086958\n",
      "distance to range 2:  0.1942221739130435\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08695652173913043 , evidence: PUBMED: clinical stu\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2413793103448276 evidence PUBMED: global pande\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24 evidence PUBMED: aim study ai\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.3107344632768362 evidence PUBMED: aims aim ana\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.29457364341085274 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24858757062146894 evidence PUBMED: anoop tmobje\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03489683544303798\n",
      "distance to range 2:  0.22325354430379749\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12025316455696203 , evidence: PUBMED: objectives s\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.20125786163522014 evidence PUBMED: objective st\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.18045112781954886 evidence PUBMED: background p\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24242424242424243 evidence PUBMED: purpose stud\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.27586206896551724 evidence PUBMED: importance m\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2450592885375494 evidence PUBMED: importance c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07759897959183674\n",
      "distance to range 2:  0.211835306122449\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07755102040816327 , evidence: PUBMED: objective pa\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2251655629139073 evidence PUBMED: background l\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2534246575342466 evidence PUBMED: aim assessin\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2356687898089172 evidence PUBMED: objective ev\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16666666666666666 evidence PUBMED: background p\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07702500000000001\n",
      "distance to range 2:  0.21732\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.078125 , evidence: PUBMED: inequitable \n",
      " -------------------------------- \n",
      "distance to range 1:  0.043385294117647064\n",
      "distance to range 2:  0.15445235294117649\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11176470588235295 , evidence: PUBMED: objectives s\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03335512820512822\n",
      "distance to range 2:  0.1780571794871795\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12179487179487179 , evidence: PUBMED: background s\n",
      " -------------------------------- \n",
      "distance to range 1:  0.1430287878787879\n",
      "distance to range 2:  0.20008515151515152\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.012121212121212121 , evidence: PUBMED: nonalcoholic\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.20087336244541484 evidence PUBMED: aimshypothes\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.19337016574585636 evidence PUBMED: background d\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03292777777777779\n",
      "distance to range 2:  0.19301444444444446\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12222222222222222 , evidence: PUBMED: granulomatos\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2662337662337662 evidence PUBMED: background p\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17307692307692307 evidence PUBMED: background r\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.28125 evidence PUBMED: background s\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17050691244239632 evidence PUBMED: objectives e\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.20454545454545456 evidence PUBMED: objectives e\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17692307692307693 evidence PUBMED: multimorbidi\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07406891891891892\n",
      "distance to range 2:  0.2099599613899614\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08108108108108109 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.216 evidence PUBMED: purposes stu\n",
      " -------------------------------- \n",
      "distance to range 1:  0.038711643835616455\n",
      "distance to range 2:  0.1800768493150685\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11643835616438356 , evidence: PUBMED: combat traum\n",
      " -------------------------------- \n",
      "distance to range 1:  0.016785220125786177\n",
      "distance to range 2:  0.16051968553459123\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13836477987421383 , evidence: PUBMED: background c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2905982905982906 evidence GPT: cardiovascular \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3157894736842105 evidence GPT: association ova\n",
      " -------------------------------- \n",
      "distance to range 1:  0.08372142857142859\n",
      "distance to range 2:  0.03428428571428574\n",
      "GPT CLASS PREDICTED =>  ratio: 0.21428571428571427 , evidence: GPT: complex interac\n",
      " -------------------------------- \n",
      "distance to range 1:  0.01693861788617887\n",
      "distance to range 2:  0.012797642276422772\n",
      "GPT CLASS PREDICTED =>  ratio: 0.23577235772357724 , evidence: GPT: thyroid cancer \n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.27049180327868855 evidence GPT: association pro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2727272727272727 evidence GPT: association bre\n",
      " -------------------------------- \n",
      "distance to range 1:  0.026117741935483885\n",
      "distance to range 2:  0.006634516129032264\n",
      "GPT CLASS PREDICTED =>  ratio: 0.24193548387096775 , evidence: GPT: impact obesity \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2727272727272727 evidence GPT: association non\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32432432432432434 evidence GPT: bidirectional r\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2636363636363636 evidence GPT: association lun\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3508771929824561 evidence GPT: association hep\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.288135593220339 evidence GPT: association gas\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03234298245614037\n",
      "distance to range 2:  0.029271754385964938\n",
      "GPT CLASS PREDICTED =>  ratio: 0.21929824561403508 , evidence: GPT: relationship pr\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2909090909090909 evidence GPT: association bre\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.275 evidence GPT: association pan\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32051282051282054 evidence GPT: role inflammati\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34065934065934067 evidence GPT: genetic suscept\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.42696629213483145 evidence GPT: impact cancer t\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3076923076923077 evidence GPT: metabolic alter\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.26373626373626374 evidence GPT: impact cancer c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3617021276595745 evidence GPT: psychosocial fa\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "distance to range 1:  0.06540641025641027\n",
      "distance to range 2:  0.06908282051282053\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.08974358974358974 , evidence: GPT: relationship in\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "distance to range 1:  0.002607627118644068\n",
      "distance to range 2:  0.045180169491525424\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.15254237288135594 , evidence: GPT: impact chronic \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29069767441860467 evidence GPT: impact cancer c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3026315789473684 evidence GPT: hepatic comorbi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3037974683544304 evidence GPT: link obesity ca\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.26582278481012656 evidence GPT: impact socioeco\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29577464788732394 evidence GPT: immunotherapy c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2631578947368421 evidence GPT: role chronic in\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2604166666666667 evidence GPT: impact cancer s\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.43157894736842106 evidence GPT: targeted therap\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04151363636363638\n",
      "distance to range 2:  0.0326609090909091\n",
      "GPT CLASS PREDICTED =>  ratio: 0.2159090909090909 , evidence: GPT: gut microbiota \n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "distance to range 1:  0.026117741935483885\n",
      "distance to range 2:  0.033516236559139795\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.12903225806451613 , evidence: GPT: impact cancer c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.36893203883495146 evidence GPT: exploring relat\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30392156862745096 evidence GPT: association can\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3584905660377358 evidence GPT: comorbidities l\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3142857142857143 evidence GPT: understanding b\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2857142857142857 evidence GPT: impact cancer r\n",
      " -------------------------------- \n",
      "distance to range 1:  0.028834210526315796\n",
      "distance to range 2:  0.006464736842105273\n",
      "GPT CLASS PREDICTED =>  ratio: 0.24210526315789474 , evidence: GPT: association can\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35454545454545455 evidence GPT: impact cancer n\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3333333333333333 evidence GPT: relationship ca\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32 evidence GPT: comorbidities t\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2621359223300971 evidence GPT: interplay cance\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.37373737373737376 evidence GPT: gastrointestina\n",
      " -------------------------------- \n",
      "---------------COUNTS---------------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  50\n",
      "CHATGPT CLASSIFIED:  41\n",
      "FAILED_TO_CLASSIFY:  0\n",
      "GPT MISCLASSIFIED AS PUBMED:  9\n",
      "PUBMED MISCLASSIFIED AS GPT:  0\n",
      "-------------------------------------------------\n",
      "------------- %PERCENTAGE% -----------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  1.0\n",
      "CHATGPT CLASSIFIED:  0.82\n",
      "FAILED_TO_CLASSIFY:  0.0\n",
      "GPT MISCLASSIFIED AS PUBMED:  0.18\n",
      "PUBMED MISCLASSIFIED AS GPT:  0.0\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# two classes classification\n",
    "\n",
    "two_articles_dataset = []\n",
    "\n",
    "for pubmed_article in pubmed_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('PUBMED: ' + pubmed_article)\n",
    "\n",
    "for gpt_article in gpt_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('GPT: ' + gpt_article)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "chatgpt_class = 0\n",
    "pubmed_class = 0\n",
    "\n",
    "failed_to_classify = 0\n",
    "misclassified_as_gpt = 0\n",
    "misclassified_as_pubmed = 0\n",
    "\n",
    "\n",
    "# RANGE 1: PUBMED\n",
    "range1_start = pubmed_min_value\n",
    "range1_end = pubmed_max_value\n",
    "\n",
    "# RANGE 2: GPT\n",
    "range2_start = gpt_min_value\n",
    "range2_end = gpt_max_value\n",
    "\n",
    "for article in two_articles_dataset:\n",
    "    \n",
    "    gpt_ratio_val    = fit_an_article(article, gpt_lcc)\n",
    "    pubmed_ratio_val = fit_an_article(article, pubmed_lcc)\n",
    "    \n",
    "    # Classifying GPT\n",
    "    if gpt_ratio_val >= range2_start and ratio_val <= range2_end :       \n",
    "        if article[:20].startswith('GPT'):\n",
    "            chatgpt_class+=1\n",
    "            print('ChatGPT : Fit ratio for individual articles: ', gpt_ratio_val, 'evidence', article[:20])\n",
    "        else:\n",
    "            misclassified_as_pubmed+=1\n",
    "            \n",
    "    # Classifying PUBMED\n",
    "    elif pubmed_ratio_val >= range1_start and ratio_val <= range1_end:\n",
    "        if article[:20].startswith('PUBMED'):\n",
    "            pubmed_class += 1\n",
    "            print('PUBMED : Fit ratio for individual articles: ', pubmed_ratio_val, 'evidence', article[:20])\n",
    "        else: \n",
    "            misclassified_as_gpt+=1\n",
    "        \n",
    "    else:\n",
    "        # Calculate distances\n",
    "        distance_to_range1 = distance_to_range(pubmed_ratio_val, range1_start, range1_end)\n",
    "        distance_to_range2 = distance_to_range(gpt_ratio_val, range2_start, range2_end) \n",
    "        \n",
    "        print('distance to range 1: ', distance_to_range1)\n",
    "        print('distance to range 2: ', distance_to_range2)        \n",
    "        \n",
    "        # RANGE 1: PUBMED SHOULD WIN\n",
    "        if distance_to_range1 < distance_to_range2:\n",
    "            if article[:20].startswith('GPT'):\n",
    "                misclassified_as_gpt+=1\n",
    "                print('PUBMED PREDICTED INCORRECTLY => ', 'ratio:', pubmed_ratio_val ,', evidence:', article[:20])                \n",
    "            else:   \n",
    "                # count+=1\n",
    "                pubmed_class += 1\n",
    "                print('PUBMED CLASS PREDICTED => ', 'ratio:', pubmed_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "        # RANGE 2: GPT SHOULD WIN\n",
    "        elif distance_to_range2 < distance_to_range1:\n",
    "            if article[:20].startswith('PUBMED'):                \n",
    "                misclassified_as_pubmed+=1\n",
    "                print('GPT PREDICTED INCORRECTLY => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])                     \n",
    "            else:\n",
    "                chatgpt_class += 1\n",
    "                print('GPT CLASS PREDICTED => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "    print(' -------------------------------- ')\n",
    "    \n",
    "    \n",
    "print('---------------COUNTS---------------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed) \n",
    "print('-------------------------------------------------') \n",
    "    \n",
    "    \n",
    "print('------------- %PERCENTAGE% -----------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class/50)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class/50)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify/50)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt/50)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed/50) \n",
    "print('-------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcb8f0-014c-43f9-99f5-837f516adc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677a29e-4e86-4d76-bbda-5a8f09a3234b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e0d91-536e-402e-a5cc-7f87f52968e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06437c25-4528-442b-bd1c-cbd8d5e6ec7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d906d-29c6-436c-b458-aaf29d6d3f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
