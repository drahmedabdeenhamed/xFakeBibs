{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ee5f81f7-c915-4754-9a11-e4042245f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements \n",
    "import json\n",
    "import obonet\n",
    "from itertools import combinations \n",
    "from Bio import Medline\n",
    "import networkx as nx\n",
    "import string\n",
    "from textblob import TextBlob  \n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import tree\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "63974b6d-8293-4445-ab3a-439a2609ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment configurations\n",
    "# MAX_NUMBER_BIGRAMS = 30\n",
    "MAX_NUMBER_ARTICLE = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8ce8bc1a-8191-4147-8c82-f02f98486f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing a medline file \n",
    "def parse_medline_rmap(medline_file):    \n",
    "    map_abstracts = {}    \n",
    "    pmid = ''\n",
    "    abstract = ''  \n",
    "    with open(medline_file) as medline_handle:\n",
    "        records = Medline.parse(medline_handle)\n",
    "        for record in records:         \n",
    "            keys = record.keys()            \n",
    "            if 'PMID' in keys and 'AB' in keys: \n",
    "\n",
    "                pmid = record['PMID']\n",
    "                abstract = record['AB']\n",
    "                \n",
    "                map_abstracts[pmid] = abstract.lower()\n",
    "    return map_abstracts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "85f0e020-a6bb-4b94-b88d-b7500d99de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_gpt_api_data(json_file):\n",
    "\n",
    "    json_records_map = {}\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Now json_data is a list of dictionaries, each representing an item in the array\n",
    "    for item in json_data:\n",
    "        gpt_id = item['GPT-ID']\n",
    "        title = item['Title']\n",
    "        abstract = item['Abstract']\n",
    "        # json_records_map[gpt_id]=(title + \" \" + abstract)\n",
    "        json_records_map[gpt_id]=(title + \" \" + abstract)        \n",
    "    return json_records_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "13080a90-1415-4f9e-ae19-a183512273b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "      \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e603b3a6-895e-4887-9d4f-611ce9a333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_abstracts = parse_medline_rmap('../dataset/pubmed-alzheimers-set-2010-2014.txt')\n",
    "cgpt_abstracts = parse_json_gpt_api_data('../dataset/alz-gpt-apis.txt')\n",
    "\n",
    "# cleaning PubMed articles from special characters\n",
    "clean_pubmed_articles = []\n",
    "for abst in list(pubmed_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_pubmed_articles.append(cleaned)\n",
    "    \n",
    "# cleaning chatGPT articles from special characters\n",
    "clean_chatGPT_articles = []\n",
    "for abst in list(cgpt_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_chatGPT_articles.append(cleaned)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b71e1951-6355-4495-96f3-f11c6de17793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emergency medical responders were activated to the home of a yearold africanamerican male in distress and with known down syndrome complicated by alzheimers disease he was found to be unresponsive and subsequently became pulseless advanced cardiac life support protocols were initiated and continued for two hours in the emergency department due to family request efforts were eventually ceased and the patient was declared dead full unrestricted autopsy examination was conducted under the coroners authorization the cause of death was determined to be a pulmonary thromboembolus in the main pulmonary artery with extension into the bilateral pulmonary arteries additional external findings included alopecia universalis penoscrotal hypospadias ostium secundum type of atrial septal defect right ventricular cardiac dilatation diffuse cerebral atrophy facial features compatible with down syndrome and generalized patches of skin depigmentation over the hands as seen in figure but also over the feet lips areola and trunk microscopic findings included features of pulmonary hypertension a microscopic image from a section of the thyroid is seen in figure\n"
     ]
    }
   ],
   "source": [
    "print(clean_pubmed_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7cd597f6-5b64-465a-940a-22512d16f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "special_list = ['abstract']\n",
    "\n",
    "def stopwords_rem_pubmed(clean_pubmed_training):\n",
    "    stopped_pubmed_training = []\n",
    "    for abst in clean_pubmed_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if token not in stop_words:\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_pubmed_training.append(valid_rec)\n",
    "    return stopped_pubmed_training\n",
    "    \n",
    "    \n",
    "def stopwords_rem_chatGPT_dataset(clean_chatGPT):    \n",
    "    stopped_chatGPT_training = []\n",
    "    for abst in clean_chatGPT_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if (token not in stop_words) and (token not in special_list):\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_chatGPT_training.append(valid_rec)   \n",
    "    return stopped_chatGPT_training\n",
    "\n",
    "\n",
    "def stopwords_rem_chatGPT_article(clean_chatGPT_article):    \n",
    "    stopped_chatGPT_training = []\n",
    "    valid_l = []\n",
    "    valid_rec = []\n",
    "    blob_object = TextBlob(clean_chatGPT_article)\n",
    "    list_tokens = blob_object.words\n",
    "\n",
    "    for token in list_tokens:        \n",
    "        if (token not in stop_words) and (token not in special_list):\n",
    "            valid_l.append(token)            \n",
    "    valid_rec = ' '.join(valid_l)\n",
    "    # stopped_chatGPT_training.append(valid_rec)   \n",
    "    return str(valid_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "756e0025-68d5-4820-afbb-3c220042d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179\n"
     ]
    }
   ],
   "source": [
    "pubmed_articles_ready = stopwords_rem_pubmed(clean_pubmed_articles)\n",
    "\n",
    "# print(len(stopped_pubmed_training))  \n",
    "gpt_articles_ready = []\n",
    "for article in clean_chatGPT_articles:\n",
    "    gpt_articles_ready.append(stopwords_rem_chatGPT_article(article))\n",
    "print(len(gpt_articles_ready))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8345cb7a-ac87-4af6-aafe-5fb4d2babe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pubmed_articles_ready[0])\n",
    "# print('-----')\n",
    "# print(gpt_articles_ready[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "d5ffe92e-af59-4d04-b6c3-9c318836e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting PubMed bigrams\n",
    "def compute_bigrams(training_articles):\n",
    "    list_bigrams = []\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range =(2, 2))\n",
    "    X1 = vectorizer.fit_transform(training_articles)\n",
    "    features = (vectorizer.get_feature_names_out())\n",
    "    # print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "    # Applying TFIDF\n",
    "    # You can still get n-grams here\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "    X2 = vectorizer.fit_transform(training_articles)\n",
    "    scores = (X2.toarray())\n",
    "    # print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "    # Getting top ranking features\n",
    "    sums = X2.sum(axis = 0)\n",
    "    data1 = []\n",
    "    for col, term in enumerate(features):\n",
    "        data1.append( (term, sums[0, col] ))\n",
    "    ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "    words = (ranking.sort_values('rank', ascending = False))\n",
    "\n",
    "    bigram_ranks = {}\n",
    "    for index, row in words.iterrows():\n",
    "        # print(row['term'],'\\t\\t\\t',  row['rank'])\n",
    "\n",
    "        splits = row['term'].split()\n",
    "        bigram_ranks[row['rank']] = (splits[0], splits[1])\n",
    "\n",
    "    count = 0    \n",
    "    for k, v in bigram_ranks.items():\n",
    "        # if count < MAX_NUMBER_BIGRAMS:\n",
    "        #     # print(k,'\\t',  v)\n",
    "        #     count += 1\n",
    "        list_bigrams.append(v)\n",
    "    return bigram_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7ff019e0-a5ba-4c93-b9ba-62f7b9065699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_model(training_articles):\n",
    "    bigrams_map_training = compute_bigrams(training_articles)\n",
    "    gpt_training_bigrams = bigrams_map_training.values()\n",
    "    \n",
    "    graph_training_model = nx.Graph()\n",
    "    graph_training_model.add_edges_from(list(gpt_training_bigrams))\n",
    "    \n",
    "    return graph_training_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ded8c1c2-3712-48c4-803c-7f828e9b62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT Training Model --------\n",
      "Original node count:  519\n",
      "Original edge count:  1194\n",
      " -------- PubMed Training Model --------\n",
      "Original node count:  742\n",
      "Original edge count:  861\n"
     ]
    }
   ],
   "source": [
    "# construct a network training model from both datasets (gpt and pubmed)\n",
    "\n",
    "gpt_training_model = construct_training_model(gpt_articles_ready[:100])\n",
    "pubmed_training_model = construct_training_model(pubmed_articles_ready[:100])\n",
    "\n",
    "# ----------   Verifying GPT Training  Model ----------# \n",
    "print(' -------- GPT Training Model --------')\n",
    "node_count = len(gpt_training_model.nodes())\n",
    "edge_count = len(gpt_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)\n",
    "\n",
    "# ----------   Verifying PubMed Training  Model ----------# \n",
    "print(' -------- PubMed Training Model --------')\n",
    "node_count = len(pubmed_training_model.nodes())\n",
    "edge_count = len(pubmed_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5ffffcb0-d548-4376-96d7-9354472b35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giant_lcc(graph_training_model):\n",
    "    gcc = sorted(nx.connected_components(graph_training_model), key=len, reverse=True)\n",
    "    giant_cc = graph_training_model.subgraph(gcc[0])\n",
    "    return giant_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b7da8e22-e4a9-4973-922d-bd411fa4ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT GIANT LCC Graph --------\n",
      "Graph with 479 nodes and 1170 edges\n",
      " -------- PUBMED GIANT LCC Graph --------\n",
      "Graph with 531 nodes and 735 edges\n"
     ]
    }
   ],
   "source": [
    "print(' -------- GPT GIANT LCC Graph --------')\n",
    "gpt_lcc = get_giant_lcc(gpt_training_model)\n",
    "print(gpt_lcc)\n",
    "\n",
    "print(' -------- PUBMED GIANT LCC Graph --------')\n",
    "pubmed_lcc = get_giant_lcc(pubmed_training_model)\n",
    "print(pubmed_lcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0950208d-636b-4784-b0a0-c1b8f4561eae",
   "metadata": {},
   "outputs": [],
   "source": [
    " # STEP2: -- compute individual articles bigrams -------\n",
    "def calibrate_model(ds_label, begin_index, end_index, training_graph, calibrate_set):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy() \n",
    "\n",
    "    ratios_added_per_fold = []\n",
    "    for abst in calibrate_set[begin_index:end_index]:\n",
    "        \n",
    "        tokens = nltk.word_tokenize(abst)\n",
    "\n",
    "        # compute the bigrams\n",
    "        bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "        # -------  check if the giant has the bigram components, add new edge \n",
    "        # -------          otherwise, don't add new edges\n",
    "        # -------  count how many nodes            \n",
    "        count = 0\n",
    "        added_edges = []\n",
    "        for bigram in bigrams:\n",
    "\n",
    "            if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "                if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                    training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                    count += 1\n",
    "                    added_edges.append((bigram[0], bigram[1]))\n",
    "        ratio_ = count / len(tokens)        \n",
    "        \n",
    "        ratios_added_per_fold.append(ratio_) \n",
    "        \n",
    "        training_graph_copy.remove_edges_from(added_edges)      \n",
    "    return ratios_added_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "bd014f6c-9a92-4e85-a279-78a8258b941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(tst_set_list):\n",
    "    average = sum(tst_set_list) / len(tst_set_list)        \n",
    "    formatted_avg = float(\"{:.5f}\".format(average))        \n",
    "    return formatted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "df86f04a-4d5e-4535-b810-1cadc3bb0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27871\n",
      "0.30034\n",
      "0.30916\n",
      "0.28195\n",
      "0.28424\n",
      "0.29355\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "gpt_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, gpt_lcc, gpt_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_g = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_g)\n",
    "        gpt_means.append(tst_mean_g)\n",
    "        \n",
    "gpt_min_value = min(gpt_means)\n",
    "gpt_max_value = max(gpt_means) \n",
    "\n",
    "# print(gpt_means)\n",
    "for ratio in gpt_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d67982e6-5346-48f0-8317-96fb6c705464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15721\n",
      "0.16999\n",
      "0.16065\n",
      "0.16134\n",
      "0.16908\n",
      "0.16493\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pubmed_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, pubmed_lcc, pubmed_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_p = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_p)\n",
    "        pubmed_means.append(tst_mean_p)\n",
    "        \n",
    "pubmed_min_value = min(pubmed_means)\n",
    "pubmed_max_value = max(pubmed_means) \n",
    "\n",
    "# print(gpt_means)\n",
    "for ratio in pubmed_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ed318152-06ef-4e6e-8a70-aba811af55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_an_article(article_text, training_graph):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy()\n",
    "    \n",
    "    # chat_no_added_edges = []\n",
    "    # for abst in stopped_pubmed_training[begin_index:end_index]:\n",
    "\n",
    "    tokens = nltk.word_tokenize(article_text)\n",
    "\n",
    "    # compute the bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # -------  check if the giant has the bigram components, add new edge \n",
    "    # -------          otherwise, don't add new edges\n",
    "    # -------  count how many nodes    \n",
    "    count = 0\n",
    "    added_edges = []\n",
    "    for bigram in bigrams:\n",
    "\n",
    "        if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "            if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                count += 1\n",
    "                added_edges.append((bigram[0], bigram[1]))\n",
    "    ratio_ = count / len(tokens)        \n",
    "    training_graph_copy.remove_edges_from(added_edges)\n",
    "        \n",
    "    return ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f5000d5a-e4bf-4940-a3ec-7a68f557d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.04\n",
      "CORRECT CLASSIFIED:  4.96\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pubmed_min_value = min(pubmed_means)\n",
    "# pubmed_max_value = max(pubmed_means) \n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in gpt_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= pubmed_min_value and ratio_val <= pubmed_max_value :       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/100)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/100)   \n",
    "print('-------------------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "cc6b890e-7be7-4214-ab38-809eae6018b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.01\n",
      "CORRECT CLASSIFIED:  0.99\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# gpt_min_value = min(gpt_means)\n",
    "# gpt_max_value = max(gpt_means)\n",
    "\n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in pubmed_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= gpt_min_value and ratio_val <= gpt_max_value:        \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/500)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/500)   \n",
    "print('-------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9efd5d03-90a9-47ae-a4d9-806c9cb44e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_range(point, range_start, range_end):\n",
    "    # Calculate the distance to the nearest endpoint of the range\n",
    "    distance = min(abs(point - range_start), abs(point - range_end))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "16301310-f569-4518-8dc2-286c02ecaadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUBMED : Fit ratio for individual articles:  0.17647058823529413 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.015234691358024688\n",
      "distance to range 2:  0.21080876543209878\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1419753086419753 , evidence: PUBMED: depression a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16666666666666666 evidence PUBMED: visceral obe\n",
      " -------------------------------- \n",
      "distance to range 1:  0.040250935672514615\n",
      "distance to range 2:  0.2670140935672515\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11695906432748537 , evidence: PUBMED: background o\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2905027932960894 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07745539877300613\n",
      "distance to range 2:  0.22349527607361963\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07975460122699386 , evidence: PUBMED: objective dr\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07127249999999999\n",
      "distance to range 2:  0.2240225\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0859375 , evidence: PUBMED: describe pat\n",
      " -------------------------------- \n",
      "distance to range 1:  0.051109204244031814\n",
      "distance to range 2:  0.2601423607427056\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10610079575596817 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.22131147540983606 evidence PUBMED: objective ev\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.21428571428571427 evidence PUBMED: several prev\n",
      " -------------------------------- \n",
      "distance to range 1:  0.09109429752066114\n",
      "distance to range 2:  0.13821413223140497\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06611570247933884 , evidence: PUBMED: pseudobulbar\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17117117117117117 evidence PUBMED: background g\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.18181818181818182 evidence PUBMED: report case \n",
      " -------------------------------- \n",
      "distance to range 1:  0.011640379746835433\n",
      "distance to range 2:  0.1964315189873418\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.14556962025316456 , evidence: PUBMED: background s\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2670807453416149 evidence PUBMED: objectives f\n",
      " -------------------------------- \n",
      "distance to range 1:  0.004343757961783434\n",
      "distance to range 2:  0.21501573248407646\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.15286624203821655 , evidence: PUBMED: background c\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15920398009950248 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.012282463768115925\n",
      "distance to range 2:  0.17726072463768117\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.14492753623188406 , evidence: PUBMED: background f\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.25146198830409355 evidence PUBMED: changes brai\n",
      " -------------------------------- \n",
      "distance to range 1:  0.059307902097902085\n",
      "distance to range 2:  0.13884986013986014\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0979020979020979 , evidence: PUBMED: onethird peo\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04536789473684209\n",
      "distance to range 2:  0.2655521052631579\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1118421052631579 , evidence: PUBMED: study invest\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.21052631578947367 evidence PUBMED: hippocampal \n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16842105263157894 evidence PUBMED: objective fe\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02677521739130434\n",
      "distance to range 2:  0.15914478260869566\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13043478260869565 , evidence: PUBMED: empirical ev\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.19858156028368795 evidence PUBMED: animal epide\n",
      " -------------------------------- \n",
      "distance to range 1:  0.004204535519125668\n",
      "distance to range 2:  0.17488486338797815\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.15300546448087432 , evidence: PUBMED: essential tr\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03695683544303796\n",
      "distance to range 2:  0.1774441772151899\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12025316455696203 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.01435285714285714\n",
      "distance to range 2:  0.22026844155844158\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.14285714285714285 , evidence: PUBMED: background p\n",
      " -------------------------------- \n",
      "distance to range 1:  0.058200099009900974\n",
      "distance to range 2:  0.179700099009901\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09900990099009901 , evidence: PUBMED: numerous lin\n",
      " -------------------------------- \n",
      "distance to range 1:  0.017675116279069758\n",
      "distance to range 2:  0.23801232558139535\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13953488372093023 , evidence: PUBMED: background l\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15950920245398773 evidence PUBMED: objective sy\n",
      " -------------------------------- \n",
      "distance to range 1:  0.010613141361256545\n",
      "distance to range 2:  0.24206078534031417\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.14659685863874344 , evidence: PUBMED: background n\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.23880597014925373 evidence PUBMED: purpose revi\n",
      " -------------------------------- \n",
      "distance to range 1:  0.023351732283464544\n",
      "distance to range 2:  0.20784385826771656\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13385826771653545 , evidence: PUBMED: objective an\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2980769230769231 evidence PUBMED: background a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24561403508771928 evidence PUBMED: background m\n",
      " -------------------------------- \n",
      "distance to range 1:  0.09054333333333332\n",
      "distance to range 2:  0.2059827272727273\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06666666666666667 , evidence: PUBMED: pathophysiol\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.208955223880597 evidence PUBMED: cognitive im\n",
      " -------------------------------- \n",
      "distance to range 1:  0.047532580645161276\n",
      "distance to range 2:  0.18193580645161292\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10967741935483871 , evidence: PUBMED: objective ev\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.18478260869565216 evidence PUBMED: objectives e\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.20105820105820105 evidence PUBMED: multimorbidi\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03276555555555555\n",
      "distance to range 2:  0.23871\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12444444444444444 , evidence: PUBMED: primary prog\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.23756906077348067 evidence PUBMED: objectives c\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17857142857142858 evidence PUBMED: epidemiologi\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06510473684210526\n",
      "distance to range 2:  0.13397315789473685\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09210526315789473 , evidence: PUBMED: chronic pain\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1610738255033557 evidence PUBMED: objectives d\n",
      " -------------------------------- \n",
      "distance to range 1:  0.015543333333333326\n",
      "distance to range 2:  0.20371\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.14166666666666666 , evidence: PUBMED: objective go\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.30714285714285716 evidence PUBMED: background c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.003363846153846134\n",
      "distance to range 2:  0.2077040828402367\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.15384615384615385 , evidence: PUBMED: objective al\n",
      " -------------------------------- \n",
      "distance to range 1:  0.029550425531914892\n",
      "distance to range 2:  0.02339085106382982\n",
      "GPT CLASS PREDICTED =>  ratio: 0.2553191489361702 , evidence: GPT: musculoskeletal\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3695652173913043 evidence GPT: impact inflamma\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3372093023255814 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.4019607843137255 evidence GPT: comorbid metabo\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35789473684210527 evidence GPT: role genetic ri\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.36082474226804123 evidence GPT: impact hyperten\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3106796116504854 evidence GPT: impact chronic \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3548387096774194 evidence GPT: effects diabete\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.28865979381443296 evidence GPT: relationship al\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34444444444444444 evidence GPT: impact depressi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32978723404255317 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35789473684210527 evidence GPT: complex relatio\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3333333333333333 evidence GPT: thyroid disorde\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3118279569892473 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30851063829787234 evidence GPT: relationship al\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.330188679245283 evidence GPT: impact anxiety \n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.38235294117647056 evidence GPT: role inflammati\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34328358208955223 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.417910447761194 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34375 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.31343283582089554 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34210526315789475 evidence GPT: association alz\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32051282051282054 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.28169014084507044 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3684210526315789 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3333333333333333 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35135135135135137 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.328125 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03220999999999999\n",
      "distance to range 2:  0.04259888888888891\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.125 , evidence: GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32432432432432434 evidence GPT: association alz\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3 evidence GPT: association alz\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3076923076923077 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30158730158730157 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3 evidence GPT: depression alzh\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29333333333333333 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3815789473684211 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3283582089552239 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3246753246753247 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30985915492957744 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2987012987012987 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "---------------COUNTS---------------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  49\n",
      "CHATGPT CLASSIFIED:  39\n",
      "FAILED_TO_CLASSIFY:  0\n",
      "GPT MISCLASSIFIED AS PUBMED:  11\n",
      "PUBMED MISCLASSIFIED AS GPT:  1\n",
      "-------------------------------------------------\n",
      "------------- %PERCENTAGE% -----------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  0.98\n",
      "CHATGPT CLASSIFIED:  0.78\n",
      "FAILED_TO_CLASSIFY:  0.0\n",
      "GPT MISCLASSIFIED AS PUBMED:  0.22\n",
      "PUBMED MISCLASSIFIED AS GPT:  0.02\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# two classes classification\n",
    "\n",
    "two_articles_dataset = []\n",
    "\n",
    "for pubmed_article in pubmed_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('PUBMED: ' + pubmed_article)\n",
    "\n",
    "for gpt_article in gpt_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('GPT: ' + gpt_article)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "chatgpt_class = 0\n",
    "pubmed_class = 0\n",
    "\n",
    "failed_to_classify = 0\n",
    "misclassified_as_gpt = 0\n",
    "misclassified_as_pubmed = 0\n",
    "\n",
    "# RANGE 1: PUBMED\n",
    "range1_start = pubmed_min_value\n",
    "range1_end = pubmed_max_value\n",
    "\n",
    "# RANGE 2: GPT\n",
    "range2_start = gpt_min_value\n",
    "range2_end = gpt_max_value\n",
    "\n",
    "\n",
    "for article in two_articles_dataset:\n",
    "    \n",
    "    gpt_ratio_val    = fit_an_article(article, gpt_lcc)\n",
    "    pubmed_ratio_val = fit_an_article(article, pubmed_lcc)\n",
    "    \n",
    "    # Classifying GPT\n",
    "    if gpt_ratio_val >= range2_start and ratio_val <= range2_end :       \n",
    "        if article[:20].startswith('GPT'):\n",
    "            chatgpt_class+=1\n",
    "            print('ChatGPT : Fit ratio for individual articles: ', gpt_ratio_val, 'evidence', article[:20])\n",
    "        else:\n",
    "            misclassified_as_pubmed+=1\n",
    "            \n",
    "    # Classifying PUBMED\n",
    "    elif pubmed_ratio_val >= range1_start and ratio_val <= range1_end:\n",
    "        if article[:20].startswith('PUBMED'):\n",
    "            pubmed_class += 1\n",
    "            print('PUBMED : Fit ratio for individual articles: ', pubmed_ratio_val, 'evidence', article[:20])\n",
    "        else: \n",
    "            misclassified_as_gpt+=1\n",
    "        \n",
    "    else:\n",
    "        # Calculate distances\n",
    "        distance_to_range1 = distance_to_range(pubmed_ratio_val, range1_start, range1_end)\n",
    "        distance_to_range2 = distance_to_range(gpt_ratio_val, range2_start, range2_end) \n",
    "        \n",
    "        print('distance to range 1: ', distance_to_range1)\n",
    "        print('distance to range 2: ', distance_to_range2)        \n",
    "        \n",
    "        # RANGE 1: PUBMED SHOULD WIN\n",
    "        if distance_to_range1 < distance_to_range2:\n",
    "            if article[:20].startswith('GPT'):\n",
    "                misclassified_as_gpt+=1\n",
    "                print('PUBMED PREDICTED INCORRECTLY => ', 'ratio:', pubmed_ratio_val ,', evidence:', article[:20])                \n",
    "            else:   \n",
    "                # count+=1\n",
    "                pubmed_class += 1\n",
    "                print('PUBMED CLASS PREDICTED => ', 'ratio:', pubmed_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "        # RANGE 2: GPT SHOULD WIN\n",
    "        elif distance_to_range2 < distance_to_range1:\n",
    "            if article[:20].startswith('PUBMED'):                \n",
    "                misclassified_as_pubmed+=1\n",
    "                print('GPT PREDICTED INCORRECTLY => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])                     \n",
    "            else:\n",
    "                chatgpt_class += 1\n",
    "                print('GPT CLASS PREDICTED => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "    print(' -------------------------------- ')\n",
    "    \n",
    "    \n",
    "print('---------------COUNTS---------------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed) \n",
    "print('-------------------------------------------------') \n",
    "    \n",
    "    \n",
    "print('------------- %PERCENTAGE% -----------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class/50)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class/50)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify/50)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt/50)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed/50) \n",
    "print('-------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4cf700-bb3e-46fd-8adc-5b3550bc8237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d462dc-e60a-4c36-9568-a2d1dd56a5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f254d-6512-411f-8b80-5d21a915bdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64b2b7-612c-4917-a5ce-a2488ab0036a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d35cc-1183-4ffb-a3b0-a0d371abd649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d74167-c2dd-4a37-8e9c-1be7347dec5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dede74a-d617-41c5-b9f7-62862e5a9531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
