{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ee5f81f7-c915-4754-9a11-e4042245f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements \n",
    "import json\n",
    "import obonet\n",
    "from itertools import combinations \n",
    "from Bio import Medline\n",
    "import networkx as nx\n",
    "import string\n",
    "from textblob import TextBlob  \n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import tree\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "63974b6d-8293-4445-ab3a-439a2609ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment configurations\n",
    "MAX_NUMBER_ARTICLE = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8ce8bc1a-8191-4147-8c82-f02f98486f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing a medline file \n",
    "def parse_medline_rmap(medline_file):    \n",
    "    map_abstracts = {}    \n",
    "    pmid = ''\n",
    "    abstract = ''  \n",
    "    with open(medline_file) as medline_handle:\n",
    "        records = Medline.parse(medline_handle)\n",
    "        for record in records:         \n",
    "            keys = record.keys()            \n",
    "            if 'PMID' in keys and 'AB' in keys: \n",
    "\n",
    "                pmid = record['PMID']\n",
    "                abstract = record['AB']\n",
    "                \n",
    "                map_abstracts[pmid] = abstract.lower()\n",
    "    return map_abstracts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "85f0e020-a6bb-4b94-b88d-b7500d99de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_gpt_api_data(json_file):\n",
    "\n",
    "    json_records_map = {}\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Now json_data is a list of dictionaries, each representing an item in the array\n",
    "    for item in json_data:\n",
    "        gpt_id = item['GPT-ID']\n",
    "        title = item['Title']\n",
    "        abstract = item['Abstract']\n",
    "        # json_records_map[gpt_id]=(title + \" \" + abstract)\n",
    "        json_records_map[gpt_id]=(title + \" \" + abstract)        \n",
    "    return json_records_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "13080a90-1415-4f9e-ae19-a183512273b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "      \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e603b3a6-895e-4887-9d4f-611ce9a333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_abstracts = parse_medline_rmap('../dataset/pubmed-cancerandc-set-2010-2014.txt')\n",
    "cgpt_abstracts = parse_json_gpt_api_data('../dataset/cancer-gpt-apis.txt')\n",
    "\n",
    "# cleaning PubMed articles from special characters\n",
    "clean_pubmed_articles = []\n",
    "for abst in list(pubmed_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_pubmed_articles.append(cleaned)\n",
    "    \n",
    "# cleaning chatGPT articles from special characters\n",
    "clean_chatGPT_articles = []\n",
    "for abst in list(cgpt_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_chatGPT_articles.append(cleaned)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b71e1951-6355-4495-96f3-f11c6de17793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a patient between and years of age died following a polypectomy as part of a colonoscopy surveillance programme for previous polyps as a consequence of this adverse event we have amended our local guidelines while perforation is a recognised complication of polypectomy it was felt that the decision taken to remove the polyp was incorrect the decision to remove a polyp should be at the endoscopists clinical discretion and should depend on polyp size the patients age and comorbidities and their performance status we recommend that polyps mm in size should be regarded as lowrisk polyps and that polypectomy of lowrisk polyps are not essential in patients aged years and older polypectomy of highrisk polyps in patients aged years and older should only be undertaken by experienced endoscopists and with appropriate discussion with the patient prior to the procedure patients aged years should be dissuaded from having further colonoscopic surveillance and should not be included in polyp detection rate reports to ensure that polypectomy decisions are not influenced by performance monitoring we recommend other endoscopy units review their local practice and consider introducing these or similar guidelines to reduce risk to older patients we also recommend that the british society of gastroenterology should include more specific guidance on surveillance and polypectomy in the older patient when the guidance is next reviewed\n"
     ]
    }
   ],
   "source": [
    "print(clean_pubmed_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7cd597f6-5b64-465a-940a-22512d16f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "special_list = ['abstract']\n",
    "\n",
    "def stopwords_rem_pubmed(clean_pubmed_training):\n",
    "    stopped_pubmed_training = []\n",
    "    for abst in clean_pubmed_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if token not in stop_words:\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_pubmed_training.append(valid_rec)\n",
    "    return stopped_pubmed_training\n",
    "    \n",
    "    \n",
    "def stopwords_rem_chatGPT_dataset(clean_chatGPT):    \n",
    "    stopped_chatGPT_training = []\n",
    "    for abst in clean_chatGPT_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if (token not in stop_words) and (token not in special_list):\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_chatGPT_training.append(valid_rec)   \n",
    "    return stopped_chatGPT_training\n",
    "\n",
    "\n",
    "def stopwords_rem_chatGPT_article(clean_chatGPT_article):    \n",
    "    stopped_chatGPT_training = []\n",
    "    valid_l = []\n",
    "    valid_rec = []\n",
    "    blob_object = TextBlob(clean_chatGPT_article)\n",
    "    list_tokens = blob_object.words\n",
    "\n",
    "    for token in list_tokens:        \n",
    "        if (token not in stop_words) and (token not in special_list):\n",
    "            valid_l.append(token)            \n",
    "    valid_rec = ' '.join(valid_l)\n",
    "    # stopped_chatGPT_training.append(valid_rec)   \n",
    "    return str(valid_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "756e0025-68d5-4820-afbb-3c220042d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202\n"
     ]
    }
   ],
   "source": [
    "pubmed_articles_ready = stopwords_rem_pubmed(clean_pubmed_articles)\n",
    "\n",
    "# print(len(stopped_pubmed_training))  \n",
    "gpt_articles_ready = []\n",
    "for article in clean_chatGPT_articles:\n",
    "    gpt_articles_ready.append(stopwords_rem_chatGPT_article(article))\n",
    "print(len(gpt_articles_ready))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8345cb7a-ac87-4af6-aafe-5fb4d2babe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pubmed_articles_ready[0])\n",
    "# print('-----')\n",
    "# print(gpt_articles_ready[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d5ffe92e-af59-4d04-b6c3-9c318836e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting PubMed bigrams\n",
    "def compute_bigrams(training_articles):\n",
    "    list_bigrams = []\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range =(2, 2))\n",
    "    X1 = vectorizer.fit_transform(training_articles)\n",
    "    features = (vectorizer.get_feature_names_out())\n",
    "    # print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "    # Applying TFIDF\n",
    "    # You can still get n-grams here\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "    X2 = vectorizer.fit_transform(training_articles)\n",
    "    scores = (X2.toarray())\n",
    "    # print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "    # Getting top ranking features\n",
    "    sums = X2.sum(axis = 0)\n",
    "    data1 = []\n",
    "    for col, term in enumerate(features):\n",
    "        data1.append( (term, sums[0, col] ))\n",
    "    ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "    words = (ranking.sort_values('rank', ascending = False))\n",
    "\n",
    "    bigram_ranks = {}\n",
    "    for index, row in words.iterrows():\n",
    "        # print(row['term'],'\\t\\t\\t',  row['rank'])\n",
    "\n",
    "        splits = row['term'].split()\n",
    "        bigram_ranks[row['rank']] = (splits[0], splits[1])\n",
    "\n",
    "    count = 0    \n",
    "    for k, v in bigram_ranks.items():\n",
    "        # if count < MAX_NUMBER_BIGRAMS:\n",
    "        #     # print(k,'\\t',  v)\n",
    "        #     count += 1\n",
    "        list_bigrams.append(v)\n",
    "    return bigram_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "7ff019e0-a5ba-4c93-b9ba-62f7b9065699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_model(training_articles):\n",
    "    bigrams_map_training = compute_bigrams(training_articles)\n",
    "    gpt_training_bigrams = bigrams_map_training.values()\n",
    "    \n",
    "    graph_training_model = nx.Graph()\n",
    "    graph_training_model.add_edges_from(list(gpt_training_bigrams))\n",
    "    \n",
    "    return graph_training_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ded8c1c2-3712-48c4-803c-7f828e9b62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT Training Model --------\n",
      "Original node count:  559\n",
      "Original edge count:  1050\n",
      " -------- PubMed Training Model --------\n",
      "Original node count:  755\n",
      "Original edge count:  803\n"
     ]
    }
   ],
   "source": [
    "# construct a network training model from both datasets (gpt and pubmed)\n",
    "\n",
    "gpt_training_model = construct_training_model(gpt_articles_ready[:100])\n",
    "pubmed_training_model = construct_training_model(pubmed_articles_ready[:100])\n",
    "\n",
    "# ----------   Verifying GPT Training  Model ----------# \n",
    "print(' -------- GPT Training Model --------')\n",
    "node_count = len(gpt_training_model.nodes())\n",
    "edge_count = len(gpt_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)\n",
    "\n",
    "# ----------   Verifying PubMed Training  Model ----------# \n",
    "print(' -------- PubMed Training Model --------')\n",
    "node_count = len(pubmed_training_model.nodes())\n",
    "edge_count = len(pubmed_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5ffffcb0-d548-4376-96d7-9354472b35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giant_lcc(graph_training_model):\n",
    "    gcc = sorted(nx.connected_components(graph_training_model), key=len, reverse=True)\n",
    "    giant_cc = graph_training_model.subgraph(gcc[0])\n",
    "    return giant_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b7da8e22-e4a9-4973-922d-bd411fa4ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT GIANT LCC Graph --------\n",
      "Graph with 489 nodes and 1008 edges\n",
      " -------- PUBMED GIANT LCC Graph --------\n",
      "Graph with 504 nodes and 664 edges\n"
     ]
    }
   ],
   "source": [
    "print(' -------- GPT GIANT LCC Graph --------')\n",
    "gpt_lcc = get_giant_lcc(gpt_training_model)\n",
    "print(gpt_lcc)\n",
    "\n",
    "print(' -------- PUBMED GIANT LCC Graph --------')\n",
    "pubmed_lcc = get_giant_lcc(pubmed_training_model)\n",
    "print(pubmed_lcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0950208d-636b-4784-b0a0-c1b8f4561eae",
   "metadata": {},
   "outputs": [],
   "source": [
    " # STEP2: -- compute individual articles bigrams -------\n",
    "def calibrate_model(ds_label, begin_index, end_index, training_graph, calibrate_set):\n",
    "\n",
    "    training_graph_copy = training_graph.copy() \n",
    "\n",
    "    ratios_added_per_fold = []\n",
    "    for abst in calibrate_set[begin_index:end_index]:\n",
    "        \n",
    "        tokens = nltk.word_tokenize(abst)\n",
    "\n",
    "        # compute the bigrams\n",
    "        bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "        # -------  check if the giant has the bigram components, add new edge \n",
    "        # -------          otherwise, don't add new edges\n",
    "        # -------  count how many nodes            \n",
    "        count = 0\n",
    "        added_edges = []\n",
    "        for bigram in bigrams:\n",
    "\n",
    "            if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "                if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                    training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                    count += 1\n",
    "                    added_edges.append((bigram[0], bigram[1]))\n",
    "        ratio_ = count / len(tokens)        \n",
    "        \n",
    "        ratios_added_per_fold.append(ratio_) \n",
    "        \n",
    "        training_graph_copy.remove_edges_from(added_edges)      \n",
    "    return ratios_added_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "bd014f6c-9a92-4e85-a279-78a8258b941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(tst_set_list):\n",
    "    average = sum(tst_set_list) / len(tst_set_list)        \n",
    "    formatted_avg = float(\"{:.5f}\".format(average))        \n",
    "    return formatted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "df86f04a-4d5e-4535-b810-1cadc3bb0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27947\n",
      "0.29009\n",
      "0.26738\n",
      "0.25622\n",
      "0.25135\n",
      "0.28374\n",
      "0.27115\n",
      "0.26867\n",
      "0.25155\n",
      "0.25541\n",
      "0.24857\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "gpt_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, gpt_lcc, gpt_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_g = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_g)\n",
    "        gpt_means.append(tst_mean_g)\n",
    "        \n",
    "gpt_min_value = min(gpt_means)\n",
    "gpt_max_value = max(gpt_means)        \n",
    "# print(gpt_means)\n",
    "for ratio in gpt_means:\n",
    "    print(ratio)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d67982e6-5346-48f0-8317-96fb6c705464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14473\n",
      "0.14822\n",
      "0.15012\n",
      "0.14254\n",
      "0.13541\n",
      "0.14044\n",
      "0.14316\n",
      "0.1455\n",
      "0.14774\n",
      "0.14771\n",
      "0.13928\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pubmed_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, pubmed_lcc, pubmed_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_p = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_p)\n",
    "        pubmed_means.append(tst_mean_p)\n",
    "        \n",
    "pubmed_min_value = min(pubmed_means)\n",
    "pubmed_max_value = max(pubmed_means)        \n",
    "# print(gpt_means)\n",
    "for ratio in pubmed_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ed318152-06ef-4e6e-8a70-aba811af55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_an_article(article_text, training_graph):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy()\n",
    "    \n",
    "    # chat_no_added_edges = []\n",
    "    # for abst in stopped_pubmed_training[begin_index:end_index]:\n",
    "\n",
    "    tokens = nltk.word_tokenize(article_text)\n",
    "\n",
    "    # compute the bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # -------  check if the giant has the bigram components, add new edge \n",
    "    # -------          otherwise, don't add new edges\n",
    "    # -------  count how many nodes    \n",
    "\n",
    "    count = 0\n",
    "    added_edges = []\n",
    "    for bigram in bigrams:\n",
    "\n",
    "        if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "            if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                count += 1\n",
    "                added_edges.append((bigram[0], bigram[1]))\n",
    "    ratio_ = count / len(tokens)        \n",
    "    training_graph_copy.remove_edges_from(added_edges)\n",
    "        \n",
    "    return ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f5000d5a-e4bf-4940-a3ec-7a68f557d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.018\n",
      "CORRECT CLASSIFIED:  0.982\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# This will detect the misclassification\n",
    "# The average of the list is: 0.15012\n",
    "# The average of the list is: 0.14254\n",
    "# The average of the list is: 0.13541\n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in gpt_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= pubmed_min_value and ratio_val <= pubmed_max_value :       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/1000)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/1000)   \n",
    "print('-------------------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "cc6b890e-7be7-4214-ab38-809eae6018b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.006\n",
      "CORRECT CLASSIFIED:  0.994\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The average of the list is: 0.29009\n",
    "# The average of the list is: 0.26738\n",
    "# The average of the list is: 0.25622\n",
    "# The average of the list is: 0.25135\n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in pubmed_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= gpt_min_value and ratio_val <= gpt_max_value :       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/1000)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/1000)   \n",
    "print('-------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5694aea0-da2d-4e37-abd8-6c4e8478c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_range(point, range_start, range_end):\n",
    "    # Calculate the distance to the nearest endpoint of the range\n",
    "    distance = min(abs(point - range_start), abs(point - range_end))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "cf2676ac-2517-48ac-9ae9-ea9c1fa01549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUBMED : Fit ratio for individual articles:  0.1553398058252427 evidence PUBMED: patients met\n",
      " -------------------------------- \n",
      "distance to range 1:  0.022076666666666675\n",
      "distance to range 2:  0.17523666666666668\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11333333333333333 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.022827218543046354\n",
      "distance to range 2:  0.10949715231788082\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11258278145695365 , evidence: PUBMED: purpose purp\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02370787234042554\n",
      "distance to range 2:  0.21665510638297875\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11170212765957446 , evidence: PUBMED: purpose stud\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2391304347826087 evidence PUBMED: diffuse larg\n",
      " -------------------------------- \n",
      "distance to range 1:  0.014720344827586204\n",
      "distance to range 2:  0.14512172413793106\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1206896551724138 , evidence: PUBMED: antiangiogen\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02002538461538461\n",
      "distance to range 2:  0.1139546153846154\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11538461538461539 , evidence: PUBMED: venous throm\n",
      " -------------------------------- \n",
      "distance to range 1:  0.037537659574468085\n",
      "distance to range 2:  0.2060168085106383\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09787234042553192 , evidence: PUBMED: background p\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03231721649484537\n",
      "distance to range 2:  0.18671432989690723\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10309278350515463 , evidence: PUBMED: background e\n",
      " -------------------------------- \n",
      "distance to range 1:  0.047781134020618565\n",
      "distance to range 2:  0.1970236082474227\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08762886597938144 , evidence: PUBMED: objectives e\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2713178294573643 evidence PUBMED: previous stu\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07141\n",
      "distance to range 2:  0.21657\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.064 , evidence: PUBMED: uremia resul\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15469613259668508 evidence PUBMED: prior studie\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06818310924369748\n",
      "distance to range 2:  0.11411621848739498\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06722689075630252 , evidence: PUBMED: fibrolamella\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06282935483870968\n",
      "distance to range 2:  0.07115064516129033\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07258064516129033 , evidence: PUBMED: background v\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1402439024390244 evidence PUBMED: introducton \n",
      " -------------------------------- \n",
      "distance to range 1:  0.0017466336633663315\n",
      "distance to range 2:  0.149560099009901\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13366336633663367 , evidence: PUBMED: context whet\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.18493150684931506 evidence PUBMED: objectives a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14746543778801843 evidence PUBMED: background i\n",
      " -------------------------------- \n",
      "distance to range 1:  0.018222500000000003\n",
      "distance to range 2:  0.18607\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1171875 , evidence: PUBMED: recently dia\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0404732911392405\n",
      "distance to range 2:  0.22325354430379749\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0949367088607595 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1907514450867052 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16326530612244897 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17391304347826086 evidence PUBMED: background e\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02517377952755906\n",
      "distance to range 2:  0.20919992125984255\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11023622047244094 , evidence: PUBMED: background v\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1497005988023952 evidence PUBMED: background u\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02002538461538461\n",
      "distance to range 2:  0.21651871794871796\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11538461538461539 , evidence: PUBMED: background d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.22807017543859648 evidence PUBMED: background c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.05256976331360946\n",
      "distance to range 2:  0.16572976331360947\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08284023668639054 , evidence: PUBMED: background i\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2676056338028169 evidence PUBMED: background p\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14375 evidence PUBMED: aim study ma\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24025974025974026 evidence PUBMED: aim study co\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03603111801242237\n",
      "distance to range 2:  0.18024701863354037\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09937888198757763 , evidence: PUBMED: purpose comp\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2689655172413793 evidence PUBMED: purpose sora\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.22602739726027396 evidence PUBMED: background i\n",
      " -------------------------------- \n",
      "distance to range 1:  0.034781069182389934\n",
      "distance to range 2:  0.18567691823899374\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10062893081761007 , evidence: PUBMED: purpose elde\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15163934426229508 evidence PUBMED: background t\n",
      " -------------------------------- \n",
      "distance to range 1:  0.10683857142857144\n",
      "distance to range 2:  0.13428428571428574\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.02857142857142857 , evidence: PUBMED: chronic hepa\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02002538461538461\n",
      "distance to range 2:  0.1551634065934066\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11538461538461539 , evidence: PUBMED: anxiety diso\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14655172413793102 evidence PUBMED: goal evaluat\n",
      " -------------------------------- \n",
      "distance to range 1:  0.028065367231638425\n",
      "distance to range 2:  0.14122536723163842\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10734463276836158 , evidence: PUBMED: objectives s\n",
      " -------------------------------- \n",
      "distance to range 1:  0.028267142857142866\n",
      "distance to range 2:  0.1771414285714286\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10714285714285714 , evidence: PUBMED: although liv\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.22535211267605634 evidence PUBMED: objectives u\n",
      " -------------------------------- \n",
      "distance to range 1:  0.051376386554621845\n",
      "distance to range 2:  0.21495655462184876\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08403361344537816 , evidence: PUBMED: aim analyze \n",
      " -------------------------------- \n",
      "distance to range 1:  0.08215556213017752\n",
      "distance to range 2:  0.15389544378698228\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.05325443786982249 , evidence: PUBMED: cancer pain \n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17676767676767677 evidence PUBMED: esophageal c\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1724137931034483 evidence PUBMED: gastric canc\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03196172413793104\n",
      "distance to range 2:  0.16236310344827587\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10344827586206896 , evidence: PUBMED: study aimed \n",
      " -------------------------------- \n",
      "distance to range 1:  0.0615844966442953\n",
      "distance to range 2:  0.16132167785234902\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0738255033557047 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03284589743589744\n",
      "distance to range 2:  0.09472384615384616\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10256410256410256 , evidence: PUBMED: although rel\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2905982905982906 evidence GPT: cardiovascular \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3157894736842105 evidence GPT: association ova\n",
      " -------------------------------- \n",
      "distance to range 1:  0.05505285714285714\n",
      "distance to range 2:  0.03428428571428574\n",
      "GPT CLASS PREDICTED =>  ratio: 0.21428571428571427 , evidence: GPT: complex interac\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07036934959349593\n",
      "distance to range 2:  0.012797642276422772\n",
      "GPT CLASS PREDICTED =>  ratio: 0.23577235772357724 , evidence: GPT: thyroid cancer \n",
      " -------------------------------- \n",
      "distance to range 1:  0.1116004761904762\n",
      "distance to range 2:  0.08983984126984129\n",
      "GPT CLASS PREDICTED =>  ratio: 0.15873015873015872 , evidence: GPT: association mel\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0664444827586207\n",
      "distance to range 2:  0.05891482758620692\n",
      "GPT CLASS PREDICTED =>  ratio: 0.1896551724137931 , evidence: GPT: association bla\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.27049180327868855 evidence GPT: association pro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2727272727272727 evidence GPT: association bre\n",
      " -------------------------------- \n",
      "distance to range 1:  0.030571290322580644\n",
      "distance to range 2:  0.006634516129032264\n",
      "GPT CLASS PREDICTED =>  ratio: 0.24193548387096775 , evidence: GPT: impact obesity \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2727272727272727 evidence GPT: association non\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32432432432432434 evidence GPT: bidirectional r\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2636363636363636 evidence GPT: association lun\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3508771929824561 evidence GPT: association hep\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.288135593220339 evidence GPT: association gas\n",
      " -------------------------------- \n",
      "distance to range 1:  0.056462631578947375\n",
      "distance to range 2:  0.029271754385964938\n",
      "GPT CLASS PREDICTED =>  ratio: 0.21929824561403508 , evidence: GPT: relationship pr\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2909090909090909 evidence GPT: association bre\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.275 evidence GPT: association pan\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32051282051282054 evidence GPT: role inflammati\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34065934065934067 evidence GPT: genetic suscept\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.42696629213483145 evidence GPT: impact cancer t\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3076923076923077 evidence GPT: metabolic alter\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.26373626373626374 evidence GPT: impact cancer c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3617021276595745 evidence GPT: psychosocial fa\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04845347826086957\n",
      "distance to range 2:  0.09639608695652174\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.08695652173913043 , evidence: GPT: impact cancer c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.09694846153846154\n",
      "distance to range 2:  0.06908282051282053\n",
      "GPT CLASS PREDICTED =>  ratio: 0.1794871794871795 , evidence: GPT: relationship in\n",
      " -------------------------------- \n",
      "distance to range 1:  0.021374912280701763\n",
      "distance to range 2:  0.05558754385964915\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.11403508771929824 , evidence: GPT: association chr\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03371508474576271\n",
      "distance to range 2:  0.045180169491525424\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.1016949152542373 , evidence: GPT: impact chronic \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29069767441860467 evidence GPT: impact cancer c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3026315789473684 evidence GPT: hepatic comorbi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3037974683544304 evidence GPT: link obesity ca\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.26582278481012656 evidence GPT: impact socioeco\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29577464788732394 evidence GPT: immunotherapy c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2631578947368421 evidence GPT: role chronic in\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2604166666666667 evidence GPT: impact cancer s\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.43157894736842106 evidence GPT: targeted therap\n",
      " -------------------------------- \n",
      "distance to range 1:  0.05586454545454546\n",
      "distance to range 2:  0.0326609090909091\n",
      "GPT CLASS PREDICTED =>  ratio: 0.2159090909090909 , evidence: GPT: gut microbiota \n",
      " -------------------------------- \n",
      "distance to range 1:  0.0012636585365853603\n",
      "distance to range 2:  0.041252926829268316\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.13414634146341464 , evidence: GPT: impact cancer c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03863580645161291\n",
      "distance to range 2:  0.033516236559139795\n",
      "GPT CLASS PREDICTED =>  ratio: 0.21505376344086022 , evidence: GPT: impact cancer c\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.36893203883495146 evidence GPT: exploring relat\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30392156862745096 evidence GPT: association can\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3584905660377358 evidence GPT: comorbidities l\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3142857142857143 evidence GPT: understanding b\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2857142857142857 evidence GPT: impact cancer r\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35454545454545455 evidence GPT: impact cancer n\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3333333333333333 evidence GPT: relationship ca\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32 evidence GPT: comorbidities t\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2621359223300971 evidence GPT: interplay cance\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.37373737373737376 evidence GPT: gastrointestina\n",
      " -------------------------------- \n",
      "---------------COUNTS---------------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  50\n",
      "CHATGPT CLASSIFIED:  44\n",
      "FAILED_TO_CLASSIFY:  0\n",
      "GPT MISCLASSIFIED AS PUBMED:  6\n",
      "PUBMED MISCLASSIFIED AS GPT:  0\n",
      "-------------------------------------------------\n",
      "------------- %PERCENTAGE% -----------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  1.0\n",
      "CHATGPT CLASSIFIED:  0.88\n",
      "FAILED_TO_CLASSIFY:  0.0\n",
      "GPT MISCLASSIFIED AS PUBMED:  0.12\n",
      "PUBMED MISCLASSIFIED AS GPT:  0.0\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# two classes classification\n",
    "\n",
    "two_articles_dataset = []\n",
    "\n",
    "for pubmed_article in pubmed_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('PUBMED: ' + pubmed_article)\n",
    "\n",
    "for gpt_article in gpt_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('GPT: ' + gpt_article)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "chatgpt_class = 0\n",
    "pubmed_class = 0\n",
    "\n",
    "failed_to_classify = 0\n",
    "misclassified_as_gpt = 0\n",
    "misclassified_as_pubmed = 0\n",
    "\n",
    "\n",
    "# RANGE 1: PUBMED\n",
    "range1_start = pubmed_min_value\n",
    "range1_end = pubmed_max_value\n",
    "\n",
    "# RANGE 2: GPT\n",
    "range2_start = gpt_min_value\n",
    "range2_end = gpt_max_value\n",
    "\n",
    "for article in two_articles_dataset:\n",
    "    \n",
    "    gpt_ratio_val    = fit_an_article(article, gpt_lcc)\n",
    "    pubmed_ratio_val = fit_an_article(article, pubmed_lcc)\n",
    "    \n",
    "    # Classifying GPT\n",
    "    if gpt_ratio_val >= range2_start and ratio_val <= range2_end :       \n",
    "        if article[:20].startswith('GPT'):\n",
    "            chatgpt_class+=1\n",
    "            print('ChatGPT : Fit ratio for individual articles: ', gpt_ratio_val, 'evidence', article[:20])\n",
    "        else:\n",
    "            misclassified_as_pubmed+=1\n",
    "            \n",
    "    # Classifying PUBMED\n",
    "    elif pubmed_ratio_val >= range1_start and ratio_val <= range1_end:\n",
    "        if article[:20].startswith('PUBMED'):\n",
    "            pubmed_class += 1\n",
    "            print('PUBMED : Fit ratio for individual articles: ', pubmed_ratio_val, 'evidence', article[:20])\n",
    "        else: \n",
    "            misclassified_as_gpt+=1\n",
    "        \n",
    "    else:\n",
    "        # Calculate distances\n",
    "        distance_to_range1 = distance_to_range(pubmed_ratio_val, range1_start, range1_end)\n",
    "        distance_to_range2 = distance_to_range(gpt_ratio_val, range2_start, range2_end) \n",
    "        \n",
    "        print('distance to range 1: ', distance_to_range1)\n",
    "        print('distance to range 2: ', distance_to_range2)        \n",
    "        \n",
    "        # RANGE 1: PUBMED SHOULD WIN\n",
    "        if distance_to_range1 < distance_to_range2:\n",
    "            if article[:20].startswith('GPT'):\n",
    "                misclassified_as_gpt+=1\n",
    "                print('PUBMED PREDICTED INCORRECTLY => ', 'ratio:', pubmed_ratio_val ,', evidence:', article[:20])                \n",
    "            else:   \n",
    "                # count+=1\n",
    "                pubmed_class += 1\n",
    "                print('PUBMED CLASS PREDICTED => ', 'ratio:', pubmed_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "        # RANGE 2: GPT SHOULD WIN\n",
    "        elif distance_to_range2 < distance_to_range1:\n",
    "            if article[:20].startswith('PUBMED'):                \n",
    "                misclassified_as_pubmed+=1\n",
    "                print('GPT PREDICTED INCORRECTLY => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])                     \n",
    "            else:\n",
    "                chatgpt_class += 1\n",
    "                print('GPT CLASS PREDICTED => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "    print(' -------------------------------- ')\n",
    "    \n",
    "    \n",
    "print('---------------COUNTS---------------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed) \n",
    "print('-------------------------------------------------') \n",
    "    \n",
    "    \n",
    "print('------------- %PERCENTAGE% -----------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class/50)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class/50)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify/50)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt/50)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed/50) \n",
    "print('-------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95ed11-089f-45d6-bacc-f0e58535e625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14d210-ce05-4088-88f9-f38157e0e246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbf331-ff6a-4466-9f61-904da5331384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19179b3d-b9d9-4cfe-8e88-2882c3599c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be860967-f633-4351-bdc3-ecee55f7d9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb625c62-360b-4be7-9df4-5f6630f473b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
