{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ee5f81f7-c915-4754-9a11-e4042245f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements \n",
    "import json\n",
    "import obonet\n",
    "from itertools import combinations \n",
    "from Bio import Medline\n",
    "import networkx as nx\n",
    "import string\n",
    "from textblob import TextBlob  \n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import tree\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "63974b6d-8293-4445-ab3a-439a2609ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment configurations\n",
    "# MAX_NUMBER_BIGRAMS = 30\n",
    "MAX_NUMBER_ARTICLE = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8ce8bc1a-8191-4147-8c82-f02f98486f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing a medline file \n",
    "def parse_medline_rmap(medline_file):    \n",
    "    map_abstracts = {}    \n",
    "    pmid = ''\n",
    "    abstract = ''  \n",
    "    with open(medline_file) as medline_handle:\n",
    "        records = Medline.parse(medline_handle)\n",
    "        for record in records:         \n",
    "            keys = record.keys()            \n",
    "            if 'PMID' in keys and 'AB' in keys: \n",
    "\n",
    "                pmid = record['PMID']\n",
    "                abstract = record['AB']\n",
    "                \n",
    "                map_abstracts[pmid] = abstract.lower()\n",
    "    return map_abstracts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "85f0e020-a6bb-4b94-b88d-b7500d99de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_gpt_api_data(json_file):\n",
    "\n",
    "    json_records_map = {}\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Now json_data is a list of dictionaries, each representing an item in the array\n",
    "    for item in json_data:\n",
    "        gpt_id = item['GPT-ID']\n",
    "        title = item['Title']\n",
    "        abstract = item['Abstract']\n",
    "        # json_records_map[gpt_id]=(title + \" \" + abstract)\n",
    "        json_records_map[gpt_id]=(title + \" \" + abstract)        \n",
    "    return json_records_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "13080a90-1415-4f9e-ae19-a183512273b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "      \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e603b3a6-895e-4887-9d4f-611ce9a333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_abstracts = parse_medline_rmap('../dataset/pubmed-alzheimers-set-2020-2024-present.txt')\n",
    "cgpt_abstracts = parse_json_gpt_api_data('../dataset/alz-gpt-apis.txt')\n",
    "\n",
    "# cleaning PubMed articles from special characters\n",
    "clean_pubmed_articles = []\n",
    "for abst in list(pubmed_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_pubmed_articles.append(cleaned)\n",
    "    \n",
    "# cleaning chatGPT articles from special characters\n",
    "clean_chatGPT_articles = []\n",
    "for abst in list(cgpt_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_chatGPT_articles.append(cleaned)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b71e1951-6355-4495-96f3-f11c6de17793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background recent reports suggest that by there will be an increase of around of cases affected by dementia in latin american countries a previous study in a southern region reported one of the highest prevalences of dementia in latin america objective to investigate the prevalence of mild cognitive impairment associated with low education rurality and demographic characteristics methods a crosssectional study recruited a communitydwelling sample of adults from rural and urban areas of two southern provinces of colombia from participants were assessed with a neuropsychological protocol validated in colombia to obtain general and regionspecific prevalence rates age sex schooling and socioeconomic level were considered and controlled for results most of the participants reported low education and socioeconomic level the participation of women was higher it was determined that the prevalence of mild cognitive impairment mci was with in the province of caquet followed by in the province of huila the amnestic mci represented the amnestic multidomain was the nonamnestic and the nonamnestic multidomain our participants reported comorbidities such as diabetes and hypertension we also observed a relationship between exposure to pesticides and mci conclusions we observed one of the highest prevalences of mci in latin america reported to date variables such as age gender and education proved risk factors for mci in the explored regions our findings are very much in line with recent studies that highlight the influence of noncanonical risk factors of dementia in underrepresented countries from latin america\n"
     ]
    }
   ],
   "source": [
    "print(clean_pubmed_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7cd597f6-5b64-465a-940a-22512d16f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "special_list = ['abstract']\n",
    "\n",
    "def stopwords_rem_pubmed(clean_pubmed_training):\n",
    "    stopped_pubmed_training = []\n",
    "    for abst in clean_pubmed_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if token not in stop_words:\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_pubmed_training.append(valid_rec)\n",
    "    return stopped_pubmed_training\n",
    "    \n",
    "    \n",
    "def stopwords_rem_chatGPT_dataset(clean_chatGPT):    \n",
    "    stopped_chatGPT_training = []\n",
    "    for abst in clean_chatGPT_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if (token not in stop_words) and (token not in special_list):\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_chatGPT_training.append(valid_rec)   \n",
    "    return stopped_chatGPT_training\n",
    "\n",
    "\n",
    "def stopwords_rem_chatGPT_article(clean_chatGPT_article):    \n",
    "    stopped_chatGPT_training = []\n",
    "    valid_l = []\n",
    "    valid_rec = []\n",
    "    blob_object = TextBlob(clean_chatGPT_article)\n",
    "    list_tokens = blob_object.words\n",
    "\n",
    "    for token in list_tokens:        \n",
    "        if (token not in stop_words) and (token not in special_list):\n",
    "            valid_l.append(token)            \n",
    "    valid_rec = ' '.join(valid_l)\n",
    "    # stopped_chatGPT_training.append(valid_rec)   \n",
    "    return str(valid_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "756e0025-68d5-4820-afbb-3c220042d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179\n"
     ]
    }
   ],
   "source": [
    "pubmed_articles_ready = stopwords_rem_pubmed(clean_pubmed_articles)\n",
    "\n",
    "# print(len(stopped_pubmed_training))  \n",
    "gpt_articles_ready = []\n",
    "for article in clean_chatGPT_articles:\n",
    "    gpt_articles_ready.append(stopwords_rem_chatGPT_article(article))\n",
    "print(len(gpt_articles_ready))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8345cb7a-ac87-4af6-aafe-5fb4d2babe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pubmed_articles_ready[0])\n",
    "# print('-----')\n",
    "# print(gpt_articles_ready[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d5ffe92e-af59-4d04-b6c3-9c318836e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting PubMed bigrams\n",
    "def compute_bigrams(training_articles):\n",
    "    list_bigrams = []\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range =(2, 2))\n",
    "    X1 = vectorizer.fit_transform(training_articles)\n",
    "    features = (vectorizer.get_feature_names_out())\n",
    "    # print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "    # Applying TFIDF\n",
    "    # You can still get n-grams here\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "    X2 = vectorizer.fit_transform(training_articles)\n",
    "    scores = (X2.toarray())\n",
    "    # print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "    # Getting top ranking features\n",
    "    sums = X2.sum(axis = 0)\n",
    "    data1 = []\n",
    "    for col, term in enumerate(features):\n",
    "        data1.append( (term, sums[0, col] ))\n",
    "    ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "    words = (ranking.sort_values('rank', ascending = False))\n",
    "\n",
    "    bigram_ranks = {}\n",
    "    for index, row in words.iterrows():\n",
    "        # print(row['term'],'\\t\\t\\t',  row['rank'])\n",
    "\n",
    "        splits = row['term'].split()\n",
    "        bigram_ranks[row['rank']] = (splits[0], splits[1])\n",
    "\n",
    "    count = 0    \n",
    "    for k, v in bigram_ranks.items():\n",
    "        # if count < MAX_NUMBER_BIGRAMS:\n",
    "        #     # print(k,'\\t',  v)\n",
    "        #     count += 1\n",
    "        list_bigrams.append(v)\n",
    "    return bigram_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7ff019e0-a5ba-4c93-b9ba-62f7b9065699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_model(training_articles):\n",
    "    bigrams_map_training = compute_bigrams(training_articles)\n",
    "    gpt_training_bigrams = bigrams_map_training.values()\n",
    "    \n",
    "    graph_training_model = nx.Graph()\n",
    "    graph_training_model.add_edges_from(list(gpt_training_bigrams))\n",
    "    \n",
    "    return graph_training_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ded8c1c2-3712-48c4-803c-7f828e9b62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT Training Model --------\n",
      "Original node count:  519\n",
      "Original edge count:  1194\n",
      " -------- PubMed Training Model --------\n",
      "Original node count:  817\n",
      "Original edge count:  958\n"
     ]
    }
   ],
   "source": [
    "# construct a network training model from both datasets (gpt and pubmed)\n",
    "\n",
    "gpt_training_model = construct_training_model(gpt_articles_ready[:100])\n",
    "pubmed_training_model = construct_training_model(pubmed_articles_ready[:100])\n",
    "\n",
    "# ----------   Verifying GPT Training  Model ----------# \n",
    "print(' -------- GPT Training Model --------')\n",
    "node_count = len(gpt_training_model.nodes())\n",
    "edge_count = len(gpt_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)\n",
    "\n",
    "# ----------   Verifying PubMed Training  Model ----------# \n",
    "print(' -------- PubMed Training Model --------')\n",
    "node_count = len(pubmed_training_model.nodes())\n",
    "edge_count = len(pubmed_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5ffffcb0-d548-4376-96d7-9354472b35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giant_lcc(graph_training_model):\n",
    "    gcc = sorted(nx.connected_components(graph_training_model), key=len, reverse=True)\n",
    "    giant_cc = graph_training_model.subgraph(gcc[0])\n",
    "    return giant_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b7da8e22-e4a9-4973-922d-bd411fa4ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT GIANT LCC Graph --------\n",
      "Graph with 479 nodes and 1170 edges\n",
      " -------- PUBMED GIANT LCC Graph --------\n",
      "Graph with 555 nodes and 812 edges\n"
     ]
    }
   ],
   "source": [
    "print(' -------- GPT GIANT LCC Graph --------')\n",
    "gpt_lcc = get_giant_lcc(gpt_training_model)\n",
    "print(gpt_lcc)\n",
    "\n",
    "print(' -------- PUBMED GIANT LCC Graph --------')\n",
    "pubmed_lcc = get_giant_lcc(pubmed_training_model)\n",
    "print(pubmed_lcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0950208d-636b-4784-b0a0-c1b8f4561eae",
   "metadata": {},
   "outputs": [],
   "source": [
    " # STEP2: -- compute individual articles bigrams -------\n",
    "def calibrate_model(ds_label, begin_index, end_index, training_graph, calibrate_set):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy() \n",
    "\n",
    "    ratios_added_per_fold = []\n",
    "    for abst in calibrate_set[begin_index:end_index]:\n",
    "        \n",
    "        tokens = nltk.word_tokenize(abst)\n",
    "\n",
    "        # compute the bigrams\n",
    "        bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "        # -------  check if the giant has the bigram components, add new edge \n",
    "        # -------          otherwise, don't add new edges\n",
    "        # -------  count how many nodes            \n",
    "        count = 0\n",
    "        added_edges = []\n",
    "        for bigram in bigrams:\n",
    "\n",
    "            if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "                if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                    training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                    count += 1\n",
    "                    added_edges.append((bigram[0], bigram[1]))\n",
    "        ratio_ = count / len(tokens)        \n",
    "        \n",
    "        ratios_added_per_fold.append(ratio_) \n",
    "        \n",
    "        training_graph_copy.remove_edges_from(added_edges)      \n",
    "    return ratios_added_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bd014f6c-9a92-4e85-a279-78a8258b941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(tst_set_list):\n",
    "    average = sum(tst_set_list) / len(tst_set_list)        \n",
    "    formatted_avg = float(\"{:.5f}\".format(average))        \n",
    "    return formatted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "df86f04a-4d5e-4535-b810-1cadc3bb0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of the list is: 0.27871\n",
      "The average of the list is: 0.30034\n",
      "The average of the list is: 0.30916\n",
      "The average of the list is: 0.28195\n",
      "The average of the list is: 0.28424\n",
      "The average of the list is: 0.29355\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "gpt_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, gpt_lcc, gpt_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_g = calc_mean(calb_ratios_list) \n",
    "        print(\"The average of the list is:\", tst_mean_g)\n",
    "        gpt_means.append(tst_mean_g)\n",
    "        \n",
    "gpt_min_value = min(gpt_means)\n",
    "gpt_max_value = max(gpt_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d67982e6-5346-48f0-8317-96fb6c705464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15681\n",
      "0.15192\n",
      "0.14786\n",
      "0.15084\n",
      "0.14679\n",
      "0.14245\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pubmed_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, pubmed_lcc, pubmed_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_p = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_p)\n",
    "        pubmed_means.append(tst_mean_p)\n",
    "        \n",
    "pubmed_min_value = min(pubmed_means)\n",
    "pubmed_max_value = max(pubmed_means) \n",
    "# print(gpt_means)\n",
    "for ratio in pubmed_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ed318152-06ef-4e6e-8a70-aba811af55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_an_article(article_text, training_graph):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy()\n",
    "    \n",
    "    # chat_no_added_edges = []\n",
    "    # for abst in stopped_pubmed_training[begin_index:end_index]:\n",
    "\n",
    "    tokens = nltk.word_tokenize(article_text)\n",
    "\n",
    "    # compute the bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # -------  check if the giant has the bigram components, add new edge \n",
    "    # -------          otherwise, don't add new edges\n",
    "    # -------  count how many nodes    \n",
    "    count = 0\n",
    "    added_edges = []\n",
    "    for bigram in bigrams:\n",
    "\n",
    "        if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "            if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                count += 1\n",
    "                added_edges.append((bigram[0], bigram[1]))\n",
    "    ratio_ = count / len(tokens)        \n",
    "    training_graph_copy.remove_edges_from(added_edges)\n",
    "\n",
    "    return ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f5000d5a-e4bf-4940-a3ec-7a68f557d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.002\n",
      "CORRECT CLASSIFIED:  0.998\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pubmed_min_value = min(pubmed_means)\n",
    "# pubmed_max_value = max(pubmed_means) \n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in gpt_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= pubmed_min_value and ratio_val <= pubmed_max_value :       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/500)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/500)   \n",
    "print('-------------------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cc6b890e-7be7-4214-ab38-809eae6018b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.008\n",
      "CORRECT CLASSIFIED:  0.992\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# gpt_min_value = min(gpt_means)\n",
    "# gpt_max_value = max(gpt_means)\n",
    "\n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in pubmed_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= gpt_min_value and ratio_val <= gpt_max_value:        \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/500)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/500)   \n",
    "print('-------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fe3d9bfc-c343-4a14-aa97-25c33c9e2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_range(point, range_start, range_end):\n",
    "    # Calculate the distance to the nearest endpoint of the range\n",
    "    distance = min(abs(point - range_start), abs(point - range_end))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "73a739e2-c9c7-494a-b422-feee31a05d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance to range 1:  0.028414912280701754\n",
      "distance to range 2:  0.19099070175438598\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11403508771929824 , evidence: PUBMED: alzheimers d\n",
      " -------------------------------- \n",
      "distance to range 1:  0.01673571428571427\n",
      "distance to range 2:  0.17585285714285714\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12571428571428572 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06697830188679245\n",
      "distance to range 2:  0.07116283018867925\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07547169811320754 , evidence: PUBMED: normal press\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0032747422680412253\n",
      "distance to range 2:  0.19108113402061858\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13917525773195877 , evidence: PUBMED: background d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24691358024691357 evidence PUBMED: objective so\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03269390243902438\n",
      "distance to range 2:  0.1384660975609756\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10975609756097561 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2047244094488189 evidence PUBMED: sleep disord\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1794871794871795 evidence PUBMED: tauopathies \n",
      " -------------------------------- \n",
      "distance to range 1:  0.0795569182389937\n",
      "distance to range 2:  0.17808106918238994\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06289308176100629 , evidence: PUBMED: micrornas mi\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2080536912751678 evidence PUBMED: background l\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.18787878787878787 evidence PUBMED: background p\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06228122362869197\n",
      "distance to range 2:  0.2576129535864979\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08016877637130802 , evidence: PUBMED: objectives d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1640625 evidence PUBMED: background a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15891472868217055 evidence PUBMED: importance r\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.19047619047619047 evidence PUBMED: background p\n",
      " -------------------------------- \n",
      "distance to range 1:  0.055240697674418596\n",
      "distance to range 2:  0.22638441860465117\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0872093023255814 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1643835616438356 evidence PUBMED: mounting evi\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.208 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17429193899782136 evidence PUBMED: since covid \n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.19607843137254902 evidence PUBMED: background c\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.19875776397515527 evidence PUBMED: ageing proce\n",
      " -------------------------------- \n",
      "distance to range 1:  0.061253418803418794\n",
      "distance to range 2:  0.22315444444444446\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0811965811965812 , evidence: PUBMED: radiation pr\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14925373134328357 evidence PUBMED: background m\n",
      " -------------------------------- \n",
      "distance to range 1:  0.040227777777777765\n",
      "distance to range 2:  0.18093222222222222\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10222222222222223 , evidence: PUBMED: stroke lateo\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0027120087336244547\n",
      "distance to range 2:  0.20010737991266375\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13973799126637554 , evidence: PUBMED: background d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.21105527638190955 evidence PUBMED: background f\n",
      " -------------------------------- \n",
      "distance to range 1:  0.028294748858447488\n",
      "distance to range 2:  0.1691209589041096\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1141552511415525 , evidence: PUBMED: study object\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1625 evidence PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0005581081081081074\n",
      "distance to range 2:  0.15033162162162164\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.14189189189189189 , evidence: PUBMED: alzheimers d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1875 evidence PUBMED: background i\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02516604938271605\n",
      "distance to range 2:  0.1984630864197531\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11728395061728394 , evidence: PUBMED: background c\n",
      " -------------------------------- \n",
      "distance to range 1:  0.005394162436548222\n",
      "distance to range 2:  0.1924155837563452\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13705583756345177 , evidence: PUBMED: objective co\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.175 evidence PUBMED: objective me\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14285714285714285 evidence PUBMED: alzheimers d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1623931623931624 evidence PUBMED: order study \n",
      " -------------------------------- \n",
      "distance to range 1:  0.005195098039215668\n",
      "distance to range 2:  0.2394943137254902\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13725490196078433 , evidence: PUBMED: literature r\n",
      " -------------------------------- \n",
      "distance to range 1:  0.004949999999999982\n",
      "distance to range 2:  0.14121\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1375 , evidence: PUBMED: lewy body de\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0008570796460176944\n",
      "distance to range 2:  0.22561265486725665\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1415929203539823 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04244999999999999\n",
      "distance to range 2:  0.14537666666666668\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1 , evidence: PUBMED: alzheimers d\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03469137931034483\n",
      "distance to range 2:  0.2097444827586207\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10775862068965517 , evidence: PUBMED: autosomal do\n",
      " -------------------------------- \n",
      "distance to range 1:  0.10398846153846153\n",
      "distance to range 2:  0.20178692307692309\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.038461538461538464 , evidence: PUBMED: primary fami\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.22058823529411764 evidence PUBMED: background t\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14666666666666667 evidence PUBMED: background a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1949685534591195 evidence PUBMED: background f\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06718118279569892\n",
      "distance to range 2:  0.22494655913978495\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07526881720430108 , evidence: PUBMED: review analy\n",
      " -------------------------------- \n",
      "distance to range 1:  0.005697863247863227\n",
      "distance to range 2:  0.09067581196581198\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.13675213675213677 , evidence: PUBMED: alzheimers d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17525773195876287 evidence PUBMED: parkinsons d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.20175438596491227 evidence PUBMED: practice eff\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24675324675324675 evidence PUBMED: background p\n",
      " -------------------------------- \n",
      "distance to range 1:  0.027695901639344253\n",
      "distance to range 2:  0.18854606557377052\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.11475409836065574 , evidence: PUBMED: bloodbrain b\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3695652173913043 evidence GPT: impact inflamma\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3372093023255814 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.4019607843137255 evidence GPT: comorbid metabo\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35789473684210527 evidence GPT: role genetic ri\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.36082474226804123 evidence GPT: impact hyperten\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3106796116504854 evidence GPT: impact chronic \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3548387096774194 evidence GPT: effects diabete\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.28865979381443296 evidence GPT: relationship al\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34444444444444444 evidence GPT: impact depressi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32978723404255317 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35789473684210527 evidence GPT: complex relatio\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3333333333333333 evidence GPT: thyroid disorde\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3118279569892473 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30851063829787234 evidence GPT: relationship al\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.330188679245283 evidence GPT: impact anxiety \n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.38235294117647056 evidence GPT: role inflammati\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34328358208955223 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.417910447761194 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34375 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.31343283582089554 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34210526315789475 evidence GPT: association alz\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32051282051282054 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.28169014084507044 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3684210526315789 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3333333333333333 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35135135135135137 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.328125 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32432432432432434 evidence GPT: association alz\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3 evidence GPT: association alz\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3076923076923077 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30158730158730157 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3 evidence GPT: depression alzh\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29333333333333333 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3815789473684211 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3283582089552239 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3246753246753247 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30985915492957744 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2987012987012987 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "---------------COUNTS---------------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  50\n",
      "CHATGPT CLASSIFIED:  38\n",
      "FAILED_TO_CLASSIFY:  0\n",
      "GPT MISCLASSIFIED AS PUBMED:  12\n",
      "PUBMED MISCLASSIFIED AS GPT:  0\n",
      "-------------------------------------------------\n",
      "------------- %PERCENTAGE% -----------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  1.0\n",
      "CHATGPT CLASSIFIED:  0.76\n",
      "FAILED_TO_CLASSIFY:  0.0\n",
      "GPT MISCLASSIFIED AS PUBMED:  0.24\n",
      "PUBMED MISCLASSIFIED AS GPT:  0.0\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# two classes classification\n",
    "\n",
    "two_articles_dataset = []\n",
    "\n",
    "for pubmed_article in pubmed_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('PUBMED: ' + pubmed_article)\n",
    "\n",
    "for gpt_article in gpt_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('GPT: ' + gpt_article)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "chatgpt_class = 0\n",
    "pubmed_class = 0\n",
    "\n",
    "failed_to_classify = 0\n",
    "misclassified_as_gpt = 0\n",
    "misclassified_as_pubmed = 0\n",
    "\n",
    "\n",
    "# RANGE 1: PUBMED\n",
    "range1_start = pubmed_min_value\n",
    "range1_end = pubmed_max_value\n",
    "\n",
    "# RANGE 2: GPT\n",
    "range2_start = gpt_min_value\n",
    "range2_end = gpt_max_value\n",
    "\n",
    "for article in two_articles_dataset:\n",
    "    \n",
    "    gpt_ratio_val    = fit_an_article(article, gpt_lcc)\n",
    "    pubmed_ratio_val = fit_an_article(article, pubmed_lcc)\n",
    "    \n",
    "    # Classifying GPT\n",
    "    if gpt_ratio_val >= range2_start and ratio_val <= range2_end :       \n",
    "        if article[:20].startswith('GPT'):\n",
    "            chatgpt_class+=1\n",
    "            print('ChatGPT : Fit ratio for individual articles: ', gpt_ratio_val, 'evidence', article[:20])\n",
    "        else:\n",
    "            misclassified_as_pubmed+=1\n",
    "            \n",
    "    # Classifying PUBMED\n",
    "    elif pubmed_ratio_val >= range1_start and ratio_val <= range1_end:\n",
    "        if article[:20].startswith('PUBMED'):\n",
    "            pubmed_class += 1\n",
    "            print('PUBMED : Fit ratio for individual articles: ', pubmed_ratio_val, 'evidence', article[:20])\n",
    "        else: \n",
    "            misclassified_as_gpt+=1\n",
    "        \n",
    "    else:\n",
    "        # Calculate distances\n",
    "        distance_to_range1 = distance_to_range(pubmed_ratio_val, range1_start, range1_end)\n",
    "        distance_to_range2 = distance_to_range(gpt_ratio_val, range2_start, range2_end) \n",
    "        \n",
    "        print('distance to range 1: ', distance_to_range1)\n",
    "        print('distance to range 2: ', distance_to_range2)        \n",
    "        \n",
    "        # RANGE 1: PUBMED SHOULD WIN\n",
    "        if distance_to_range1 < distance_to_range2:\n",
    "            if article[:20].startswith('GPT'):\n",
    "                misclassified_as_gpt+=1\n",
    "                print('PUBMED PREDICTED INCORRECTLY => ', 'ratio:', pubmed_ratio_val ,', evidence:', article[:20])                \n",
    "            else:   \n",
    "                # count+=1\n",
    "                pubmed_class += 1\n",
    "                print('PUBMED CLASS PREDICTED => ', 'ratio:', pubmed_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "        # RANGE 2: GPT SHOULD WIN\n",
    "        elif distance_to_range2 < distance_to_range1:\n",
    "            if article[:20].startswith('PUBMED'):                \n",
    "                misclassified_as_pubmed+=1\n",
    "                print('GPT PREDICTED INCORRECTLY => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])                     \n",
    "            else:\n",
    "                chatgpt_class += 1\n",
    "                print('GPT CLASS PREDICTED => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "    print(' -------------------------------- ')\n",
    "    \n",
    "    \n",
    "print('---------------COUNTS---------------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed) \n",
    "print('-------------------------------------------------') \n",
    "    \n",
    "    \n",
    "print('------------- %PERCENTAGE% -----------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class/50)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class/50)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify/50)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt/50)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed/50) \n",
    "print('-------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0257099-6b35-4b7f-8637-3d7bf0429929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17035385-f006-4b85-b0e3-8f07deec6575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666a1e0-b8e5-4d4e-87ab-227364c94151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bbbe28-2f1e-4a32-804d-d734565b448a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d8f62-3928-4591-8df5-bcc217786188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbc80c-38e6-4590-831d-c2b8116188f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
