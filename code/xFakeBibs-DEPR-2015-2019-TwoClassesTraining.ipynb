{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ee5f81f7-c915-4754-9a11-e4042245f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements \n",
    "import json\n",
    "import obonet\n",
    "from itertools import combinations \n",
    "from Bio import Medline\n",
    "import networkx as nx\n",
    "import string\n",
    "from textblob import TextBlob  \n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import tree\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "63974b6d-8293-4445-ab3a-439a2609ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment configurations\n",
    "# MAX_NUMBER_BIGRAMS = 30\n",
    "MAX_NUMBER_ARTICLE = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8ce8bc1a-8191-4147-8c82-f02f98486f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing a medline file \n",
    "def parse_medline_rmap(medline_file):    \n",
    "    map_abstracts = {}    \n",
    "    pmid = ''\n",
    "    abstract = ''  \n",
    "    with open(medline_file) as medline_handle:\n",
    "        records = Medline.parse(medline_handle)\n",
    "        for record in records:         \n",
    "            keys = record.keys()            \n",
    "            if 'PMID' in keys and 'AB' in keys: \n",
    "\n",
    "                pmid = record['PMID']\n",
    "                abstract = record['AB']\n",
    "                \n",
    "                map_abstracts[pmid] = abstract.lower()\n",
    "    return map_abstracts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "85f0e020-a6bb-4b94-b88d-b7500d99de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_gpt_api_data(json_file):\n",
    "\n",
    "    json_records_map = {}\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Now json_data is a list of dictionaries, each representing an item in the array\n",
    "    for item in json_data:\n",
    "        gpt_id = item['GPT-ID']\n",
    "        title = item['Title']\n",
    "        abstract = item['Abstract']\n",
    "        # json_records_map[gpt_id]=(title + \" \" + abstract)\n",
    "        json_records_map[gpt_id]=(title + \" \" + abstract)        \n",
    "    return json_records_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "13080a90-1415-4f9e-ae19-a183512273b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "      \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e603b3a6-895e-4887-9d4f-611ce9a333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_abstracts = parse_medline_rmap('../dataset/pubmed-depression-set-2015-2019.txt')\n",
    "cgpt_abstracts = parse_json_gpt_api_data('../dataset/depr-gpt-apis.txt')\n",
    "\n",
    "# cleaning PubMed articles from special characters\n",
    "clean_pubmed_articles = []\n",
    "for abst in list(pubmed_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_pubmed_articles.append(cleaned)\n",
    "    \n",
    "# cleaning chatGPT articles from special characters\n",
    "clean_chatGPT_articles = []\n",
    "for abst in list(cgpt_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_chatGPT_articles.append(cleaned)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b71e1951-6355-4495-96f3-f11c6de17793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_pubmed_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7cd597f6-5b64-465a-940a-22512d16f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "special_list = ['abstract']\n",
    "\n",
    "def stopwords_rem_pubmed(clean_pubmed_training):\n",
    "    stopped_pubmed_training = []\n",
    "    for abst in clean_pubmed_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if token not in stop_words:\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_pubmed_training.append(valid_rec)\n",
    "    return stopped_pubmed_training\n",
    "    \n",
    "    \n",
    "def stopwords_rem_chatGPT_dataset(clean_chatGPT):    \n",
    "    stopped_chatGPT_training = []\n",
    "    for abst in clean_chatGPT_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if (token not in stop_words) and (token not in special_list):\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_chatGPT_training.append(valid_rec)   \n",
    "    return stopped_chatGPT_training\n",
    "\n",
    "\n",
    "def stopwords_rem_chatGPT_article(clean_chatGPT_article):    \n",
    "    stopped_chatGPT_training = []\n",
    "    valid_l = []\n",
    "    valid_rec = []\n",
    "    blob_object = TextBlob(clean_chatGPT_article)\n",
    "    list_tokens = blob_object.words\n",
    "\n",
    "    for token in list_tokens:        \n",
    "        if (token not in stop_words) and (token not in special_list):\n",
    "            valid_l.append(token)            \n",
    "    valid_rec = ' '.join(valid_l)\n",
    "    # stopped_chatGPT_training.append(valid_rec)   \n",
    "    return str(valid_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "756e0025-68d5-4820-afbb-3c220042d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506\n"
     ]
    }
   ],
   "source": [
    "pubmed_articles_ready = stopwords_rem_pubmed(clean_pubmed_articles)\n",
    "\n",
    "# print(len(stopped_pubmed_training))  \n",
    "gpt_articles_ready = []\n",
    "for article in clean_chatGPT_articles:\n",
    "    gpt_articles_ready.append(stopwords_rem_chatGPT_article(article))\n",
    "print(len(gpt_articles_ready))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8345cb7a-ac87-4af6-aafe-5fb4d2babe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pubmed_articles_ready[0])\n",
    "# print('-----')\n",
    "# print(gpt_articles_ready[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d5ffe92e-af59-4d04-b6c3-9c318836e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting PubMed bigrams\n",
    "def compute_bigrams(training_articles):\n",
    "    list_bigrams = []\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range =(2, 2))\n",
    "    X1 = vectorizer.fit_transform(training_articles)\n",
    "    features = (vectorizer.get_feature_names_out())\n",
    "    # print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "    # Applying TFIDF\n",
    "    # You can still get n-grams here\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "    X2 = vectorizer.fit_transform(training_articles)\n",
    "    scores = (X2.toarray())\n",
    "    # print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "    # Getting top ranking features\n",
    "    sums = X2.sum(axis = 0)\n",
    "    data1 = []\n",
    "    for col, term in enumerate(features):\n",
    "        data1.append( (term, sums[0, col] ))\n",
    "    ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "    words = (ranking.sort_values('rank', ascending = False))\n",
    "\n",
    "    bigram_ranks = {}\n",
    "    for index, row in words.iterrows():\n",
    "        # print(row['term'],'\\t\\t\\t',  row['rank'])\n",
    "\n",
    "        splits = row['term'].split()\n",
    "        bigram_ranks[row['rank']] = (splits[0], splits[1])\n",
    "\n",
    "    count = 0    \n",
    "    for k, v in bigram_ranks.items():\n",
    "        # if count < MAX_NUMBER_BIGRAMS:\n",
    "        #     # print(k,'\\t',  v)\n",
    "        #     count += 1\n",
    "        list_bigrams.append(v)\n",
    "    return bigram_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7ff019e0-a5ba-4c93-b9ba-62f7b9065699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_model(training_articles):\n",
    "    bigrams_map_training = compute_bigrams(training_articles)\n",
    "    gpt_training_bigrams = bigrams_map_training.values()\n",
    "    \n",
    "    graph_training_model = nx.Graph()\n",
    "    graph_training_model.add_edges_from(list(gpt_training_bigrams))\n",
    "    \n",
    "    return graph_training_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ded8c1c2-3712-48c4-803c-7f828e9b62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT Training Model --------\n",
      "Original node count:  577\n",
      "Original edge count:  1108\n",
      " -------- PubMed Training Model --------\n",
      "Original node count:  775\n",
      "Original edge count:  826\n"
     ]
    }
   ],
   "source": [
    "# construct a network training model from both datasets (gpt and pubmed)\n",
    "\n",
    "gpt_training_model = construct_training_model(gpt_articles_ready[:100])\n",
    "pubmed_training_model = construct_training_model(pubmed_articles_ready[:100])\n",
    "\n",
    "# ----------   Verifying GPT Training  Model ----------# \n",
    "print(' -------- GPT Training Model --------')\n",
    "node_count = len(gpt_training_model.nodes())\n",
    "edge_count = len(gpt_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)\n",
    "\n",
    "# ----------   Verifying PubMed Training  Model ----------# \n",
    "print(' -------- PubMed Training Model --------')\n",
    "node_count = len(pubmed_training_model.nodes())\n",
    "edge_count = len(pubmed_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5ffffcb0-d548-4376-96d7-9354472b35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giant_lcc(graph_training_model):\n",
    "    gcc = sorted(nx.connected_components(graph_training_model), key=len, reverse=True)\n",
    "    giant_cc = graph_training_model.subgraph(gcc[0])\n",
    "    return giant_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b7da8e22-e4a9-4973-922d-bd411fa4ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT GIANT LCC Graph --------\n",
      "Graph with 494 nodes and 1061 edges\n",
      " -------- PUBMED GIANT LCC Graph --------\n",
      "Graph with 513 nodes and 679 edges\n"
     ]
    }
   ],
   "source": [
    "print(' -------- GPT GIANT LCC Graph --------')\n",
    "gpt_lcc = get_giant_lcc(gpt_training_model)\n",
    "print(gpt_lcc)\n",
    "\n",
    "print(' -------- PUBMED GIANT LCC Graph --------')\n",
    "pubmed_lcc = get_giant_lcc(pubmed_training_model)\n",
    "print(pubmed_lcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0950208d-636b-4784-b0a0-c1b8f4561eae",
   "metadata": {},
   "outputs": [],
   "source": [
    " # STEP2: -- compute individual articles bigrams -------\n",
    "def calibrate_model(ds_label, begin_index, end_index, training_graph, calibrate_set):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy() \n",
    "\n",
    "    ratios_added_per_fold = []\n",
    "    for abst in calibrate_set[begin_index:end_index]:\n",
    "        \n",
    "        tokens = nltk.word_tokenize(abst)\n",
    "\n",
    "        # compute the bigrams\n",
    "        bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "        # -------  check if the giant has the bigram components, add new edge \n",
    "        # -------          otherwise, don't add new edges\n",
    "        # -------  count how many nodes            \n",
    "        count = 0\n",
    "        added_edges = []\n",
    "        for bigram in bigrams:\n",
    "\n",
    "            if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "                if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                    training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                    count += 1\n",
    "                    added_edges.append((bigram[0], bigram[1]))\n",
    "        ratio_ = count / len(tokens)        \n",
    "        \n",
    "        ratios_added_per_fold.append(ratio_) \n",
    "        \n",
    "        training_graph_copy.remove_edges_from(added_edges)      \n",
    "    return ratios_added_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bd014f6c-9a92-4e85-a279-78a8258b941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(tst_set_list):\n",
    "    average = sum(tst_set_list) / len(tst_set_list)        \n",
    "    formatted_avg = float(\"{:.5f}\".format(average))        \n",
    "    return formatted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "df86f04a-4d5e-4535-b810-1cadc3bb0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of the list is: 0.28825\n",
      "The average of the list is: 0.30136\n",
      "The average of the list is: 0.3247\n",
      "The average of the list is: 0.27978\n",
      "The average of the list is: 0.30504\n",
      "The average of the list is: 0.30281\n",
      "The average of the list is: 0.29469\n",
      "The average of the list is: 0.28088\n",
      "The average of the list is: 0.27455\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "gpt_means = []\n",
    "for index in range(100,1000):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, gpt_lcc, gpt_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_g = calc_mean(calb_ratios_list) \n",
    "        print(\"The average of the list is:\", tst_mean_g)\n",
    "        gpt_means.append(tst_mean_g)\n",
    "        \n",
    "gpt_min_value = min(gpt_means)\n",
    "gpt_max_value = max(gpt_means) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d67982e6-5346-48f0-8317-96fb6c705464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1353\n",
      "0.11871\n",
      "0.11528\n",
      "0.11393\n",
      "0.11208\n",
      "0.09421\n",
      "0.11869\n",
      "0.09695\n",
      "0.10749\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pubmed_means = []\n",
    "for index in range(100,1000):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, pubmed_lcc, pubmed_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_p = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_p)\n",
    "        pubmed_means.append(tst_mean_p)\n",
    "        \n",
    "pubmed_min_value = min(pubmed_means)\n",
    "pubmed_max_value = max(pubmed_means) \n",
    "# print(gpt_means)\n",
    "for ratio in pubmed_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ed318152-06ef-4e6e-8a70-aba811af55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_an_article(article_text, training_graph):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy()\n",
    "    \n",
    "    # chat_no_added_edges = []\n",
    "    # for abst in stopped_pubmed_training[begin_index:end_index]:\n",
    "\n",
    "    tokens = nltk.word_tokenize(article_text)\n",
    "\n",
    "    # compute the bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # -------  check if the giant has the bigram components, add new edge \n",
    "    # -------          otherwise, don't add new edges\n",
    "    # -------  count how many nodes    \n",
    "\n",
    "    count = 0\n",
    "    added_edges = []\n",
    "    for bigram in bigrams:\n",
    "\n",
    "        if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "            if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                count += 1\n",
    "                added_edges.append((bigram[0], bigram[1]))\n",
    "    ratio_ = count / len(tokens)        \n",
    "    training_graph_copy.remove_edges_from(added_edges)\n",
    "        \n",
    "    return ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f5000d5a-e4bf-4940-a3ec-7a68f557d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.012\n",
      "CORRECT CLASSIFIED:  0.988\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pubmed_min_value = min(pubmed_means)\n",
    "# pubmed_max_value = max(pubmed_means)\n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in gpt_articles_ready[1000:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= pubmed_min_value and ratio_val <= pubmed_max_value:       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/500)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/500)   \n",
    "print('-------------------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "cc6b890e-7be7-4214-ab38-809eae6018b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.004\n",
      "CORRECT CLASSIFIED:  0.996\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# gpt_min_value = min(gpt_means)\n",
    "# gpt_max_value = max(gpt_means) \n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in pubmed_articles_ready[1000:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= gpt_min_value and ratio_val <= gpt_max_value:       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/500)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/500)   \n",
    "print('-------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "78695b66-9629-4a82-a9d8-3a9504da902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_range(point, range_start, range_end):\n",
    "    # Calculate the distance to the nearest endpoint of the range\n",
    "    distance = min(abs(point - range_start), abs(point - range_end))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c0fe7649-02f1-42ef-9f50-cce134b3a13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUBMED : Fit ratio for individual articles:  0.11242603550295859 evidence PUBMED: depression c\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14285714285714285 evidence PUBMED: rationale ob\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.09859154929577464 evidence PUBMED: background m\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.10309278350515463 evidence PUBMED: commentary h\n",
      " -------------------------------- \n",
      "distance to range 1:  0.015778627450980395\n",
      "distance to range 2:  0.2222624183006536\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0784313725490196 , evidence: PUBMED: background d\n",
      " -------------------------------- \n",
      "distance to range 1:  0.041018510638297874\n",
      "distance to range 2:  0.16816702127659577\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.05319148936170213 , evidence: PUBMED: across lifes\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.3192771084337349 evidence PUBMED: background d\n",
      " -------------------------------- \n",
      "distance to range 1:  0.037871971830985916\n",
      "distance to range 2:  0.21821197183098592\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.056338028169014086 , evidence: PUBMED: parkinsons d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13414634146341464 evidence PUBMED: background e\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07315736842105264\n",
      "distance to range 2:  0.24297105263157898\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.021052631578947368 , evidence: PUBMED: six decades \n",
      " -------------------------------- \n",
      "distance to range 1:  0.022781428571428577\n",
      "distance to range 2:  0.21740714285714288\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07142857142857142 , evidence: PUBMED: neuropeptide\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03171\n",
      "distance to range 2:  0.22455000000000003\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.0625 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.11888111888111888 evidence PUBMED: background i\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0006848201438848928\n",
      "distance to range 2:  0.195413309352518\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09352517985611511 , evidence: PUBMED: stress depre\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02031837438423645\n",
      "distance to range 2:  0.24991945812807884\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07389162561576355 , evidence: PUBMED: background l\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.11320754716981132 evidence PUBMED: therapist pa\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0016174074074074146\n",
      "distance to range 2:  0.23751296296296298\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09259259259259259 , evidence: PUBMED: background r\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03091886075949367\n",
      "distance to range 2:  0.24290443037974685\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06329113924050633 , evidence: PUBMED: correlations\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0004600000000000021\n",
      "distance to range 2:  0.10788333333333336\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09375 , evidence: PUBMED: breast cance\n",
      " -------------------------------- \n",
      "distance to range 1:  0.06604098591549296\n",
      "distance to range 2:  0.24638098591549298\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.028169014084507043 , evidence: PUBMED: background s\n",
      " -------------------------------- \n",
      "distance to range 1:  0.034210000000000004\n",
      "distance to range 2:  0.1812166666666667\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06 , evidence: PUBMED: holistic car\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03286030674846626\n",
      "distance to range 2:  0.2622800613496933\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06134969325153374 , evidence: PUBMED: methylliberi\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.18064516129032257 evidence PUBMED: objective ex\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.12658227848101267 evidence PUBMED: background e\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14583333333333334 evidence PUBMED: study aimed \n",
      " -------------------------------- \n",
      "distance to range 1:  0.0033009090909090905\n",
      "distance to range 2:  0.2399179653679654\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09090909090909091 , evidence: PUBMED: microvascula\n",
      " -------------------------------- \n",
      "distance to range 1:  0.010339032258064512\n",
      "distance to range 2:  0.22293709677419357\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08387096774193549 , evidence: PUBMED: objective tw\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13114754098360656 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16981132075471697 evidence PUBMED: studies sugg\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.17197452229299362 evidence PUBMED: objective pr\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1111111111111111 evidence PUBMED: background o\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07485516129032257\n",
      "distance to range 2:  0.24874354838709678\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.01935483870967742 , evidence: PUBMED: heart failur\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02184157894736842\n",
      "distance to range 2:  0.2482342105263158\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07236842105263158 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07602818181818183\n",
      "distance to range 2:  0.24727727272727273\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.01818181818181818 , evidence: PUBMED: skull fractu\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03206310734463277\n",
      "distance to range 2:  0.24630141242937856\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.062146892655367235 , evidence: PUBMED: ingestion to\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0766661403508772\n",
      "distance to range 2:  0.26577807017543864\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.017543859649122806 , evidence: PUBMED: nationwide o\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1323529411764706 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.12781954887218044 evidence PUBMED: objective st\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.11976047904191617 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.035730467836257315\n",
      "distance to range 2:  0.25115818713450294\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.05847953216374269 , evidence: PUBMED: background r\n",
      " -------------------------------- \n",
      "distance to range 1:  0.019022030075187976\n",
      "distance to range 2:  0.199362030075188\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07518796992481203 , evidence: PUBMED: background d\n",
      " -------------------------------- \n",
      "distance to range 1:  0.05421\n",
      "distance to range 2:  0.23455\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.04 , evidence: PUBMED: depressed fo\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.09433962264150944 evidence PUBMED: background d\n",
      " -------------------------------- \n",
      "distance to range 1:  0.040955562130177516\n",
      "distance to range 2:  0.25088136094674557\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.05325443786982249 , evidence: PUBMED: background g\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15 evidence PUBMED: purpose stud\n",
      " -------------------------------- \n",
      "distance to range 1:  0.010876666666666673\n",
      "distance to range 2:  0.20047592592592595\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08333333333333333 , evidence: PUBMED: purpose stud\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.13333333333333333 evidence PUBMED: background a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.11864406779661017 evidence PUBMED: although eff\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.11188811188811189 evidence PUBMED: insomnia dep\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1504424778761062 evidence PUBMED: attentiondef\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2777777777777778 evidence GPT: gutbrain axis e\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3375 evidence GPT: depression card\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.37662337662337664 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3684210526315789 evidence GPT: depression anxi\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32894736842105265 evidence GPT: impact depressi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.27848101265822783 evidence GPT: depression preg\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.36904761904761907 evidence GPT: relationship ch\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.4444444444444444 evidence GPT: depression olde\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.4264705882352941 evidence GPT: depression obes\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2948717948717949 evidence GPT: depression neur\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.41025641025641024 evidence GPT: women depressio\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3870967741935484 evidence GPT: depression canc\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29347826086956524 evidence GPT: relationship de\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.375 evidence GPT: nonpharmacologi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.38095238095238093 evidence GPT: depression auto\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.36470588235294116 evidence GPT: depression rheu\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.313953488372093 evidence GPT: relationship de\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3026315789473684 evidence GPT: depression alzh\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35714285714285715 evidence GPT: depression canc\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.5 evidence GPT: role inflammato\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3023255813953488 evidence GPT: depression diab\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3157894736842105 evidence GPT: depression schi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3103448275862069 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3258426966292135 evidence GPT: depression rheu\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3372093023255814 evidence GPT: depression hiva\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2891566265060241 evidence GPT: depression mult\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2926829268292683 evidence GPT: depression chro\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3625 evidence GPT: depression park\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3488372093023256 evidence GPT: depression card\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2987012987012987 evidence GPT: depression eati\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.33783783783783783 evidence GPT: depression coro\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.40540540540540543 evidence GPT: depression asso\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3246753246753247 evidence GPT: depression coex\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2948717948717949 evidence GPT: depression rela\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32894736842105265 evidence GPT: depression asso\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2875 evidence GPT: depression como\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3466666666666667 evidence GPT: depression como\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.27631578947368424 evidence GPT: depression asso\n",
      " -------------------------------- \n",
      "---------------COUNTS---------------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  50\n",
      "CHATGPT CLASSIFIED:  38\n",
      "FAILED_TO_CLASSIFY:  0\n",
      "GPT MISCLASSIFIED AS PUBMED:  12\n",
      "PUBMED MISCLASSIFIED AS GPT:  0\n",
      "-------------------------------------------------\n",
      "------------- %PERCENTAGE% -----------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  1.0\n",
      "CHATGPT CLASSIFIED:  0.76\n",
      "FAILED_TO_CLASSIFY:  0.0\n",
      "GPT MISCLASSIFIED AS PUBMED:  0.24\n",
      "PUBMED MISCLASSIFIED AS GPT:  0.0\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# two classes classification\n",
    "\n",
    "two_articles_dataset = []\n",
    "\n",
    "for pubmed_article in pubmed_articles_ready[1000:1050]:\n",
    "    two_articles_dataset.append('PUBMED: ' + pubmed_article)\n",
    "\n",
    "for gpt_article in gpt_articles_ready[1000:1050]:\n",
    "    two_articles_dataset.append('GPT: ' + gpt_article)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "chatgpt_class = 0\n",
    "pubmed_class = 0\n",
    "\n",
    "failed_to_classify = 0\n",
    "misclassified_as_gpt = 0\n",
    "misclassified_as_pubmed = 0\n",
    "\n",
    "# 0.09421 and ratio_val <= 0.1353\n",
    "# 0.27455 and ratio_val <= 0.3247 : \n",
    "\n",
    "# RANGE 1: PUBMED\n",
    "range1_start = pubmed_min_value\n",
    "range1_end = pubmed_max_value\n",
    "\n",
    "# RANGE 2: GPT\n",
    "range2_start = gpt_min_value\n",
    "range2_end = gpt_max_value\n",
    "\n",
    "for article in two_articles_dataset:\n",
    "    \n",
    "    gpt_ratio_val    = fit_an_article(article, gpt_lcc)\n",
    "    pubmed_ratio_val = fit_an_article(article, pubmed_lcc)\n",
    "    \n",
    "    # Classifying GPT\n",
    "    if gpt_ratio_val >= range2_start and ratio_val <= range2_end :       \n",
    "        if article[:20].startswith('GPT'):\n",
    "            chatgpt_class+=1\n",
    "            print('ChatGPT : Fit ratio for individual articles: ', gpt_ratio_val, 'evidence', article[:20])\n",
    "        else:\n",
    "            misclassified_as_pubmed+=1\n",
    "            \n",
    "    # Classifying PUBMED\n",
    "    elif pubmed_ratio_val >= range1_start and ratio_val <= range1_end:\n",
    "        if article[:20].startswith('PUBMED'):\n",
    "            pubmed_class += 1\n",
    "            print('PUBMED : Fit ratio for individual articles: ', pubmed_ratio_val, 'evidence', article[:20])\n",
    "        else: \n",
    "            misclassified_as_gpt+=1\n",
    "        \n",
    "    else:\n",
    "        # Calculate distances\n",
    "        distance_to_range1 = distance_to_range(pubmed_ratio_val, range1_start, range1_end)\n",
    "        distance_to_range2 = distance_to_range(gpt_ratio_val, range2_start, range2_end) \n",
    "        \n",
    "        print('distance to range 1: ', distance_to_range1)\n",
    "        print('distance to range 2: ', distance_to_range2)        \n",
    "        \n",
    "        # RANGE 1: PUBMED SHOULD WIN\n",
    "        if distance_to_range1 < distance_to_range2:\n",
    "            if article[:20].startswith('GPT'):\n",
    "                misclassified_as_gpt+=1\n",
    "                print('PUBMED PREDICTED INCORRECTLY => ', 'ratio:', pubmed_ratio_val ,', evidence:', article[:20])                \n",
    "            else:   \n",
    "                # count+=1\n",
    "                pubmed_class += 1\n",
    "                print('PUBMED CLASS PREDICTED => ', 'ratio:', pubmed_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "        # RANGE 2: GPT SHOULD WIN\n",
    "        elif distance_to_range2 < distance_to_range1:\n",
    "            if article[:20].startswith('PUBMED'):                \n",
    "                misclassified_as_pubmed+=1\n",
    "                print('GPT PREDICTED INCORRECTLY => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])                     \n",
    "            else:\n",
    "                chatgpt_class += 1\n",
    "                print('GPT CLASS PREDICTED => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "    print(' -------------------------------- ')\n",
    "    \n",
    "    \n",
    "print('---------------COUNTS---------------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed) \n",
    "print('-------------------------------------------------') \n",
    "    \n",
    "    \n",
    "print('------------- %PERCENTAGE% -----------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class/50)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class/50)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify/50)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt/50)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed/50) \n",
    "print('-------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ae0fb-a4ab-4c9f-87b8-c92c18b934c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad22ad-db8d-4528-b5cf-f1ae21880bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a538f-e45f-405b-9b08-25287eff5344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a47701-aedc-441e-b0ec-ea419fb2e49d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
