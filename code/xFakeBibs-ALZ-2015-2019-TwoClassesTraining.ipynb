{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ee5f81f7-c915-4754-9a11-e4042245f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements \n",
    "import json\n",
    "import obonet\n",
    "from itertools import combinations \n",
    "from Bio import Medline\n",
    "import networkx as nx\n",
    "import string\n",
    "from textblob import TextBlob  \n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import tree\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "63974b6d-8293-4445-ab3a-439a2609ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment configurations\n",
    "# MAX_NUMBER_BIGRAMS = 30\n",
    "MAX_NUMBER_ARTICLE = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8ce8bc1a-8191-4147-8c82-f02f98486f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing a medline file \n",
    "def parse_medline_rmap(medline_file):    \n",
    "    map_abstracts = {}    \n",
    "    pmid = ''\n",
    "    abstract = ''  \n",
    "    with open(medline_file) as medline_handle:\n",
    "        records = Medline.parse(medline_handle)\n",
    "        for record in records:         \n",
    "            keys = record.keys()            \n",
    "            if 'PMID' in keys and 'AB' in keys: \n",
    "\n",
    "                pmid = record['PMID']\n",
    "                abstract = record['AB']\n",
    "                \n",
    "                map_abstracts[pmid] = abstract.lower()\n",
    "    return map_abstracts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "85f0e020-a6bb-4b94-b88d-b7500d99de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_gpt_api_data(json_file):\n",
    "\n",
    "    json_records_map = {}\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Now json_data is a list of dictionaries, each representing an item in the array\n",
    "    for item in json_data:\n",
    "        gpt_id = item['GPT-ID']\n",
    "        title = item['Title']\n",
    "        abstract = item['Abstract']\n",
    "        # json_records_map[gpt_id]=(title + \" \" + abstract)\n",
    "        json_records_map[gpt_id]=(title + \" \" + abstract)        \n",
    "    return json_records_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "13080a90-1415-4f9e-ae19-a183512273b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "      \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e603b3a6-895e-4887-9d4f-611ce9a333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_abstracts = parse_medline_rmap('../dataset/pubmed-alzheimers-set-2015-2019.txt')\n",
    "cgpt_abstracts = parse_json_gpt_api_data('../dataset/alz-gpt-apis.txt')\n",
    "\n",
    "# cleaning PubMed articles from special characters\n",
    "clean_pubmed_articles = []\n",
    "for abst in list(pubmed_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_pubmed_articles.append(cleaned)\n",
    "    \n",
    "# cleaning chatGPT articles from special characters\n",
    "clean_chatGPT_articles = []\n",
    "for abst in list(cgpt_abstracts.values())[0:]:\n",
    "    cleaned = remove_string_special_characters(abst)    \n",
    "    clean_chatGPT_articles.append(cleaned)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b71e1951-6355-4495-96f3-f11c6de17793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction it has been suggested that the development of poststroke apathy psa and depression psd may be more strongly associated with generalised brain pathology rather than the stroke lesion itself the present study aimed to investigate associations between imaging markers of lesionrelated and generalised brain pathology and the development of psa and psd during a oneyear followup patients and methods in a prospective cohort study stroke patients received tesla mri at baseline three months poststroke for evaluation of lesionrelated vascular and degenerative brain pathology presence of lacunes microbleeds white matter hyperintensities and enlarged perivascular spaces was summed to provide a measure of total cerebral small vessel disease csvd burden range the mini international neuropsychiatric interview and apathy evaluation scale were administered at baseline and repeated at and month followup to define presence of psd and psa respectively results populationaveraged logistic regression models showed that global brain atrophy and severe csvd burden score were significantly associated with the odds of having psa orgee ci and ci respectively independent of stroke lesion volume and comorbid psd medium csvd burden score was significantly associated with the odds of having psd orgee ci independent of stroke lesion volume comorbid psa and prestroke depression no associations were found with lesionrelated markers conclusions the results suggest that generalised degenerative and vascular brain pathology rather than lesionrelated pathology is an important predictor for the development of psa and less strongly for psd\n"
     ]
    }
   ],
   "source": [
    "print(clean_pubmed_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7cd597f6-5b64-465a-940a-22512d16f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "special_list = ['abstract']\n",
    "\n",
    "def stopwords_rem_pubmed(clean_pubmed_training):\n",
    "    stopped_pubmed_training = []\n",
    "    for abst in clean_pubmed_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if token not in stop_words:\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_pubmed_training.append(valid_rec)\n",
    "    return stopped_pubmed_training\n",
    "    \n",
    "    \n",
    "def stopwords_rem_chatGPT_dataset(clean_chatGPT):    \n",
    "    stopped_chatGPT_training = []\n",
    "    for abst in clean_chatGPT_training[:MAX_NUMBER_ARTICLE]:\n",
    "        valid_l = []\n",
    "        valid_rec = []\n",
    "        blob_object = TextBlob(abst)\n",
    "        list_tokens = blob_object.words\n",
    "\n",
    "        for token in list_tokens:        \n",
    "            if (token not in stop_words) and (token not in special_list):\n",
    "                valid_l.append(token)            \n",
    "        valid_rec = ' '.join(valid_l)\n",
    "        stopped_chatGPT_training.append(valid_rec)   \n",
    "    return stopped_chatGPT_training\n",
    "\n",
    "\n",
    "def stopwords_rem_chatGPT_article(clean_chatGPT_article):    \n",
    "    stopped_chatGPT_training = []\n",
    "    valid_l = []\n",
    "    valid_rec = []\n",
    "    blob_object = TextBlob(clean_chatGPT_article)\n",
    "    list_tokens = blob_object.words\n",
    "\n",
    "    for token in list_tokens:        \n",
    "        if (token not in stop_words) and (token not in special_list):\n",
    "            valid_l.append(token)            \n",
    "    valid_rec = ' '.join(valid_l)\n",
    "    # stopped_chatGPT_training.append(valid_rec)   \n",
    "    return str(valid_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "756e0025-68d5-4820-afbb-3c220042d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179\n"
     ]
    }
   ],
   "source": [
    "pubmed_articles_ready = stopwords_rem_pubmed(clean_pubmed_articles)\n",
    "\n",
    "# print(len(stopped_pubmed_training))  \n",
    "gpt_articles_ready = []\n",
    "for article in clean_chatGPT_articles:\n",
    "    gpt_articles_ready.append(stopwords_rem_chatGPT_article(article))\n",
    "print(len(gpt_articles_ready))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8345cb7a-ac87-4af6-aafe-5fb4d2babe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pubmed_articles_ready[0])\n",
    "# print('-----')\n",
    "# print(gpt_articles_ready[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d5ffe92e-af59-4d04-b6c3-9c318836e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting PubMed bigrams\n",
    "def compute_bigrams(training_articles):\n",
    "    list_bigrams = []\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range =(2, 2))\n",
    "    X1 = vectorizer.fit_transform(training_articles)\n",
    "    features = (vectorizer.get_feature_names_out())\n",
    "    # print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "    # Applying TFIDF\n",
    "    # You can still get n-grams here\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "    X2 = vectorizer.fit_transform(training_articles)\n",
    "    scores = (X2.toarray())\n",
    "    # print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "    # Getting top ranking features\n",
    "    sums = X2.sum(axis = 0)\n",
    "    data1 = []\n",
    "    for col, term in enumerate(features):\n",
    "        data1.append( (term, sums[0, col] ))\n",
    "    ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "    words = (ranking.sort_values('rank', ascending = False))\n",
    "\n",
    "    bigram_ranks = {}\n",
    "    for index, row in words.iterrows():\n",
    "        # print(row['term'],'\\t\\t\\t',  row['rank'])\n",
    "\n",
    "        splits = row['term'].split()\n",
    "        bigram_ranks[row['rank']] = (splits[0], splits[1])\n",
    "\n",
    "    count = 0    \n",
    "    for k, v in bigram_ranks.items():\n",
    "        # if count < MAX_NUMBER_BIGRAMS:\n",
    "        #     # print(k,'\\t',  v)\n",
    "        #     count += 1\n",
    "        list_bigrams.append(v)\n",
    "    return bigram_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7ff019e0-a5ba-4c93-b9ba-62f7b9065699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_model(training_articles):\n",
    "    bigrams_map_training = compute_bigrams(training_articles)\n",
    "    gpt_training_bigrams = bigrams_map_training.values()\n",
    "    \n",
    "    graph_training_model = nx.Graph()\n",
    "    graph_training_model.add_edges_from(list(gpt_training_bigrams))\n",
    "    \n",
    "    return graph_training_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ded8c1c2-3712-48c4-803c-7f828e9b62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT Training Model --------\n",
      "Original node count:  519\n",
      "Original edge count:  1194\n",
      " -------- PubMed Training Model --------\n",
      "Original node count:  774\n",
      "Original edge count:  940\n"
     ]
    }
   ],
   "source": [
    "# construct a network training model from both datasets (gpt and pubmed)\n",
    "\n",
    "gpt_training_model = construct_training_model(gpt_articles_ready[:100])\n",
    "pubmed_training_model = construct_training_model(pubmed_articles_ready[:100])\n",
    "\n",
    "# ----------   Verifying GPT Training  Model ----------# \n",
    "print(' -------- GPT Training Model --------')\n",
    "node_count = len(gpt_training_model.nodes())\n",
    "edge_count = len(gpt_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)\n",
    "\n",
    "# ----------   Verifying PubMed Training  Model ----------# \n",
    "print(' -------- PubMed Training Model --------')\n",
    "node_count = len(pubmed_training_model.nodes())\n",
    "edge_count = len(pubmed_training_model.edges())\n",
    "print('Original node count: ', node_count)\n",
    "print('Original edge count: ', edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5ffffcb0-d548-4376-96d7-9354472b35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giant_lcc(graph_training_model):\n",
    "    gcc = sorted(nx.connected_components(graph_training_model), key=len, reverse=True)\n",
    "    giant_cc = graph_training_model.subgraph(gcc[0])\n",
    "    return giant_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b7da8e22-e4a9-4973-922d-bd411fa4ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- GPT GIANT LCC Graph --------\n",
      "Graph with 479 nodes and 1170 edges\n",
      " -------- PUBMED GIANT LCC Graph --------\n",
      "Graph with 558 nodes and 814 edges\n"
     ]
    }
   ],
   "source": [
    "print(' -------- GPT GIANT LCC Graph --------')\n",
    "gpt_lcc = get_giant_lcc(gpt_training_model)\n",
    "print(gpt_lcc)\n",
    "\n",
    "print(' -------- PUBMED GIANT LCC Graph --------')\n",
    "pubmed_lcc = get_giant_lcc(pubmed_training_model)\n",
    "print(pubmed_lcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0950208d-636b-4784-b0a0-c1b8f4561eae",
   "metadata": {},
   "outputs": [],
   "source": [
    " # STEP2: -- compute individual articles bigrams -------\n",
    "def calibrate_model(ds_label, begin_index, end_index, training_graph, calibrate_set):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy() \n",
    "\n",
    "    ratios_added_per_fold = []\n",
    "    for abst in calibrate_set[begin_index:end_index]:\n",
    "        \n",
    "        tokens = nltk.word_tokenize(abst)\n",
    "\n",
    "        # compute the bigrams\n",
    "        bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "        # -------  check if the giant has the bigram components, add new edge \n",
    "        # -------          otherwise, don't add new edges\n",
    "        # -------  count how many nodes            \n",
    "        count = 0\n",
    "        added_edges = []\n",
    "        for bigram in bigrams:\n",
    "\n",
    "            if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "                if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                    training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                    count += 1\n",
    "                    added_edges.append((bigram[0], bigram[1]))\n",
    "        ratio_ = count / len(tokens)        \n",
    "        \n",
    "        ratios_added_per_fold.append(ratio_) \n",
    "        \n",
    "        training_graph_copy.remove_edges_from(added_edges)      \n",
    "    return ratios_added_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bd014f6c-9a92-4e85-a279-78a8258b941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(tst_set_list):\n",
    "    average = sum(tst_set_list) / len(tst_set_list)        \n",
    "    formatted_avg = float(\"{:.5f}\".format(average))        \n",
    "    return formatted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "df86f04a-4d5e-4535-b810-1cadc3bb0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27871\n",
      "0.30034\n",
      "0.30916\n",
      "0.28195\n",
      "0.28424\n",
      "0.29355\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "gpt_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, gpt_lcc, gpt_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_g = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_g)\n",
    "        gpt_means.append(tst_mean_g)\n",
    "        \n",
    "gpt_min_value = min(gpt_means)\n",
    "gpt_max_value = max(gpt_means) \n",
    "# print(gpt_means)\n",
    "for ratio in gpt_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d67982e6-5346-48f0-8317-96fb6c705464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14516\n",
      "0.15653\n",
      "0.15326\n",
      "0.15819\n",
      "0.14422\n",
      "0.15104\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pubmed_means = []\n",
    "for index in range(100,MAX_NUMBER_ARTICLE):\n",
    "    label_prefix = 'TEST-'\n",
    "    if index % 100 == 0:\n",
    "        count += 1\n",
    "        calb_ratios_list = calibrate_model(label_prefix + str(count), index, index+100, pubmed_lcc, pubmed_articles_ready)\n",
    "        # print(calb_ratios_list)\n",
    "        tst_mean_p = calc_mean(calb_ratios_list) \n",
    "        # print(\"The average of the list is:\", tst_mean_p)\n",
    "        pubmed_means.append(tst_mean_p)\n",
    "        \n",
    "pubmed_min_value = min(pubmed_means)\n",
    "pubmed_max_value = max(pubmed_means) \n",
    "# print(gpt_means)\n",
    "for ratio in pubmed_means:\n",
    "    print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ed318152-06ef-4e6e-8a70-aba811af55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_an_article(article_text, training_graph):\n",
    "    \n",
    "    training_graph_copy = training_graph.copy()\n",
    "    \n",
    "    # chat_no_added_edges = []\n",
    "    # for abst in stopped_pubmed_training[begin_index:end_index]:\n",
    "\n",
    "    tokens = nltk.word_tokenize(article_text)\n",
    "\n",
    "    # compute the bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # -------  check if the giant has the bigram components, add new edge \n",
    "    # -------          otherwise, don't add new edges\n",
    "    # -------  count how many nodes    \n",
    "\n",
    "    count = 0\n",
    "    added_edges = []\n",
    "    for bigram in bigrams:\n",
    "\n",
    "        if training_graph_copy.has_node(bigram[0]) and training_graph_copy.has_node(bigram[1]):\n",
    "\n",
    "            if not training_graph_copy.has_edge(bigram[0], bigram[1]):\n",
    "\n",
    "                training_graph_copy.add_edge(bigram[0], bigram[1])\n",
    "                count += 1\n",
    "                added_edges.append((bigram[0], bigram[1]))\n",
    "    ratio_ = count / len(tokens)        \n",
    "    training_graph_copy.remove_edges_from(added_edges)\n",
    "        \n",
    "    return ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f5000d5a-e4bf-4940-a3ec-7a68f557d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.002\n",
      "CORRECT CLASSIFIED:  0.498\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#  Train against PubMed classification \n",
    "# pubmed_min_value = min(pubmed_means)\n",
    "# pubmed_max_value = max(pubmed_means) \n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in gpt_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= pubmed_min_value and ratio_val <= pubmed_max_value:       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/1000)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/1000)   \n",
    "print('-------------------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc6b890e-7be7-4214-ab38-809eae6018b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "MISCLASSIFIED:  0.0125\n",
      "CORRECT CLASSIFIED:  1.2375\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# gpt_min_value = min(gpt_means)\n",
    "# gpt_max_value = max(gpt_means) \n",
    "\n",
    "misclassified = 0\n",
    "correct_classified = 0\n",
    "for article in pubmed_articles_ready[200:MAX_NUMBER_ARTICLE]:\n",
    "    # print(type(article))\n",
    "    ratio_val = fit_an_article(article, gpt_lcc)\n",
    "    if ratio_val >= gpt_min_value and ratio_val <= gpt_max_value:       \n",
    "        misclassified+=1\n",
    "        # print('MISCLASSIFIED: Fit ratio for individual articles: ', ratio_val)\n",
    "    else:\n",
    "        correct_classified+=1\n",
    "        # print('CORRECT CLASS: Fit ratio for individual articles: ', ratio_val)\n",
    "print('-------------------------------------------------')        \n",
    "print('MISCLASSIFIED: ', misclassified/400)\n",
    "print('CORRECT CLASSIFIED: ', correct_classified/400)   \n",
    "print('-------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c0fe7649-02f1-42ef-9f50-cce134b3a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_range(point, range_start, range_end):\n",
    "    # Calculate the distance to the nearest endpoint of the range\n",
    "    distance = min(abs(point - range_start), abs(point - range_end))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8e0395fb-8741-4876-b4ce-84376dce27e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance to range 1:  0.10957796766743648\n",
      "distance to range 2:  0.2348300923787529\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.03464203233256351 , evidence: PUBMED: sanctity pur\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1724137931034483 evidence PUBMED: relationship\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.25308641975308643 evidence PUBMED: background p\n",
      " -------------------------------- \n",
      "distance to range 1:  0.021771020408163255\n",
      "distance to range 2:  0.08143108843537417\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12244897959183673 , evidence: PUBMED: vascular mil\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.25517241379310346 evidence PUBMED: alzheimers d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24427480916030533 evidence PUBMED: background p\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04847531914893616\n",
      "distance to range 2:  0.16168872340425533\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09574468085106383 , evidence: PUBMED: background n\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1807909604519774 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.20161290322580644 evidence PUBMED: objectives a\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1794871794871795 evidence PUBMED: cerebrospina\n",
      " -------------------------------- \n",
      "distance to range 1:  0.048981904761904754\n",
      "distance to range 2:  0.21748551020408163\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09523809523809523 , evidence: PUBMED: background p\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14754098360655737 evidence PUBMED: objective es\n",
      " -------------------------------- \n",
      "distance to range 1:  0.05578462585034012\n",
      "distance to range 2:  0.0950365306122449\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08843537414965986 , evidence: PUBMED: hivassociate\n",
      " -------------------------------- \n",
      "distance to range 1:  0.03552434782608695\n",
      "distance to range 2:  0.12653608695652174\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.10869565217391304 , evidence: PUBMED: background r\n",
      " -------------------------------- \n",
      "distance to range 1:  0.02365262411347517\n",
      "distance to range 2:  0.19714971631205674\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12056737588652482 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.10031756097560975\n",
      "distance to range 2:  0.22017341463414636\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.04390243902439024 , evidence: PUBMED: prenatal ear\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.24705882352941178 evidence PUBMED: background m\n",
      " -------------------------------- \n",
      "distance to range 1:  0.047205074626865656\n",
      "distance to range 2:  0.18169507462686568\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09701492537313433 , evidence: PUBMED: work critica\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16279069767441862 evidence PUBMED: background p\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.22151898734177214 evidence PUBMED: background m\n",
      " -------------------------------- \n",
      "distance to range 1:  0.014723597122302146\n",
      "distance to range 2:  0.1851848201438849\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12949640287769784 , evidence: PUBMED: evidence acc\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1595744680851064 evidence PUBMED: objectives e\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.3028169014084507 evidence PUBMED: purpose aim \n",
      " -------------------------------- \n",
      "distance to range 1:  0.014866766169154222\n",
      "distance to range 2:  0.2389090049751244\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12935323383084577 , evidence: PUBMED: pathological\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14864864864864866 evidence PUBMED: objectives i\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.20422535211267606 evidence PUBMED: background s\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16666666666666666 evidence PUBMED: insomnia com\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16756756756756758 evidence PUBMED: objective as\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.152317880794702 evidence PUBMED: objectives m\n",
      " -------------------------------- \n",
      "distance to range 1:  0.009604615384615378\n",
      "distance to range 2:  0.16973564102564104\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1346153846153846 , evidence: PUBMED: background a\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07525448275862068\n",
      "distance to range 2:  0.10629620689655173\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06896551724137931 , evidence: PUBMED: yearold man \n",
      " -------------------------------- \n",
      "distance to range 1:  0.06887753424657533\n",
      "distance to range 2:  0.1554223287671233\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07534246575342465 , evidence: PUBMED: inheritance \n",
      " -------------------------------- \n",
      "distance to range 1:  0.05385855421686746\n",
      "distance to range 2:  0.20039674698795182\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09036144578313253 , evidence: PUBMED: obesity pred\n",
      " -------------------------------- \n",
      "distance to range 1:  0.08910188976377952\n",
      "distance to range 2:  0.18422181102362206\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.05511811023622047 , evidence: PUBMED: growing numb\n",
      " -------------------------------- \n",
      "distance to range 1:  0.048981904761904754\n",
      "distance to range 2:  0.2668052380952381\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.09523809523809523 , evidence: PUBMED: objective de\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2642857142857143 evidence PUBMED: context risk\n",
      " -------------------------------- \n",
      "distance to range 1:  0.10933627906976742\n",
      "distance to range 2:  0.20894255813953488\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.03488372093023256 , evidence: PUBMED: last decade \n",
      " -------------------------------- \n",
      "distance to range 1:  0.021578490566037722\n",
      "distance to range 2:  0.22210622641509437\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.12264150943396226 , evidence: PUBMED: introduction\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.14634146341463414 evidence PUBMED: introduction\n",
      " -------------------------------- \n",
      "distance to range 1:  0.04421999999999998\n",
      "distance to range 2:  0.23325545454545454\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1 , evidence: PUBMED: consumption \n",
      " -------------------------------- \n",
      "distance to range 1:  0.07555047210300428\n",
      "distance to range 2:  0.2486670815450644\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.06866952789699571 , evidence: PUBMED: th nd novemb\n",
      " -------------------------------- \n",
      "distance to range 1:  0.07163935483870966\n",
      "distance to range 2:  0.1254841935483871\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.07258064516129033 , evidence: PUBMED: people age f\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.19047619047619047 evidence PUBMED: background i\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.15104166666666666 evidence PUBMED: objective sy\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.16101694915254236 evidence PUBMED: populationba\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.1568627450980392 evidence PUBMED: background h\n",
      " -------------------------------- \n",
      "distance to range 1:  0.003976097560975583\n",
      "distance to range 2:  0.18724658536585367\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.1402439024390244 , evidence: PUBMED: nation becom\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.25949367088607594 evidence PUBMED: background t\n",
      " -------------------------------- \n",
      "distance to range 1:  0.055510322580645144\n",
      "distance to range 2:  0.13354870967741936\n",
      "PUBMED CLASS PREDICTED =>  ratio: 0.08870967741935484 , evidence: PUBMED: alzheimers d\n",
      " -------------------------------- \n",
      "PUBMED : Fit ratio for individual articles:  0.2641509433962264 evidence PUBMED: background i\n",
      " -------------------------------- \n",
      "distance to range 1:  0.027198723404255304\n",
      "distance to range 2:  0.02339085106382982\n",
      "GPT CLASS PREDICTED =>  ratio: 0.2553191489361702 , evidence: GPT: musculoskeletal\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3695652173913043 evidence GPT: impact inflamma\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3372093023255814 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.4019607843137255 evidence GPT: comorbid metabo\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35789473684210527 evidence GPT: role genetic ri\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.36082474226804123 evidence GPT: impact hyperten\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3106796116504854 evidence GPT: impact chronic \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3548387096774194 evidence GPT: effects diabete\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.28865979381443296 evidence GPT: relationship al\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34444444444444444 evidence GPT: impact depressi\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32978723404255317 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35789473684210527 evidence GPT: complex relatio\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3333333333333333 evidence GPT: thyroid disorde\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3118279569892473 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30851063829787234 evidence GPT: relationship al\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.330188679245283 evidence GPT: impact anxiety \n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.38235294117647056 evidence GPT: role inflammati\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34328358208955223 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.417910447761194 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34375 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.31343283582089554 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.34210526315789475 evidence GPT: association alz\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32051282051282054 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.28169014084507044 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3684210526315789 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "distance to range 1:  0.010886666666666656\n",
      "distance to range 2:  0.03871000000000002\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.13333333333333333 , evidence: GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3333333333333333 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.35135135135135137 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.328125 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "distance to range 1:  0.08866444444444443\n",
      "distance to range 2:  0.04259888888888891\n",
      "GPT CLASS PREDICTED =>  ratio: 0.2361111111111111 , evidence: GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.32432432432432434 evidence GPT: association alz\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3 evidence GPT: association alz\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3076923076923077 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30158730158730157 evidence GPT: association alz\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3 evidence GPT: depression alzh\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.29333333333333333 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3815789473684211 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "distance to range 1:  0.0013628571428571379\n",
      "distance to range 2:  0.018969740259740298\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.14285714285714285 , evidence: GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3283582089552239 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.3246753246753247 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.30985915492957744 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "ChatGPT : Fit ratio for individual articles:  0.2987012987012987 evidence GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "distance to range 1:  0.020763209876543204\n",
      "distance to range 2:  0.10587049382716052\n",
      "PUBMED PREDICTED INCORRECTLY =>  ratio: 0.12345679012345678 , evidence: GPT: alzheimers dise\n",
      " -------------------------------- \n",
      "---------------COUNTS---------------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  50\n",
      "CHATGPT CLASSIFIED:  40\n",
      "FAILED_TO_CLASSIFY:  0\n",
      "GPT MISCLASSIFIED AS PUBMED:  10\n",
      "PUBMED MISCLASSIFIED AS GPT:  0\n",
      "-------------------------------------------------\n",
      "------------- %PERCENTAGE% -----------------------\n",
      "Number of publications analyzed:  0\n",
      "PUBMED CLASSIFIED:  1.0\n",
      "CHATGPT CLASSIFIED:  0.8\n",
      "FAILED_TO_CLASSIFY:  0.0\n",
      "GPT MISCLASSIFIED AS PUBMED:  0.2\n",
      "PUBMED MISCLASSIFIED AS GPT:  0.0\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# two classes classification\n",
    "\n",
    "two_articles_dataset = []\n",
    "\n",
    "for pubmed_article in pubmed_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('PUBMED: ' + pubmed_article)\n",
    "\n",
    "for gpt_article in gpt_articles_ready[200:250]:\n",
    "    two_articles_dataset.append('GPT: ' + gpt_article)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "chatgpt_class = 0\n",
    "pubmed_class = 0\n",
    "\n",
    "failed_to_classify = 0\n",
    "misclassified_as_gpt = 0\n",
    "misclassified_as_pubmed = 0\n",
    "\n",
    "\n",
    "# RANGE 1: PUBMED\n",
    "range1_start = pubmed_min_value\n",
    "range1_end = pubmed_max_value\n",
    "\n",
    "# RANGE 2: GPT\n",
    "range2_start = gpt_min_value\n",
    "range2_end = gpt_max_value\n",
    "\n",
    "for article in two_articles_dataset:\n",
    "    \n",
    "    gpt_ratio_val    = fit_an_article(article, gpt_lcc)\n",
    "    pubmed_ratio_val = fit_an_article(article, pubmed_lcc)\n",
    "    \n",
    "    # Classifying GPT\n",
    "    if gpt_ratio_val >= range2_start and ratio_val <= range2_end :       \n",
    "        if article[:20].startswith('GPT'):\n",
    "            chatgpt_class+=1\n",
    "            print('ChatGPT : Fit ratio for individual articles: ', gpt_ratio_val, 'evidence', article[:20])\n",
    "        else:\n",
    "            misclassified_as_pubmed+=1\n",
    "            \n",
    "    # Classifying PUBMED\n",
    "    elif pubmed_ratio_val >= range1_start and ratio_val <= range1_end:\n",
    "        if article[:20].startswith('PUBMED'):\n",
    "            pubmed_class += 1\n",
    "            print('PUBMED : Fit ratio for individual articles: ', pubmed_ratio_val, 'evidence', article[:20])\n",
    "        else: \n",
    "            misclassified_as_gpt+=1\n",
    "        \n",
    "    else:\n",
    "        # Calculate distances\n",
    "        distance_to_range1 = distance_to_range(pubmed_ratio_val, range1_start, range1_end)\n",
    "        distance_to_range2 = distance_to_range(gpt_ratio_val, range2_start, range2_end) \n",
    "        \n",
    "        print('distance to range 1: ', distance_to_range1)\n",
    "        print('distance to range 2: ', distance_to_range2)        \n",
    "        \n",
    "        # RANGE 1: PUBMED SHOULD WIN\n",
    "        if distance_to_range1 < distance_to_range2:\n",
    "            if article[:20].startswith('GPT'):\n",
    "                misclassified_as_gpt+=1\n",
    "                print('PUBMED PREDICTED INCORRECTLY => ', 'ratio:', pubmed_ratio_val ,', evidence:', article[:20])                \n",
    "            else:   \n",
    "                # count+=1\n",
    "                pubmed_class += 1\n",
    "                print('PUBMED CLASS PREDICTED => ', 'ratio:', pubmed_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "        # RANGE 2: GPT SHOULD WIN\n",
    "        elif distance_to_range2 < distance_to_range1:\n",
    "            if article[:20].startswith('PUBMED'):                \n",
    "                misclassified_as_pubmed+=1\n",
    "                print('GPT PREDICTED INCORRECTLY => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])                     \n",
    "            else:\n",
    "                chatgpt_class += 1\n",
    "                print('GPT CLASS PREDICTED => ', 'ratio:', gpt_ratio_val , ', evidence:', article[:20])\n",
    "\n",
    "    print(' -------------------------------- ')\n",
    "    \n",
    "    \n",
    "print('---------------COUNTS---------------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed) \n",
    "print('-------------------------------------------------') \n",
    "    \n",
    "    \n",
    "print('------------- %PERCENTAGE% -----------------------')    \n",
    "print('Number of publications analyzed: ', count)\n",
    "print('PUBMED CLASSIFIED: ', pubmed_class/50)   \n",
    "print('CHATGPT CLASSIFIED: ', chatgpt_class/50)   \n",
    "print('FAILED_TO_CLASSIFY: ', failed_to_classify/50)\n",
    "print('GPT MISCLASSIFIED AS PUBMED: ', misclassified_as_gpt/50)   \n",
    "print('PUBMED MISCLASSIFIED AS GPT: ', misclassified_as_pubmed/50) \n",
    "print('-------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05568f1f-c4fe-40bf-af2a-0154a6c8348a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c308484-0964-4f73-8dc0-725750d2197b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a966b1e-0b7f-4237-86ef-d6325c60681d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891b25b-0669-4cef-b8b0-adc22224bafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d712fb-8fd7-48bb-a9d1-41a25a5773a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
